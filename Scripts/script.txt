#!/usr/bin/env python3
"""
Tempest AI Model: Hybrid expert-guided and DQN-based gameplay system.
- Makes intelligent decisions based on enemy positions and level types
- Uses a Deep Q-Network (DQN) for reinforcement learning
- Expert system provides guidance and training examples
- Communicates with Tempest via socket connection
"""

# Prevent direct execution
if __name__ == "__main__":
    print("This is not the main application, run 'main.py' instead")
    exit(1)

# Global debug flag - set to False to disable debug output
DEBUG_MODE = False

# Override the built-in print function to always flush output
# This ensures proper line breaks in output when running in background
import builtins
_original_print = builtins.print

def _flushing_print(*args, **kwargs):
    # Use CR+LF line endings consistently
    new_args = []
    for arg in args:
        if isinstance(arg, str):
            # Strip trailing whitespace and line endings
            arg = arg.rstrip()
            new_args.append(arg)
        else:
            new_args.append(arg)
    
    # Set end parameter to use CR+LF
    kwargs["end"] = "\r\n"
    kwargs['flush'] = True
    return _original_print(*new_args, **kwargs)

builtins.print = _flushing_print

import os
import time
import struct
import random
import sys
import subprocess
import warnings
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Deque
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
# Removed StepLR and AMP imports (hybrid-only path doesn't use them)
import select
import threading
import queue
from collections import deque, namedtuple
from datetime import datetime
# Removed stable_baselines3 imports (no longer used after refactor)
import socket
import traceback
from torch.nn import SmoothL1Loss # Import SmoothL1Loss
# Robust import for running as script (Scripts on sys.path) and as package (repo root on sys.path)
try:
    from nstep_buffer import NStepReplayBuffer  # when running `python Scripts/main.py`
except Exception:
    try:
        from Scripts.nstep_buffer import NStepReplayBuffer  # when repo root is on sys.path
    except Exception:
        # If executed as package (python -m Scripts.main) and Scripts is a package
        from .nstep_buffer import NStepReplayBuffer

# Platform-specific imports for KeyboardHandler
import sys
# Initialize all potential module variables to None first
msvcrt = termios = tty = fcntl = None

if sys.platform == 'win32':
    try:
        import msvcrt
    except ImportError:
        # msvcrt remains None
        print("Warning: msvcrt module not found on Windows. Keyboard input will be disabled.")
elif sys.platform in ('linux', 'darwin'):
    try:
        import termios
        import tty
        import fcntl
        import select
    except ImportError:
        # termios, tty, fcntl remain None
        print("Warning: termios, tty, or fcntl module not found. Keyboard input will be disabled.")
else:
    # All remain None
    print(f"Warning: Unsupported platform '{sys.platform}' for keyboard input.")

# Import from config.py
from config import (
    SERVER_CONFIG,
    RL_CONFIG,
    MODEL_DIR,
    LATEST_MODEL_PATH,
    metrics as config_metrics,
    ServerConfigData,
    RLConfigData,
    RESET_METRICS
)

# Suppress warnings
warnings.filterwarnings('ignore')

# Global flag to track if running interactively
# Check this early before any potential tty interaction
IS_INTERACTIVE = sys.stdin.isatty()
print(f"Script Start: sys.stdin.isatty() = {IS_INTERACTIVE}") # DEBUG

# Initialize configuration
server_config = ServerConfigData()
rl_config = RLConfigData()

# Use values from config
params_count = server_config.params_count
state_size = rl_config.state_size

@dataclass
class FrameData:
    """Game state data for a single frame"""
    state: np.ndarray
    reward: float
    action: Tuple[bool, bool, float]  # fire, zap, spinner
    mode: int
    gamestate: int    # Added: Game state value from Lua
    done: bool
    attract: bool
    save_signal: bool
    enemy_seg: int
    player_seg: int
    open_level: bool
    expert_fire: bool  # Added: Expert system fire recommendation
    expert_zap: bool   # Added: Expert system zap recommendation
    level_number: int  # Added: Current level number from Lua
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'FrameData':
        """Create FrameData from dictionary"""
        return cls(
            state=data["state"],
            reward=data["reward"],
            action=data["action"],
            mode=data["mode"],
            gamestate=data.get("gamestate", 0),  # Default to 0 if not provided
            done=data["done"],
            attract=data["attract"],
            save_signal=data["save_signal"],
            enemy_seg=data["enemy_seg"],
            player_seg=data["player_seg"],
            open_level=data["open_level"],
            expert_fire=data["expert_fire"],
            expert_zap=data["expert_zap"],
            level_number=data.get("level_number", 0)  # Default to 0 if not provided
        )

# Configuration constants
SERVER_CONFIG = server_config
RL_CONFIG = rl_config

# Initialize devices (dual GPU setup: inference on GPU1, training on GPU0)
if torch.cuda.is_available():
    # Check if both GPUs are available for dual GPU setup
    if torch.cuda.device_count() >= 2:
        training_device = torch.device("cuda:0")
        inference_device = torch.device("cuda:1")
        print(f"Using dual GPU setup: Training on {training_device}, Inference on {inference_device}")
        print(f"Available GPUs: {torch.cuda.device_count()}")
        print(f"GPU 0: {torch.cuda.get_device_name(0)}")
        print(f"GPU 1: {torch.cuda.get_device_name(1)}")
    else:
        # Fall back to single GPU if only one is available
        training_device = torch.device("cuda:0")
        inference_device = torch.device("cuda:0")
        print(f"Only {torch.cuda.device_count()} GPU(s) available, using single GPU setup: {training_device}")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    training_device = torch.device("mps")
    inference_device = torch.device("mps")
    print(f"Using MPS for both inference and training: {training_device}")
else:
    training_device = torch.device("cpu")
    inference_device = torch.device("cpu")
    print(f"Using CPU for both inference and training: {training_device}")

# Low-risk math speedups (CUDA only): allow TF32 and tune matmul/cudnn
try:
    if training_device.type == 'cuda':
        # Enable TF32 where available for faster matmuls
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        # Prefer faster algorithms for float32 matmul in PyTorch 2+
        try:
            torch.set_float32_matmul_precision('high')
        except Exception:
            pass
except Exception:
    pass

# Display key configuration parameters
print(f"Learning rate: {RL_CONFIG.lr}")
print(f"Batch size: {RL_CONFIG.batch_size}")
print(f"Memory size: {RL_CONFIG.memory_size:,}")
print(f"Hidden size: {RL_CONFIG.hidden_size}")
print(f"Number of layers: {RL_CONFIG.num_layers}")

# Calculate and display layer architecture
layer_sizes = []
for i in range(RL_CONFIG.num_layers):
    pair_index = i // 2  # 0,0 -> 1,1 -> 2,2 -> ...
    layer_size = max(32, RL_CONFIG.hidden_size // (2 ** pair_index))
    layer_sizes.append(layer_size)

print(f"Layer architecture:")
print(f"  Layer 1: {RL_CONFIG.state_size} → {layer_sizes[0]}")
for i in range(1, len(layer_sizes)):
    print(f"  Layer {i+1}: {layer_sizes[i-1]} → {layer_sizes[i]}")
print(f"  Head layers: {layer_sizes[-1]} → {max(64, layer_sizes[-1] // 2)}")

print(f"Dueling: {'enabled' if getattr(RL_CONFIG, 'use_dueling', False) else 'disabled' } ")
print("Replay: hybrid experience buffer (discrete + continuous)")
print(f"Mixed precision: {'enabled' if getattr(RL_CONFIG, 'use_mixed_precision', False) else 'disabled'}")
print(f"State size: {RL_CONFIG.state_size}")

# For compatibility with dual-device code
device = training_device  # Legacy compatibility

# Initialize metrics
metrics = config_metrics

# Global reference to server for metrics display
metrics.global_server = None

# Legacy discrete-only replay types removed; hybrid-only

# Discrete-only QNetwork removed (hybrid-only)

class NoisyLinear(nn.Module):
    """Factorized Gaussian NoisyNet layer (adds noise only in training mode)."""
    def __init__(self, in_features: int, out_features: int, sigma_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        # Learnable parameters
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Buffers for noise
        self.register_buffer('weight_eps', torch.empty(out_features, in_features))
        self.register_buffer('bias_eps', torch.empty(out_features))

        self.reset_parameters(sigma_init)
        self.reset_noise()

    def reset_parameters(self, sigma_init: float):
        mu_range = 1 / np.sqrt(self.in_features)
        with torch.no_grad():
            self.weight_mu.uniform_(-mu_range, mu_range)
            self.bias_mu.uniform_(-mu_range, mu_range)
            self.weight_sigma.fill_(sigma_init / np.sqrt(self.in_features))
            self.bias_sigma.fill_(sigma_init / np.sqrt(self.out_features))

    def _scale_noise(self, size):
        x = torch.randn(size, device=self.weight_mu.device)
        return x.sign() * x.abs().sqrt()

    def reset_noise(self):
        eps_in = self._scale_noise(self.in_features)
        eps_out = self._scale_noise(self.out_features)
        self.weight_eps.copy_(eps_out.ger(eps_in))
        self.bias_eps.copy_(eps_out)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        if self.training:
            # Use current sampled noise; do not mutate buffers during forward
            weight = self.weight_mu + self.weight_sigma * self.weight_eps
            bias = self.bias_mu + self.bias_sigma * self.bias_eps
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        return F.linear(input, weight, bias)

class HybridDQN(nn.Module):
    """Hybrid DQN with discrete fire/zap actions + continuous spinner.
    
    Architecture:
    - Shared trunk: processes state features
    - Discrete head: outputs Q-values for 4 fire/zap combinations
    - Continuous head: outputs single continuous spinner value in [-0.9, +0.9]
    
    Forward pass returns: (discrete_q_values, continuous_spinner)
    - discrete_q_values: (batch_size, 4) Q-values for fire/zap combinations
    - continuous_spinner: (batch_size, 1) spinner values in [-0.9, +0.9]
    """
    def __init__(self, state_size: int, discrete_actions: int = 4, 
                 hidden_size: int = 512, num_layers: int = 3, 
                 use_dueling: bool = False, use_noisy: bool = False, noisy_std: float = 0.1):
        super(HybridDQN, self).__init__()
        
        self.state_size = state_size
        self.discrete_actions = discrete_actions
        self.use_dueling = use_dueling
        self.use_noisy = use_noisy
        self.num_layers = num_layers
        
        # Shared trunk for feature extraction
        LinearOrNoisy = NoisyLinear if use_noisy else nn.Linear
        
        # Dynamic layer sizing with pairs: A,A -> A/2,A/2 -> A/4,A/4 -> ...
        # Pattern: 512,512 -> 256,256 -> 128,128 -> ...
        layer_sizes = []
        current_size = hidden_size
        for i in range(num_layers):
            # Determine size for this layer pair
            pair_index = i // 2  # 0,0 -> 1,1 -> 2,2 -> ...
            layer_size = max(32, hidden_size // (2 ** pair_index))
            layer_sizes.append(layer_size)
        
        # Create shared layers dynamically
        self.shared_layers = nn.ModuleList()
        
        # First layer: state_size -> hidden_size
        if use_noisy:
            self.shared_layers.append(LinearOrNoisy(state_size, layer_sizes[0], noisy_std))
        else:
            self.shared_layers.append(LinearOrNoisy(state_size, layer_sizes[0]))
        
        # Subsequent layers: hidden_size -> hidden_size/2 -> hidden_size/4 -> ...
        for i in range(1, num_layers):
            if use_noisy:
                self.shared_layers.append(LinearOrNoisy(layer_sizes[i-1], layer_sizes[i], noisy_std))
            else:
                self.shared_layers.append(LinearOrNoisy(layer_sizes[i-1], layer_sizes[i]))
        
        # Final layer size for heads
        shared_output_size = layer_sizes[-1]
        head_size = max(64, shared_output_size // 2)  # Head layer size
        
        if use_dueling:
            # Dueling architecture for discrete Q-values
            self.discrete_val_fc = nn.Linear(shared_output_size, head_size)
            self.discrete_adv_fc = nn.Linear(shared_output_size, head_size)
            self.discrete_val_out = nn.Linear(head_size, 1)  # State value
            self.discrete_adv_out = nn.Linear(head_size, discrete_actions)  # Advantages
        else:
            # Standard architecture for discrete Q-values
            self.discrete_fc = nn.Linear(shared_output_size, head_size)
            self.discrete_out = nn.Linear(head_size, discrete_actions)
        
        # Continuous head for spinner (always separate from dueling)
        self.continuous_fc1 = nn.Linear(shared_output_size, head_size)
        continuous_head_size = max(32, head_size // 2)
        self.continuous_fc2 = nn.Linear(head_size, continuous_head_size)
        self.continuous_out = nn.Linear(continuous_head_size, 1)
        
        # Initialize continuous head with smaller weights for stable training
        torch.nn.init.xavier_normal_(self.continuous_out.weight, gain=0.1)
        torch.nn.init.constant_(self.continuous_out.bias, 0.0)
    
    def reset_noise(self):
        """Reset noise layers if using noisy networks"""
        if self.use_noisy:
            for m in self.modules():
                if isinstance(m, NoisyLinear):
                    m.reset_noise()
    
    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """Forward pass returning (discrete_q_values, continuous_spinner)
        
        Args:
            x: Input state tensor (batch_size, state_size)
            
        Returns:
            discrete_q_values: Q-values for fire/zap combinations (batch_size, 4)
            continuous_spinner: Spinner values in [-0.9, +0.9] (batch_size, 1)
        """
        # Shared feature extraction through dynamic layers
        shared = x
        for layer in self.shared_layers:
            shared = F.relu(layer(shared))
        
        # Discrete Q-values head
        if self.use_dueling:
            # Dueling network: V(s) + A(s,a) - mean(A(s,·))
            discrete_val = F.relu(self.discrete_val_fc(shared))
            discrete_val = self.discrete_val_out(discrete_val)  # (B, 1)
            
            discrete_adv = F.relu(self.discrete_adv_fc(shared))
            discrete_adv = self.discrete_adv_out(discrete_adv)  # (B, discrete_actions)
            
            # Center advantages around their mean
            discrete_q = discrete_val + (discrete_adv - discrete_adv.mean(dim=1, keepdim=True))
        else:
            # Standard Q-network
            discrete = F.relu(self.discrete_fc(shared))
            discrete_q = self.discrete_out(discrete)  # (B, discrete_actions)
        
        # Continuous spinner head
        continuous = F.relu(self.continuous_fc1(shared))
        continuous = F.relu(self.continuous_fc2(continuous))
        continuous_raw = self.continuous_out(continuous)  # (B, 1)
        
        # Apply tanh to bound spinner to [-1, +1] then scale to [-0.9, +0.9]
        continuous_spinner = torch.tanh(continuous_raw) * 0.9
        
        return discrete_q, continuous_spinner

class HybridReplayBuffer:
    """Experience replay buffer for hybrid discrete-continuous actions.
    
    Stores experiences as: (state, discrete_action, continuous_action, reward, next_state, done)
    - discrete_action: integer index for fire/zap combination (0-3)
    - continuous_action: float spinner value in [-0.9, +0.9]
    """
    def __init__(self, capacity: int, state_size: int):
        self.capacity = capacity
        self.position = 0
        self.size = 0
        self.state_size = int(state_size)
        
        # Pre-allocated arrays for maximum speed
        self.states = np.empty((capacity, self.state_size), dtype=np.float32)
        self.discrete_actions = np.empty((capacity,), dtype=np.int32)
        self.continuous_actions = np.empty((capacity,), dtype=np.float32)
        self.rewards = np.empty((capacity,), dtype=np.float32)
        self.next_states = np.empty((capacity, self.state_size), dtype=np.float32)
        self.dones = np.empty((capacity,), dtype=np.bool_)
    
    def push(self, state, discrete_action, continuous_action, reward, next_state, done):
        """Add experience to buffer"""
        # Coerce inputs to proper types
        discrete_idx = int(discrete_action) if not isinstance(discrete_action, int) else discrete_action
        continuous_val = float(continuous_action) if not isinstance(continuous_action, float) else continuous_action
        
        # Clamp continuous action to valid range
        continuous_val = max(-0.9, min(0.9, continuous_val))
        
        # Store experience
        try:
            s = np.asarray(state, dtype=np.float32)
            if s.ndim > 1:
                s = s.reshape(-1)
            if s.size < self.state_size:
                tmp = np.zeros((self.state_size,), dtype=np.float32)
                tmp[:s.size] = s
                s = tmp
            elif s.size > self.state_size:
                s = s[:self.state_size]
            self.states[self.position, :] = s
        except Exception:
            # Fallback to zeros if something went wrong
            self.states[self.position, :] = 0.0
        self.discrete_actions[self.position] = discrete_idx
        self.continuous_actions[self.position] = continuous_val
        self.rewards[self.position] = reward
        try:
            ns = np.asarray(next_state, dtype=np.float32)
            if ns.ndim > 1:
                ns = ns.reshape(-1)
            if ns.size < self.state_size:
                tmp = np.zeros((self.state_size,), dtype=np.float32)
                tmp[:ns.size] = ns
                ns = tmp
            elif ns.size > self.state_size:
                ns = ns[:self.state_size]
            self.next_states[self.position, :] = ns
        except Exception:
            self.next_states[self.position, :] = 0.0
        self.dones[self.position] = done
        
        # Update position and size
        self.position = (self.position + 1) % self.capacity
        if self.size < self.capacity:
            self.size += 1
    
    def sample(self, batch_size):
        """Sample batch of experiences with optional recent-window bias."""
        if self.size < batch_size:
            return None

        # Default: uniform over buffer
        indices = None
        try:
            bias = float(getattr(RL_CONFIG, 'recent_sample_bias', 0.0) or 0.0)
            window_frac = float(getattr(RL_CONFIG, 'recent_window_frac', 0.0) or 0.0)
        except Exception:
            bias, window_frac = 0.0, 0.0

        if bias <= 0.0 or window_frac <= 0.0:
            indices = np.random.randint(0, self.size, size=batch_size)
        else:
            # Split batch: some from recent window, remainder from full buffer
            recent_count = int(batch_size * min(max(bias, 0.0), 1.0))
            global_count = batch_size - recent_count
            # Recent window range [start, size)
            recent_window = int(self.size * (1.0 - min(max(window_frac, 0.0), 1.0)))
            recent_low = max(0, min(recent_window, self.size - 1))
            if recent_count > 0 and recent_low < self.size:
                recent_idx = np.random.randint(recent_low, self.size, size=recent_count)
            else:
                recent_idx = np.empty((0,), dtype=np.int64)
            if global_count > 0:
                global_idx = np.random.randint(0, self.size, size=global_count)
            else:
                global_idx = np.empty((0,), dtype=np.int64)
            indices = np.concatenate([recent_idx, global_idx])
        
        # Vectorized gather for batch data
        states_np = self.states[indices]
        next_states_np = self.next_states[indices]
        batch_discrete_actions = self.discrete_actions[indices]
        batch_continuous_actions = self.continuous_actions[indices]
        batch_rewards = self.rewards[indices]
        batch_dones = self.dones[indices]

        # Convert to tensors (pin memory for fast H2D when on CUDA)
        use_pinned = (training_device.type == 'cuda')
        states = torch.from_numpy(states_np).float()
        next_states = torch.from_numpy(next_states_np).float()
        discrete_actions = torch.from_numpy(batch_discrete_actions.reshape(-1, 1)).long()
        continuous_actions = torch.from_numpy(batch_continuous_actions.reshape(-1, 1)).float()
        rewards = torch.from_numpy(batch_rewards.reshape(-1, 1)).float()
        dones = torch.from_numpy(batch_dones.reshape(-1, 1).astype(np.uint8)).float()

        if use_pinned:
            states = states.pin_memory()
            next_states = next_states.pin_memory()
            discrete_actions = discrete_actions.pin_memory()
            continuous_actions = continuous_actions.pin_memory()
            rewards = rewards.pin_memory()
            dones = dones.pin_memory()

        non_block = True if training_device.type == 'cuda' else False
        states = states.to(training_device, non_blocking=non_block)
        next_states = next_states.to(training_device, non_blocking=non_block)
        discrete_actions = discrete_actions.to(training_device, non_blocking=non_block)
        continuous_actions = continuous_actions.to(training_device, non_blocking=non_block)
        rewards = rewards.to(training_device, non_blocking=non_block)
        dones = dones.to(training_device, non_blocking=non_block)
        
        return states, discrete_actions, continuous_actions, rewards, next_states, dones
    
    def __len__(self):
        return self.size

## Hybrid-only path: legacy discrete-only DQNAgent removed
## N-step learning handled upstream (server) or via dedicated buffer; hybrid agent expects 1-step or n-step rewards provided.
    
class KeyboardHandler:
    """Cross-platform non-blocking keyboard input handler."""
    def __init__(self):
        self.platform = sys.platform
        # Store module references or None if import failed
        self.msvcrt = msvcrt 
        self.termios = termios
        self.tty = tty
        self.fcntl = fcntl
        
        self.fd = None
        self.old_settings = None

        if not IS_INTERACTIVE:
            return 

        if self.platform == 'win32':
            if self.msvcrt:
                print("KeyboardHandler: Using msvcrt for Windows.")
        elif self.platform in ('linux', 'darwin'):
            if self.termios: 
                try:
                    self.fd = sys.stdin.fileno()
                    self.old_settings = self.termios.tcgetattr(self.fd)
                    print(f"KeyboardHandler: Using termios/tty for {self.platform}.")
                except self.termios.error as e:
                    print(f"Warning: Failed to get terminal attributes: {e}. Keyboard input might be impaired.")
                    self.fd = None 
                    self.old_settings = None
            else:
                 print("Warning: Unix terminal modules failed to import. Keyboard input disabled.")
        else:
            print(f"Warning: Unsupported platform '{self.platform}' for KeyboardHandler. Keyboard input disabled.")

    def setup_terminal(self):
        """Set the terminal to raw/non-blocking (Unix-only)."""
        if self.platform in ('linux', 'darwin') and self.fd is not None and self.old_settings is not None:
            # Use instance attributes to check if modules were imported successfully
            if self.tty and self.fcntl and self.termios:
                try:
                    self.tty.setraw(self.fd)
                    flags = self.fcntl.fcntl(self.fd, self.fcntl.F_GETFL)
                    self.fcntl.fcntl(self.fd, self.fcntl.F_SETFL, flags | os.O_NONBLOCK)
                except self.termios.error as e:
                    print(f"Warning: Could not set terminal to raw/non-blocking mode: {e}")
            else:
                 print("Warning: Required Unix terminal modules not available, cannot set raw mode.")
        # No setup needed for Windows msvcrt

    def __enter__(self):
        self.setup_terminal()
        return self
        
    def __exit__(self, *args):
        self.restore_terminal()
        
    def check_key(self):
        """Check for keyboard input non-blockingly (cross-platform)."""
        if not IS_INTERACTIVE:
            return None

        try:
            if self.platform == 'win32' and self.msvcrt:
                if self.msvcrt.kbhit():
                    # Read the byte and decode assuming simple ASCII/UTF-8 key
                    key_byte = self.msvcrt.getch()
                    try:
                        return key_byte.decode('utf-8')
                    except UnicodeDecodeError:
                        return None # Or handle special keys differently
                else:
                    return None # No key waiting on Windows
            elif self.platform in ('linux', 'darwin') and self.fd is not None:
                 # Check if required modules are available via instance attributes
                 if self.termios and select: # Need termios for setup, select for check
                     try:
                         if select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                             return sys.stdin.read(1)
                         else:
                             return None 
                     except Exception as e:
                        return None
                 else:
                     print("Warning: Unix termios/select modules not available for key check.")
                     return None
            else:
                return None
        except (IOError, TypeError) as e:
            return None

    def restore_terminal(self):
        """Restore original terminal settings (Unix-only)."""
        if self.platform in ('linux', 'darwin') and self.fd is not None and self.old_settings is not None:
             # Check if termios module is available via instance attribute
             if self.termios:
                try:
                    self.termios.tcsetattr(self.fd, self.termios.TCSADRAIN, self.old_settings)
                except self.termios.error as e:
                    print(f"Warning: Could not restore terminal settings: {e}")
             else:
                 print("Warning: Unix termios module not available for restore.")
        # No restore needed for Windows msvcrt

    def set_raw_mode(self):
        """Set terminal back to raw mode (Unix-only)."""
        if self.platform in ('linux', 'darwin') and self.fd is not None:
             # Check if tty/termios modules are available via instance attributes
             if self.tty and self.termios:
                try:
                    self.tty.setraw(self.fd)
                except self.termios.error as e: 
                    print(f"Warning: Could not set terminal to raw mode: {e}")
             else:
                  print("Warning: Unix tty/termios modules not available for set_raw_mode.")
        # No equivalent needed for Windows msvcrt

def print_with_terminal_restore(kb_handler, *args, **kwargs):
    """Print with proper terminal settings (cross-platform safe)."""
    # Only attempt restore/set_raw if on Unix and handler is valid
    is_unix_like = kb_handler and kb_handler.platform in ('linux', 'darwin')
    
    if IS_INTERACTIVE and is_unix_like:
        kb_handler.restore_terminal()
        
    # Standard print call - works on all platforms
    print(*args, **kwargs, flush=True)
    
    if IS_INTERACTIVE and is_unix_like:
        kb_handler.set_raw_mode()

def setup_environment():
    """Set up environment for socket server"""
    print("\nSetting up environment...")
    os.makedirs(MODEL_DIR, exist_ok=True)
    print(f"Model directory: {MODEL_DIR}")
    print(f"Server will listen on {SERVER_CONFIG.host}:{SERVER_CONFIG.port}")
    print(f"Ready to handle up to {SERVER_CONFIG.max_clients} clients")

class HybridDQNAgent:
    """Hybrid DQN Agent with discrete fire/zap actions + continuous spinner"""
    
    def __init__(self, state_size, discrete_actions=4, learning_rate=RL_CONFIG.lr, 
                 gamma=RL_CONFIG.gamma, epsilon=RL_CONFIG.epsilon, 
                 epsilon_min=RL_CONFIG.epsilon_min, memory_size=RL_CONFIG.memory_size, 
                 batch_size=RL_CONFIG.batch_size):
        
        self.state_size = state_size
        self.discrete_actions = discrete_actions
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        self.last_save_time = 0.0
        
        # Hybrid neural networks
        self.qnetwork_local = HybridDQN(
            state_size=state_size,
            discrete_actions=discrete_actions,
            hidden_size=RL_CONFIG.hidden_size,
            num_layers=RL_CONFIG.num_layers,
            use_dueling=RL_CONFIG.use_dueling,
            use_noisy=RL_CONFIG.use_noisy_nets
        ).to(training_device)
        
        self.qnetwork_target = HybridDQN(
            state_size=state_size,
            discrete_actions=discrete_actions,
            hidden_size=RL_CONFIG.hidden_size,
            num_layers=RL_CONFIG.num_layers,
            use_dueling=RL_CONFIG.use_dueling,
            use_noisy=RL_CONFIG.use_noisy_nets
        ).to(training_device)
        
        # Create inference copy on inference device
        if inference_device != training_device:
            print(f"Creating dedicated hybrid inference network on {inference_device}")
            self.qnetwork_inference = HybridDQN(
                state_size=state_size,
                discrete_actions=discrete_actions,
                hidden_size=RL_CONFIG.hidden_size,
                num_layers=RL_CONFIG.num_layers,
                use_dueling=RL_CONFIG.use_dueling,
                use_noisy=RL_CONFIG.use_noisy_nets
            ).to(inference_device)
        else:
            self.qnetwork_inference = self.qnetwork_local
        
        # Initialize target network
        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())
        # Establish persistent modes:
        # - training (local) stays in train mode
        # - target stays in eval mode
        # - inference stays in eval mode if it's a dedicated copy
        self.qnetwork_local.train()
        self.qnetwork_target.eval()
        if self.qnetwork_inference is not self.qnetwork_local:
            self.qnetwork_inference.eval()
        
        # Store device references
        self.inference_device = inference_device
        self.training_device = training_device
        
        # Optimizer with separate parameter groups for discrete and continuous components
        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)
        
        # Experience replay (vectorized 2D storage)
        self.memory = HybridReplayBuffer(memory_size, state_size=self.state_size)

        # AMP persistent settings
        self.use_amp = bool(getattr(RL_CONFIG, 'use_mixed_precision', False) and self.training_device.type == 'cuda')
        if self.use_amp:
            try:
                from torch.cuda.amp import autocast as _autocast, GradScaler
                self.autocast = _autocast
                self.scaler = GradScaler(enabled=True)
            except Exception:
                self.use_amp = False
                import contextlib
                self.autocast = contextlib.nullcontext
                self.scaler = None
        else:
            import contextlib
            self.autocast = contextlib.nullcontext
            self.scaler = None
        
        # Training queue and metrics
        self.train_queue = queue.Queue(maxsize=100)
        self.training_steps = 0
        self.last_target_update = 0
        self.last_inference_sync = 0
        # Honor global training enable toggle
        self.training_enabled = True

        # Background training worker(s)
        self.running = True
        self.num_training_workers = int(getattr(RL_CONFIG, 'training_workers', 1) or 1)
        # Serialize optimizer updates across threads to avoid race conditions
        self.training_lock = threading.Lock()
        self.training_threads = []
        for i in range(self.num_training_workers):
            t = threading.Thread(target=self.background_train, daemon=True, name=f"HybridTrainWorker-{i}")
            t.start()
            self.training_threads.append(t)

        # Inference sync heartbeat to ensure periodic sync even if training is sparse
        self._sync_interval_seconds = float(getattr(RL_CONFIG, 'inference_sync_interval_sec', 10.0) or 10.0)
        self._sync_thread = threading.Thread(target=self._inference_sync_heartbeat, daemon=True, name="HybridInferenceSyncHeartbeat")
        self._sync_thread.start()
        
    def act(self, state, epsilon=0.0, add_noise=True):
        """Select hybrid action using epsilon-greedy for discrete + Gaussian noise for continuous
        
        Returns:
            discrete_action: int (0-3) for fire/zap combination
            continuous_action: float in [-0.9, +0.9] for spinner
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(self.inference_device)
        
        # Do not flip modes every call; rely on persistent .eval() for dedicated inference model
        with torch.no_grad():
            discrete_q, continuous_pred = self.qnetwork_inference(state)
        
        # Discrete action selection (epsilon-greedy)
        if random.random() < epsilon:
            discrete_action = random.randint(0, self.discrete_actions - 1)
        else:
            discrete_action = discrete_q.cpu().data.numpy().argmax()
        
        # Continuous action selection (predicted value + optional exploration noise)
        continuous_action = continuous_pred.cpu().data.numpy()[0, 0]
        if add_noise and epsilon > 0:
            # Add Gaussian noise scaled by epsilon for exploration
            noise_scale = epsilon * 0.3  # 30% of action range at full epsilon
            noise = np.random.normal(0, noise_scale)
            continuous_action = np.clip(continuous_action + noise, -0.9, 0.9)
        
        return int(discrete_action), float(continuous_action)
    
    def step(self, state, discrete_action, continuous_action, reward, next_state, done):
        """Add experience to memory and queue training"""
        self.memory.push(state, discrete_action, continuous_action, reward, next_state, done)
        
        # Queue multiple training steps per experience
        if not getattr(metrics, 'training_enabled', True) or not self.training_enabled:
            return
        # Apply training_steps_per_sample ONLY in the background trainer to avoid double-counting here.
        # Enqueue a single token per environment sample to minimize queue overhead.
        try:
            self.train_queue.put_nowait(True)
        except queue.Full:
            pass
        # Optional telemetry
        try:
            metrics.training_queue_size = int(self.train_queue.qsize())
        except Exception:
            pass

    def background_train(self):
        """Background worker that drains the train_queue and performs train steps."""
        worker_id = threading.current_thread().name
        print(f"Training thread {worker_id} started on {self.training_device}")
        while self.running:
            try:
                _ = self.train_queue.get()  # block until work arrives
                # Skip training work if disabled; still mark task done
                if not getattr(metrics, 'training_enabled', True) or not self.training_enabled:
                    self.train_queue.task_done()
                    continue
                steps_per_req = int(getattr(RL_CONFIG, 'training_steps_per_sample', 2) or 1)
                for _ in range(steps_per_req):
                    loss_val = self.train_step()
                    if loss_val is None:
                        loss_val = 0.0
                # Optional telemetry after consuming a token
                try:
                    metrics.training_queue_size = int(self.train_queue.qsize())
                except Exception:
                    pass
                self.train_queue.task_done()
            except AssertionError:
                raise
            except Exception as e:
                print(f"Hybrid training error in {worker_id}: {e}")
                import traceback; traceback.print_exc()
                time.sleep(0.01)
    
    def train_step(self):
        """Perform one optimizer update with gradient accumulation over micro-batches.

        Accumulates gradients over RL_CONFIG.gradient_accumulation_steps micro-batches,
        each of size RL_CONFIG.batch_size, then applies a single optimizer.step().
        The entire accumulation window is performed under a lock to keep weights
        consistent across micro-batches when multiple training workers are enabled.
        """
        # Global gate
        if not getattr(metrics, 'training_enabled', True) or not self.training_enabled:
            return 0.0
        # Post-load burn-in: require some fresh frames after loading before training
        try:
            loaded_fc = int(getattr(metrics, 'loaded_frame_count', 0) or 0)
            require_new = int(getattr(RL_CONFIG, 'min_new_frames_after_load_to_train', 0) or 0)
            if loaded_fc > 0 and (metrics.frame_count - loaded_fc) < require_new:
                return 0.0
        except Exception:
            pass

        # Require at least one micro-batch worth of data
        if len(self.memory) < self.batch_size:
            return 0.0

        # Accumulation setup
        grad_accum_steps = max(1, int(getattr(RL_CONFIG, 'gradient_accumulation_steps', 1) or 1))
        use_amp = self.use_amp
        scaler = self.scaler
        w_cont = float(getattr(RL_CONFIG, 'continuous_loss_weight', 0.5) or 0.5)

        # Keep track of loss for telemetry
        total_loss_value = 0.0

        # Perform accumulation and optimizer step atomically for thread safety
        with self.training_lock:
            # Zero gradients before accumulation
            self.optimizer.zero_grad(set_to_none=True)

            preclip_grad_norm = 0.0
            postclip_grad_norm = 0.0

            for acc_idx in range(grad_accum_steps):
                # Sample micro-batch
                batch = self.memory.sample(self.batch_size)
                if batch is None:
                    if acc_idx == 0:
                        return 0.0
                    else:
                        break

                states, discrete_actions, continuous_actions, rewards, next_states, dones = batch

                # Forward pass (local) under autocast as configured
                if use_amp:
                    with self.autocast():
                        discrete_q_pred, continuous_pred = self.qnetwork_local(states)
                        discrete_q_selected = discrete_q_pred.gather(1, discrete_actions)
                else:
                    discrete_q_pred, continuous_pred = self.qnetwork_local(states)
                    discrete_q_selected = discrete_q_pred.gather(1, discrete_actions)

                # Target computation detached
                with torch.no_grad():
                    if bool(getattr(RL_CONFIG, 'use_double_dqn', True)):
                        next_q_local, _ = self.qnetwork_local(next_states)
                        next_actions = next_q_local.argmax(dim=1, keepdim=True)
                        next_q_target, _ = self.qnetwork_target(next_states)
                        discrete_q_next_max = next_q_target.gather(1, next_actions)
                    else:
                        next_q_target, _ = self.qnetwork_target(next_states)
                        discrete_q_next_max = next_q_target.max(1)[0].unsqueeze(1)

                    # n-step gamma and reward transforms
                    n_step = int(getattr(RL_CONFIG, 'n_step', 1) or 1)
                    gamma_boot = (self.gamma ** n_step) if n_step > 1 else self.gamma
                    r = rewards
                    try:
                        rs = float(getattr(RL_CONFIG, 'reward_scale', 1.0) or 1.0)
                        if rs != 1.0:
                            r = r * rs
                        rc = float(getattr(RL_CONFIG, 'reward_clamp_abs', 0.0) or 0.0)
                        if rc > 0.0:
                            r = torch.clamp(r, -rc, rc)
                        if bool(getattr(RL_CONFIG, 'reward_tanh', False)):
                            r = torch.tanh(r)
                    except Exception:
                        pass
                    discrete_targets = r + (gamma_boot * discrete_q_next_max * (1 - dones))
                    continuous_targets = continuous_actions

                # Match dtypes under AMP
                if use_amp:
                    try:
                        discrete_targets = discrete_targets.to(discrete_q_selected.dtype)
                        continuous_targets = continuous_targets.to(continuous_pred.dtype)
                    except Exception:
                        pass

                # Losses per micro-batch (autocast already applied for forward)
                if use_amp:
                    with self.autocast():
                        d_loss = F.huber_loss(discrete_q_selected, discrete_targets)
                        c_loss = F.mse_loss(continuous_pred, continuous_targets)
                        micro_total = d_loss + w_cont * c_loss
                        # Scale loss for accumulation so effective LR remains constant
                        micro_total = micro_total / float(grad_accum_steps)
                else:
                    d_loss = F.huber_loss(discrete_q_selected, discrete_targets)
                    c_loss = F.mse_loss(continuous_pred, continuous_targets)
                    micro_total = (d_loss + w_cont * c_loss) / float(grad_accum_steps)

                total_loss_value += float((d_loss + w_cont * c_loss).item()) / float(grad_accum_steps)

                # Backward accumulate
                if use_amp and scaler is not None:
                    scaler.scale(micro_total).backward()
                else:
                    micro_total.backward()

            # Unscale once before measuring/clipping
            if use_amp and scaler is not None:
                try:
                    scaler.unscale_(self.optimizer)
                except Exception:
                    pass

            # Replace non-finite grads and compute pre-clip norm
            preclip_sq_sum = 0.0
            try:
                for p in self.qnetwork_local.parameters():
                    if p.grad is not None:
                        if not torch.isfinite(p.grad).all():
                            p.grad.data = torch.nan_to_num(p.grad.data, nan=0.0, posinf=0.0, neginf=0.0)
                        g = p.grad.data.norm(2)
                        if torch.isfinite(g):
                            preclip_sq_sum += float(g.item() ** 2)
                preclip_grad_norm = (preclip_sq_sum ** 0.5)
            except Exception:
                preclip_grad_norm = 0.0

            # Optional gradient value clamp and norm clip
            postclip_grad_norm = preclip_grad_norm
            if hasattr(RL_CONFIG, 'max_grad_norm') and RL_CONFIG.max_grad_norm > 0:
                try:
                    max_gv = float(getattr(RL_CONFIG, 'max_grad_value', 0.0) or 0.0)
                    if max_gv > 0.0:
                        for p in self.qnetwork_local.parameters():
                            if p.grad is not None:
                                p.grad.data.clamp_(-max_gv, max_gv)
                    torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), RL_CONFIG.max_grad_norm)
                    post_sq_sum = 0.0
                    for p in self.qnetwork_local.parameters():
                        if p.grad is not None:
                            g = p.grad.data.norm(2)
                            if torch.isfinite(g):
                                x = float(g.item())
                                post_sq_sum += x * x
                    postclip_grad_norm = (post_sq_sum ** 0.5)
                except Exception:
                    postclip_grad_norm = preclip_grad_norm

            # Optimizer step
            if use_amp and scaler is not None:
                scaler.step(self.optimizer)
                scaler.update()
            else:
                self.optimizer.step()

        # LR scheduling (frame-based)
        try:
            self._apply_lr_schedule()
        except Exception:
            pass
        
        # Update training counters and metrics
        self.training_steps += 1
        try:
            metrics.total_training_steps += 1
            metrics.training_steps_interval += 1
            metrics.memory_buffer_size = len(self.memory)
        except Exception:
            pass
        # Track loss values for display
        try:
            metrics.losses.append(float(total_loss_value))
            metrics.loss_sum_interval += float(total_loss_value)
            metrics.loss_count_interval += 1
        except Exception:
            pass
        # Publish grad diagnostics
        try:
            # Only publish finite diagnostics
            if preclip_grad_norm > 0 and np.isfinite(preclip_grad_norm) and np.isfinite(postclip_grad_norm):
                metrics.grad_clip_delta = float(postclip_grad_norm / max(preclip_grad_norm, 1e-12))
                metrics.grad_norm = float(preclip_grad_norm)
            else:
                metrics.grad_clip_delta = 1.0
                metrics.grad_norm = 0.0
        except Exception:
            pass

        # Periodic hard refresh during warmup or watchdog interval
        try:
            # Hard refresh during warmup steps
            warm_until = int(getattr(RL_CONFIG, 'warmup_hard_refresh_until_steps', 0) or 0)
            warm_every = int(getattr(RL_CONFIG, 'warmup_hard_refresh_every_steps', 0) or 0)
            if warm_until > 0 and self.training_steps <= warm_until and warm_every > 0:
                if (self.training_steps % warm_every) == 0:
                    self.update_target_network()
            # Time-based watchdog
            watchdog_secs = float(getattr(RL_CONFIG, 'hard_update_watchdog_seconds', 0.0) or 0.0)
            if watchdog_secs > 0.0:
                last_t = float(getattr(metrics, 'last_hard_target_update_time', 0.0) or 0.0)
                if last_t <= 0.0 or (time.time() - last_t) >= watchdog_secs:
                    self.update_target_network()
            # Step-based periodic refresh even when using soft target
            step_every = int(getattr(RL_CONFIG, 'hard_target_refresh_every_steps', 0) or 0)
            if step_every > 0 and (self.training_steps % step_every) == 0:
                self.update_target_network()
        except Exception:
            pass

        # Soft target update
        if getattr(RL_CONFIG, 'use_soft_target', True):
            tau = getattr(RL_CONFIG, 'tau', 0.005)
            for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):
                target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
            # Telemetry for target updates (soft)
            try:
                metrics.last_target_update_frame = metrics.frame_count
                metrics.last_target_update_time = time.time()
            except Exception:
                pass
        
        # Hard target update
        elif self.training_steps % RL_CONFIG.target_update_freq == 0:
            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())
            # Telemetry for hard target updates
            try:
                metrics.last_target_update_frame = metrics.frame_count
                metrics.last_target_update_time = time.time()
                metrics.last_hard_target_update_frame = metrics.frame_count
                metrics.last_hard_target_update_time = time.time()
            except Exception:
                pass
        
        # Sync inference network
        if inference_device != training_device and self.training_steps % 100 == 0:
            self.sync_inference_network()
        elif self.training_steps % 100 == 0:
            # Even in single-device mode, update sync telemetry periodically
            try:
                metrics.last_inference_sync_frame = metrics.frame_count
                metrics.last_inference_sync_time = time.time()
            except Exception:
                pass
        
        return float(total_loss_value)

    def _apply_lr_schedule(self):
        """Compute and apply per-frame LR based on RL_CONFIG schedule."""
        sched = str(getattr(RL_CONFIG, 'lr_schedule', 'none') or 'none').lower()
        if sched == 'none':
            return
        # Determine current global frame
        try:
            fc = int(getattr(metrics, 'frame_count', 0) or 0)
        except Exception:
            fc = 0
        lr_base = float(getattr(RL_CONFIG, 'lr_base', RL_CONFIG.lr) or RL_CONFIG.lr)
        lr_min = float(getattr(RL_CONFIG, 'lr_min', lr_base * 0.1) or (lr_base * 0.1))
        warm = int(getattr(RL_CONFIG, 'lr_warmup_frames', 0) or 0)
        hold = int(getattr(RL_CONFIG, 'lr_hold_until_frames', 0) or 0)
        decay_end = int(getattr(RL_CONFIG, 'lr_decay_until_frames', 0) or 0)

        # Piecewise: warmup -> hold -> cosine decay -> floor
        if fc <= warm and warm > 0:
            # Linear from lr_min to lr_base
            t = fc / max(warm, 1)
            lr_now = lr_min + (lr_base - lr_min) * t
        elif fc <= hold or decay_end <= hold:
            lr_now = lr_base
        elif fc <= decay_end:
            # Cosine from lr_base to lr_min over [hold, decay_end]
            import math
            t = (fc - hold) / max(decay_end - hold, 1)
            lr_now = lr_min + 0.5 * (lr_base - lr_min) * (1 + math.cos(math.pi * t))
        else:
            lr_now = lr_min

        # Apply to optimizer groups
        for g in self.optimizer.param_groups:
            g['lr'] = float(lr_now)

    def set_training_enabled(self, enabled: bool):
        """Enable/disable training at runtime. When disabling, drain pending work."""
        self.training_enabled = bool(enabled)
        # Reflect into global metrics for consistency
        try:
            metrics.training_enabled = bool(enabled)
        except Exception:
            pass
        # If disabling, drain queue quickly to avoid stale tasks
        if not self.training_enabled:
            try:
                while not self.train_queue.empty():
                    self.train_queue.get_nowait()
                    self.train_queue.task_done()
            except Exception:
                pass

    def sync_inference_network(self):
        """Synchronize inference network weights and record telemetry."""
        did_copy = False
        try:
            if hasattr(self, 'qnetwork_inference') and self.qnetwork_inference is not self.qnetwork_local:
                with torch.no_grad():
                    state = {}
                    for k, p in self.qnetwork_local.state_dict().items():
                        state[k] = p.to(self.inference_device)
                    self.qnetwork_inference.load_state_dict(state)
                did_copy = True
        finally:
            try:
                metrics.last_inference_sync_frame = metrics.frame_count
                metrics.last_inference_sync_time = time.time()
            except Exception:
                pass
        return did_copy

    def _inference_sync_heartbeat(self):
        """Periodic sync to keep inference network fresh even with low training activity."""
        while self.running:
            try:
                now = time.time()
                last = getattr(metrics, 'last_inference_sync_time', 0.0)
                if last <= 0.0 or (now - last) >= self._sync_interval_seconds:
                    self.sync_inference_network()
                time.sleep(1.0)
            except Exception:
                time.sleep(1.0)

    def update_target_network(self):
        """Hard update target network from local and record telemetry."""
        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())
        try:
            metrics.last_target_update_frame = metrics.frame_count
            metrics.last_target_update_time = time.time()
            metrics.last_hard_target_update_frame = metrics.frame_count
            metrics.last_hard_target_update_time = time.time()
        except Exception:
            pass

    def force_hard_target_update(self):
        """Alias for explicit hard target refresh (server compatibility)."""
        self.update_target_network()
    
    def save(self, filepath):
        """Save hybrid model checkpoint with basic rate limiting"""
        is_forced_save = ("exit" in str(filepath)) or ("shutdown" in str(filepath))
        now = time.time()
        min_interval = 30.0
        if not is_forced_save and (now - self.last_save_time) < min_interval:
            return
        # Persist key training/metrics to avoid losing progress on restart
        # Respect saved_expert_ratio when override/expert_mode is active
        try:
            if metrics.expert_mode or metrics.override_expert:
                ratio_to_save = metrics.saved_expert_ratio
            else:
                ratio_to_save = metrics.expert_ratio
        except Exception:
            ratio_to_save = float(getattr(metrics, 'expert_ratio', 0.0))
        checkpoint = {
            'local_state_dict': self.qnetwork_local.state_dict(),
            'target_state_dict': self.qnetwork_target.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_steps': self.training_steps,
            'state_size': self.state_size,
            'discrete_actions': self.discrete_actions,
            'memory_size': len(self.memory),
            'architecture': 'hybrid',  # Mark as hybrid architecture
            # Metrics/state for persistence
            'frame_count': int(getattr(metrics, 'frame_count', 0)),
            'epsilon': float(getattr(metrics, 'epsilon', getattr(RL_CONFIG, 'epsilon_start', 0.1))),
            'expert_ratio': float(ratio_to_save),
            'last_decay_step': int(getattr(metrics, 'last_decay_step', 0)),
            'last_epsilon_decay_step': int(getattr(metrics, 'last_epsilon_decay_step', 0)),
            'episode_rewards': list(getattr(metrics, 'episode_rewards', []) or []),
            'dqn_rewards': list(getattr(metrics, 'dqn_rewards', []) or []),
            'expert_rewards': list(getattr(metrics, 'expert_rewards', []) or []),
        }
        torch.save(checkpoint, filepath)
        self.last_save_time = now
        if is_forced_save:
            print(f"Hybrid model saved to {filepath} (frame {getattr(metrics, 'frame_count', 0)})")
    
    def load(self, filepath):
        """Load hybrid model checkpoint"""
        from config import RESET_METRICS, SERVER_CONFIG, FORCE_FRESH_MODEL

        if FORCE_FRESH_MODEL:
            print("FORCE_FRESH_MODEL is True - skipping model load, starting fresh weights")
            return False

        if os.path.exists(filepath):
            try:
                checkpoint = torch.load(filepath, map_location=training_device)

                # Verify architecture compatibility
                if checkpoint.get('architecture') != 'hybrid':
                    print("Warning: Loading non-hybrid checkpoint into hybrid agent")
                    return False

                # Load weights (tolerate minor shape mismatches by strict=True here as it's same arch)
                self.qnetwork_local.load_state_dict(checkpoint['local_state_dict'])
                self.qnetwork_target.load_state_dict(checkpoint['target_state_dict'])
                try:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    # Ensure LR honors current config
                    for g in self.optimizer.param_groups:
                        g['lr'] = RL_CONFIG.lr
                    print(f"Optimizer state loaded. LR set to config value: {RL_CONFIG.lr}")
                except Exception:
                    print("Optimizer state not loaded (mismatch). Using fresh optimizer.")

                self.training_steps = int(checkpoint.get('training_steps', 0))

                # Sync inference network
                if inference_device != training_device:
                    self.qnetwork_inference.load_state_dict(checkpoint['local_state_dict'])

                # Sanity-check Q-values on load to catch corruption/explosion
                try:
                    with torch.no_grad():
                        dummy = torch.zeros(1, self.state_size, device=self.training_device)
                        dq, _ = self.qnetwork_local(dummy)
                        qmax = float(dq.max().item())
                        qmin = float(dq.min().item())
                        if max(abs(qmax), abs(qmin)) > 10.0:
                            print("WARNING: Loaded hybrid model has extreme Q-values. Rescaling weights...")
                            scale = 5.0 / max(abs(qmax), abs(qmin))
                            for p in self.qnetwork_local.parameters():
                                p.data.mul_(scale)
                            for p in self.qnetwork_target.parameters():
                                p.data.mul_(scale)
                            dq2, _ = self.qnetwork_local(dummy)
                            print(f"Rescaled Q-value range: [{float(dq2.min().item()):.3f}, {float(dq2.max().item()):.3f}]")
                except Exception:
                    pass

                # Restore metrics unless explicitly reset
                if RESET_METRICS:
                    print("RESET_METRICS=True: resetting epsilon/expert_ratio; leaving frame_count at 0")
                    metrics.epsilon = RL_CONFIG.epsilon_start
                    metrics.expert_ratio = RL_CONFIG.expert_ratio_start
                    metrics.last_decay_step = 0
                    metrics.last_epsilon_decay_step = 0
                    metrics.frame_count = 0
                    metrics.loaded_frame_count = 0
                    try:
                        metrics.episode_rewards.clear(); metrics.dqn_rewards.clear(); metrics.expert_rewards.clear()
                    except Exception:
                        pass
                else:
                    # Frame count and training schedule
                    metrics.frame_count = int(checkpoint.get('frame_count', 0))
                    metrics.loaded_frame_count = int(metrics.frame_count)
                    # Epsilon/Expert
                    metrics.epsilon = float(checkpoint.get('epsilon', RL_CONFIG.epsilon_start))
                    metrics.expert_ratio = float(checkpoint.get('expert_ratio', RL_CONFIG.expert_ratio_start))
                    metrics.last_decay_step = int(checkpoint.get('last_decay_step', 0))
                    metrics.last_epsilon_decay_step = int(checkpoint.get('last_epsilon_decay_step', 0))
                    # Reward histories (bounded by deque maxlen)
                    try:
                        if hasattr(metrics, 'episode_rewards'):
                            metrics.episode_rewards.clear()
                            for v in (checkpoint.get('episode_rewards', []) or [])[-metrics.episode_rewards.maxlen:]:
                                metrics.episode_rewards.append(float(v))
                        if hasattr(metrics, 'dqn_rewards'):
                            metrics.dqn_rewards.clear()
                            for v in (checkpoint.get('dqn_rewards', []) or [])[-metrics.dqn_rewards.maxlen:]:
                                metrics.dqn_rewards.append(float(v))
                        if hasattr(metrics, 'expert_rewards'):
                            metrics.expert_rewards.clear()
                            for v in (checkpoint.get('expert_rewards', []) or [])[-metrics.expert_rewards.maxlen:]:
                                metrics.expert_rewards.append(float(v))
                    except Exception:
                        pass

                    # Respect server toggles that may force resets regardless of checkpoint
                    if getattr(SERVER_CONFIG, 'reset_expert_ratio', False):
                        print(f"Resetting expert_ratio to start per RL_CONFIG: {RL_CONFIG.expert_ratio_start}")
                        metrics.expert_ratio = RL_CONFIG.expert_ratio_start
                    if getattr(SERVER_CONFIG, 'reset_epsilon', False):
                        print(f"Resetting epsilon to start per SERVER_CONFIG: {RL_CONFIG.epsilon_start}")
                        metrics.epsilon = RL_CONFIG.epsilon_start
                        # Continue decays from the current frame interval so we don't retroactively overshoot
                        try:
                            metrics.last_epsilon_decay_step = int(metrics.frame_count // RL_CONFIG.epsilon_decay_steps)
                        except Exception:
                            metrics.last_epsilon_decay_step = 0
                    if getattr(SERVER_CONFIG, 'force_expert_ratio_recalc', False):
                        print(f"Force recalculating expert_ratio based on {metrics.frame_count:,} frames...")
                        decay_expert_ratio(metrics.frame_count)
                        print(f"Expert ratio recalculated to: {metrics.expert_ratio:.4f}")
                    if getattr(SERVER_CONFIG, 'reset_frame_count', False):
                        print("Resetting frame count per SERVER_CONFIG")
                        metrics.frame_count = 0
                        metrics.loaded_frame_count = 0
                        metrics.last_decay_step = 0
                        metrics.last_epsilon_decay_step = 0

                print(f"Loaded hybrid model from {filepath}")
                print(f"  - Resuming from frame: {metrics.frame_count}")
                print(f"  - Resuming epsilon: {metrics.epsilon:.4f}")
                print(f"  - Resuming expert_ratio: {metrics.expert_ratio:.4f}")
                print(f"  - Resuming last_decay_step: {metrics.last_decay_step}")
                return True
            except Exception as e:
                print(f"Error loading hybrid checkpoint: {e}")
                return False
        else:
            print(f"No checkpoint found at {filepath}. Starting new hybrid model.")
            return False

    def stop(self, join: bool = True, timeout: float = 2.0):
        """Gracefully stop background threads and heartbeat."""
        self.running = False
        # Unblock workers by enqueueing no-op tokens
        try:
            for _ in range(max(1, self.num_training_workers)):
                self.train_queue.put_nowait(True)
        except Exception:
            pass
        if join:
            for t in self.training_threads:
                try:
                    t.join(timeout=timeout)
                except Exception:
                    pass
            try:
                self._sync_thread.join(timeout=timeout)
            except Exception:
                pass

    def get_q_value_range(self):
        """Return min/max Q-values from the discrete head over a representative batch.

        We prefer sampling the replay buffer; if insufficient data, we probe a zero state.
        """
        try:
            with torch.no_grad():
                min_q = max_q = None

                buf_len = 0
                try:
                    buf_len = len(self.memory)
                except Exception:
                    buf_len = 0

                probe_bs = 256
                min_required = max(64, min(probe_bs, getattr(RL_CONFIG, 'batch_size', 64)))

                if buf_len >= min_required:
                    sample = self.memory.sample(min(probe_bs, buf_len))
                    if sample is not None:
                        states = sample[0]
                        if isinstance(states, torch.Tensor):
                            states = states.to(self.inference_device, non_blocking=True)
                            discrete_q, _ = self.qnetwork_inference(states)
                            min_q = discrete_q.min().item()
                            max_q = discrete_q.max().item()

                if min_q is None or max_q is None:
                    dummy = torch.zeros(1, self.state_size, device=self.inference_device)
                    discrete_q, _ = self.qnetwork_inference(dummy)
                    min_q = discrete_q.min().item()
                    max_q = discrete_q.max().item()

                return float(min_q), float(max_q)
        except Exception:
            return float('nan'), float('nan')

# Thread-safe metrics storage
class SafeMetrics:
    def __init__(self, metrics):
        self.metrics = metrics
        self.lock = threading.Lock()
    
    def update_frame_count(self, delta: int = 1):
        with self.lock:
            # Update total frame count
            if delta < 1:
                delta = 1
            self.metrics.frame_count += delta
            # Track interval frames for display rate
            try:
                self.metrics.frames_count_interval += delta
            except Exception:
                pass
            
            # Update FPS tracking
            current_time = time.time()
            
            # Initialize last_fps_time if this is the first frame
            if self.metrics.last_fps_time == 0:
                self.metrics.last_fps_time = current_time
                
            # Count frames for this second
            self.metrics.frames_last_second += delta
            
            # Calculate FPS every second
            elapsed = current_time - self.metrics.last_fps_time
            if elapsed >= 1.0:
                # Calculate frames per second with more accuracy
                new_fps = self.metrics.frames_last_second / elapsed
                
                # Store the new FPS value
                self.metrics.fps = new_fps
                
                # Reset counters
                self.metrics.frames_last_second = 0
                self.metrics.last_fps_time = current_time
                
            return self.metrics.frame_count
            
    def get_epsilon(self):
        with self.lock:
            return self.metrics.epsilon

    def get_effective_epsilon(self):
        """Thread-safe access to effective epsilon (0.0 when override_epsilon is ON)."""
        with self.lock:
            try:
                return 0.0 if getattr(self.metrics, 'override_epsilon', False) else float(self.metrics.epsilon)
            except Exception:
                return float(self.metrics.epsilon)

    def get_effective_epsilon_with_state(self, gamestate: int):
        """Return effective epsilon possibly adjusted by game state.

        Currently: if GS_ZoomingDown (0x20), return epsilon * RL_CONFIG.zoom_epsilon_scale (default 0.25).
        Honors override_epsilon by returning 0.0 regardless of state.
        """
        with self.lock:
            try:
                if getattr(self.metrics, 'override_epsilon', False):
                    return 0.0
                eps = float(self.metrics.epsilon)
                if gamestate == 0x20:
                    try:
                        scale = float(getattr(RL_CONFIG, 'zoom_epsilon_scale', 0.25) or 0.25)
                    except Exception:
                        scale = 0.25
                    eps = eps * scale
                # Bound within [0,1] as an effective runtime parameter
                if eps < 0.0:
                    eps = 0.0
                elif eps > 1.0:
                    eps = 1.0
                return eps
            except Exception:
                return float(self.metrics.epsilon)
            
    def update_epsilon(self):
        with self.lock:
            self.metrics.epsilon = decay_epsilon(self.metrics.frame_count)
            return self.metrics.epsilon
            
    def update_expert_ratio(self):
        with self.lock:
            # Respect override_expert: freeze expert_ratio at 0 while override is ON
            if self.metrics.override_expert:
                return self.metrics.expert_ratio
            decay_expert_ratio(self.metrics.frame_count)
            return self.metrics.expert_ratio
    
    def add_episode_reward(self, total_reward, dqn_reward, expert_reward):
        """Record per-episode rewards in a thread-safe way.

        Forward to the underlying MetricsData.add_episode_reward when available so that:
        - All episodes (including zero/negative totals) are recorded to keep deques aligned
        - Interval accumulators (for per-row means) are updated consistently for Rwrd/DQN/Exp
        Fallback to direct appends if the underlying metrics object lacks the method.
        """
        with self.lock:
            try:
                add_fn = getattr(self.metrics, 'add_episode_reward', None)
                if callable(add_fn):
                    add_fn(float(total_reward), float(dqn_reward), float(expert_reward))
                    return
            except Exception:
                pass
            # Fallback: append directly without filtering to preserve alignment
            try:
                self.metrics.episode_rewards.append(float(total_reward))
            except Exception:
                pass
            try:
                self.metrics.dqn_rewards.append(float(dqn_reward))
            except Exception:
                pass
            try:
                self.metrics.expert_rewards.append(float(expert_reward))
            except Exception:
                pass
    
    def increment_guided_count(self):
        with self.lock:
            self.metrics.guided_count += 1
    
    def increment_total_controls(self):
        with self.lock:
            self.metrics.total_controls += 1
            
    def update_action_source(self, source):
        with self.lock:
            self.metrics.last_action_source = source
            
    def update_game_state(self, enemy_seg, open_level):
        with self.lock:
            self.metrics.enemy_seg = enemy_seg
            self.metrics.open_level = open_level
    
    def get_expert_ratio(self):
        with self.lock:
            return self.metrics.expert_ratio
            
    def is_override_active(self):
        with self.lock:
            return self.metrics.override_expert

    def get_fps(self):
        with self.lock:
            return self.metrics.fps

    # Thread-safe helpers for common aggregated metrics
    def add_inference_time(self, t: float):
        """Accumulate inference time and count in a thread-safe way."""
        try:
            dt = float(t)
        except Exception:
            dt = 0.0
        with self.lock:
            # Underlying MetricsData holds these fields
            try:
                self.metrics.total_inference_time += dt
                self.metrics.total_inference_requests += 1
            except Exception:
                # If fields are missing for any reason, initialize defensively
                try:
                    if not hasattr(self.metrics, 'total_inference_time'):
                        self.metrics.total_inference_time = 0.0
                    if not hasattr(self.metrics, 'total_inference_requests'):
                        self.metrics.total_inference_requests = 0
                    self.metrics.total_inference_time += dt
                    self.metrics.total_inference_requests += 1
                except Exception:
                    pass


def parse_frame_data(data: bytes) -> Optional[FrameData]:
    """Parse binary frame data from Lua into game state - SIMPLIFIED with float32 payload"""
    try:
        if not data or len(data) < 10:  # Minimal size check
            print("ERROR: Received empty or too small data packet", flush=True)
            sys.exit(1)
        
        # Fixed OOB header format (must match Lua exactly). Precompute once.
        # Format: ">HdBBBHHHBBBhBhBBBBBffffff"
        global _FMT_OOB, _HDR_OOB
        try:
            _FMT_OOB
        except NameError:
            _FMT_OOB = ">HdBBBHHHBBBhBhBBBBB"
            _HDR_OOB = struct.calcsize(_FMT_OOB)

        if len(data) < _HDR_OOB:
            print(f"ERROR: Received data too small: {len(data)} bytes, need {_HDR_OOB}", flush=True)
            sys.exit(1)

        values = struct.unpack(_FMT_OOB, data[:_HDR_OOB])
        (num_values, reward, gamestate, game_mode, done, frame_counter, score_high, score_low,
         save_signal, fire, zap, spinner, is_attract, nearest_enemy, player_seg, is_open,
         expert_fire, expert_zap, level_number) = values
        header_size = _HDR_OOB
        
        # Combine score components
        score = (score_high * 65536) + score_low
        
        state_data = memoryview(data)[header_size:]
        
        # SIMPLIFIED: Parse as float32 array directly (no complex normalization needed!)
        try:
            # Each float32 is 4 bytes, big-endian format
            expected_bytes = num_values * 4
            if len(state_data) < expected_bytes:
                print(f"ERROR: Expected {expected_bytes} bytes for {num_values} floats, got {len(state_data)}", flush=True)
                sys.exit(1)
                
            state = np.frombuffer(state_data, dtype='>f4', count=num_values)  # big-endian float32
            state = state.astype(np.float32)  # Ensure correct dtype
            
        except ValueError as e:
            print(f"ERROR: frombuffer failed: {e}", flush=True)
            sys.exit(1)
            
        if state.size != num_values:
            print(f"ERROR: Expected {num_values} state values but got {state.size}", flush=True)
            sys.exit(1)

        # CRITICAL: Check for NaN/Inf values that would cause training instability
        nan_count = np.sum(np.isnan(state))
        inf_count = np.sum(np.isinf(state))
        if nan_count > 0 or inf_count > 0:
            print(f"[CRITICAL] Frame {frame_counter}: {nan_count} NaN values, {inf_count} Inf values detected! "
                  f"This will cause training instability!", flush=True)

        # Ensure all values are in expected range [-1, 1] with warnings for issues
        out_of_range_count = np.sum((state < -1.001) | (state > 1.001))
        if out_of_range_count > 0:
            print(f"[WARNING] Frame {frame_counter}: {out_of_range_count} values outside [-1,1] range", flush=True)
            state = np.clip(state, -1.0, 1.0)
        
        frame_data = FrameData(
            state=state,
            reward=reward,
            action=(bool(fire), bool(zap), spinner),
            mode=game_mode,
            gamestate=gamestate,
            done=bool(done),
            attract=bool(is_attract),
            save_signal=bool(save_signal),
            enemy_seg=nearest_enemy,
            player_seg=player_seg,
            open_level=bool(is_open),
            expert_fire=bool(expert_fire),
            expert_zap=bool(expert_zap),
            level_number=level_number
        )
        
        return frame_data
    except Exception as e:
        print(f"ERROR parsing frame data: {e}", flush=True)
        sys.exit(1)

def display_metrics_header(kb=None):
    """Display header for metrics table"""
    if not IS_INTERACTIVE: return
    header = (
        f"{'Frame':>8} | {'FPS':>5} | {'Clients':>7} | {'Mean Reward':>12} | {'DQN Reward':>10} | {'Loss':>8} | "
        f"{'Epsilon':>7} | {'Guided %':>8} | {'Mem Size':>8} | {'Avg Level':>9} | {'Level Type':>10} | {'OVR':>3} | {'Expert':>6} | "
        f"{'InferSync ΔF/Δt':>16} | {'HardUpd ΔF/Δt':>17} | {'Q-Value Range':>14}"
    )
    print_with_terminal_restore(kb, f"\n{'-' * len(header)}")
    print_with_terminal_restore(kb, header)
    print_with_terminal_restore(kb, f"{'-' * len(header)}")

def display_metrics_row(agent, kb=None):
    """Display current metrics in tabular format"""
    if not IS_INTERACTIVE: return
    mean_reward = np.mean(list(metrics.episode_rewards)) if metrics.episode_rewards else float('nan')
    dqn_reward = np.mean(list(metrics.dqn_rewards)) if metrics.dqn_rewards else float('nan')
    mean_loss = np.mean(list(metrics.losses)) if metrics.losses else float('nan')
    guided_ratio = metrics.expert_ratio
    mem_size = len(agent.memory) if agent else 0
    
    # Get client count from server if available
    client_count = 0
    if hasattr(metrics, 'global_server') and metrics.global_server:
        with metrics.global_server.client_lock:
            client_count = len(metrics.global_server.clients)
            # Update the metrics.client_count value
            metrics.client_count = client_count
    
    # Determine override status
    # Display simple ON/OFF for override in its own column; Expert has its own ON/OFF column
    override_status = "ON" if metrics.override_expert else "OFF"
    
    # Display average level as 1-based instead of 0-based
    display_level = metrics.average_level + 1.0
    
    # Get Q-value range from the agent
    q_range = "N/A"
    if agent:
        try:
            min_q, max_q = agent.get_q_value_range()
            if not (np.isnan(min_q) or np.isnan(max_q)):
                q_range = f"[{min_q:.2f}, {max_q:.2f}]"
        except Exception:
            q_range = "Error"

    # Compute frames/time since last inference sync and last HARD target update
    now = time.time()
    sync_df = metrics.frame_count - getattr(metrics, 'last_inference_sync_frame', 0)
    targ_df = metrics.frame_count - getattr(metrics, 'last_hard_target_update_frame', 0)
    last_sync_time = getattr(metrics, 'last_inference_sync_time', 0.0)
    last_targ_time = getattr(metrics, 'last_hard_target_update_time', 0.0)
    sync_dt = (now - last_sync_time) if last_sync_time > 0.0 else None
    targ_dt = (now - last_targ_time) if last_targ_time > 0.0 else None
    sync_col = f"{sync_df:6d}/{(f'{sync_dt:6.1f}s' if sync_dt is not None else '   n/a')}"
    targ_col = f"{targ_df:6d}/{(f'{targ_dt:6.1f}s' if targ_dt is not None else '   n/a')}"
    
    row = (
        f"{metrics.frame_count:8d} | {metrics.fps:5.1f} | {client_count:>7} | {mean_reward:12.2f} | {dqn_reward:10.2f} | "
        f"{mean_loss:8.2f} | {metrics.epsilon:7.3f} | {guided_ratio*100:7.2f}% | "
    f"{mem_size:8d} | {display_level:9.2f} | {'Open' if metrics.open_level else 'Closed':10} | {override_status:>3} | {'ON' if metrics.expert_mode else 'OFF':>6} | "
        f"{sync_col:>16} | {targ_col:>17} | {q_range:>14}"
    )
    print_with_terminal_restore(kb, row)

def get_expert_action(enemy_seg, player_seg, is_open_level, expert_fire=False, expert_zap=False):
    """Expert policy to move toward nearest enemy with neutral tie-breaker.
    Returns (fire, zap, spinner)
    """
    # Check for INVALID_SEGMENT (-32768) or no target (-1) which indicates no valid target (like during tube transitions)
    if enemy_seg == -32768 or enemy_seg == -1:  # INVALID_SEGMENT or no target
        return expert_fire, expert_zap, 0  # Use Lua's recommendations with no movement

    # Normalize to ring indices
    enemy_seg = int(enemy_seg) % 16
    player_seg = int(player_seg) % 16

    # Compute signed shortest distance with neutral tie-break at exactly 8
    if is_open_level:
        # Open level: direct arithmetic distance; treat exact 8 as neutral tie
        relative_dist = enemy_seg - player_seg
        if abs(relative_dist) == 8:
            relative_dist = 8 if random.random() < 0.5 else -8
    else:
        clockwise = (enemy_seg - player_seg) % 16
        counter = (player_seg - enemy_seg) % 16
        if clockwise < 8:
            relative_dist = clockwise
        elif counter < 8:
            relative_dist = -counter
        else:
            # Tie (8 vs 8)
            relative_dist = 8 if random.random() < 0.5 else -8

    if relative_dist == 0:
        return expert_fire, expert_zap, 0  # No movement needed

    # Calculate intensity based on distance
    distance = abs(relative_dist)
    intensity = min(0.9, 0.3 + (distance * 0.05))  # Match Lua intensity calculation

    # For positive relative_dist (need to move clockwise), use negative spinner
    spinner = -intensity if relative_dist > 0 else intensity

    return expert_fire, expert_zap, spinner

### Legacy conversion helpers removed (pure hybrid model)

def encode_action_to_game(fire, zap, spinner):
    """Convert action values to game-compatible format.

    Spinner mapping:
    - Game expects an integer spinner command in [-32, +31]
    - We interpret 'spinner' here as a normalized float roughly in [-1.0, +1.0]
    - Encode by scaling with 32 and rounding, then clamp to the valid range
    """
    # Scale and round symmetrically with 32 to match original semantics
    try:
        sval = float(spinner)
    except Exception:
        sval = 0.0
    spinner_val = int(round(sval * 32.0))
    # Clamp to hardware range [-32, +31]
    if spinner_val > 31:
        spinner_val = 31
    elif spinner_val < -32:
        spinner_val = -32
    return int(fire), int(zap), int(spinner_val)

def fire_zap_to_discrete(fire, zap):
    """Convert fire/zap booleans to discrete action index (0-3)"""
    return int(fire) * 2 + int(zap)

def discrete_to_fire_zap(discrete_action):
    """Convert discrete action index (0-3) to fire/zap booleans"""
    discrete_action = int(discrete_action)
    fire = (discrete_action >> 1) & 1  # Extract fire bit
    zap = discrete_action & 1         # Extract zap bit
    return bool(fire), bool(zap)

def get_expert_hybrid_action(enemy_seg, player_seg, is_open_level, expert_fire=False, expert_zap=False):
    """Get expert action in hybrid format (discrete_action, continuous_spinner)
    
    Returns:
        discrete_action: int (0-3) for fire/zap combination
        continuous_spinner: float in [-0.9, +0.9] for movement (full expert range)
    """
    # Get continuous expert action
    fire, zap, spinner = get_expert_action(enemy_seg, player_seg, is_open_level, expert_fire, expert_zap)
    
    # Convert to hybrid format - preserve full expert system range!
    discrete_action = fire_zap_to_discrete(fire, zap)
    continuous_spinner = float(spinner)  # No clamping - use expert's full range
    
    return discrete_action, continuous_spinner

def hybrid_to_game_action(discrete_action, continuous_spinner):
    """Convert hybrid action to game format
    
    Args:
        discrete_action: int (0-3) for fire/zap combination
        continuous_spinner: float in [-0.3, +0.3] for movement
        
    Returns:
        fire_cmd: int (0 or 1)
        zap_cmd: int (0 or 1) 
        spinner_cmd: int (-9 to +9, scaled from spinner * 31)
    """
    fire, zap = discrete_to_fire_zap(discrete_action)
    return encode_action_to_game(fire, zap, continuous_spinner)

def legacy_action_to_hybrid(action_idx):
    """Legacy path removed. Placeholder returns no-op hybrid action."""
    return 0, 0.0

def hybrid_to_legacy_action(discrete_action, continuous_spinner):
    """Legacy path removed. Placeholder returns center legacy index (unused)."""
    return 3

def decay_epsilon(frame_count):
    """Calculate decayed exploration rate using step-based decay with adaptive floor.

    Goal: when stuck near the ~2.5 band, temporarily raise the epsilon floor to
    encourage exploration without destabilizing training.
    """
    step_interval = frame_count // RL_CONFIG.epsilon_decay_steps

    # Only decay if a new step interval is reached
    if step_interval > metrics.last_epsilon_decay_step:
        # Apply decay multiplicatively for the number of steps missed
        num_steps_to_apply = step_interval - metrics.last_epsilon_decay_step
        decay_multiplier = RL_CONFIG.epsilon_decay_factor ** num_steps_to_apply
        metrics.epsilon *= decay_multiplier

        # Update the last step tracker
        metrics.last_epsilon_decay_step = step_interval


    # Enforce floor so epsilon doesn't decay below target minimum
    try:
        floor = float(getattr(RL_CONFIG, 'epsilon_end', getattr(RL_CONFIG, 'epsilon_min', 0.0)))
    except Exception:
        floor = 0.0
    if metrics.epsilon < floor:
        metrics.epsilon = floor

    # Always return the current epsilon value (which might have just been decayed)
    return metrics.epsilon

def decay_expert_ratio(current_step):
    """Update expert ratio periodically with a performance- and slope-aware floor.

    Stronger hold when near the 2.5 plateau:
    - Hold >=50% expert until BOTH (DQN5M > 2.55) AND (slope >= +0.03)
    - Then allow >=45% expert while 2.55 < DQN5M <= 2.70 and slope >= 0.00
    - Above that, revert to configured min (e.g., 0.40)
    """
    # Skip decay if expert mode or override is active
    if metrics.expert_mode or metrics.override_expert:
        return metrics.expert_ratio
    
    # DON'T auto-initialize to start value at frame 0 - respect loaded checkpoint values
    # Only initialize if expert_ratio is somehow invalid (negative or > 1)
    if current_step == 0 and (metrics.expert_ratio < 0 or metrics.expert_ratio > 1):
        metrics.expert_ratio = RL_CONFIG.expert_ratio_start
        metrics.last_decay_step = 0
        return metrics.expert_ratio

    step_interval = current_step // RL_CONFIG.expert_ratio_decay_steps

    # Apply scheduled decay when we cross an interval boundary
    if step_interval > metrics.last_decay_step:
        steps_to_apply = step_interval - metrics.last_decay_step
        for _ in range(steps_to_apply):
            metrics.expert_ratio *= RL_CONFIG.expert_ratio_decay
        metrics.last_decay_step = step_interval

    return metrics.expert_ratio

# Discrete-only SimpleReplayBuffer removed (hybrid-only)
#!/usr/bin/env python3
"""
Configuration and shared types for Tempest AI.
"""

# Prevent direct execution
if __name__ == "__main__":
    print("This is not the main application, run 'main.py' instead")
    exit(1)

import os
import sys
import time
import threading
from dataclasses import dataclass, field
from typing import Deque, Dict, List, Optional, Tuple
from collections import deque

# Global flags
IS_INTERACTIVE = sys.stdin.isatty()

# Flag to control metric reset on load
RESET_METRICS = False  # Set to True to ignore saved epsilon/expert ratio - FRESH START
FORCE_FRESH_MODEL = False # Set to True to completely ignore saved model and start fresh

# Directory paths
MODEL_DIR = "models"
LATEST_MODEL_PATH = f"{MODEL_DIR}/tempest_model_latest.pt"

@dataclass
class ServerConfigData:
    """Configuration for socket server"""
    host: str = "0.0.0.0"  # Listen on all interfaces
    port: int = 9999
    max_clients: int = 36
    params_count: int = 175
    reset_frame_count: bool = False   # Resume from checkpoint - don't reset frame count
    reset_expert_ratio: bool = False  # Resume from checkpoint - don't reset expert ratio  
    reset_epsilon: bool = True       # Resume from checkpoint - don't reset epsilon
     
    force_expert_ratio_recalc: bool = False  # Don't force recalculation of expert ratio

# Create instance of ServerConfigData first
SERVER_CONFIG = ServerConfigData()

@dataclass
class RLConfigData:
    """Reinforcement Learning Configuration"""
    state_size: int = SERVER_CONFIG.params_count  # Use value from ServerConfigData
    # Hybrid action space: 4 discrete fire/zap combinations + 1 continuous spinner
    discrete_action_size: int = 4  # fire/zap combinations: (0,0), (1,0), (0,1), (1,1)
    continuous_action_size: int = 1  # spinner value in [-0.3, +0.3]
    # Legacy removed: discrete 18-action size (pure hybrid model)
    # Phase 1 Optimization: Larger batch + accumulation for better GPU utilization
    batch_size: int = 65536               # Increased for better GPU utilization with AMP enabled
    lr: float = 0.0025                    # PLATEAU BREAKER: Double LR from 0.0025 to escape local optimum
    gradient_accumulation_steps: int = 1  # Increased to simulate 131k effective batch for throughput
    gamma: float = 0.995                   # Reverted from 0.92 - lower gamma made plateau worse
    epsilon: float = 0.25                 # Next-run start: exploration rate (see decay schedule below)
    epsilon_start: float = 0.25           # Start at 0.20 on next run
    # Quick Win: keep a bit more random exploration while DQN catches up
    epsilon_min: float = 0.01            # Floor for exploration
    epsilon_end: float = 0.01            # Target floor
    epsilon_decay_steps: int = 10000     # Decay applied every 10k frames
    epsilon_decay_factor: float = 0.995
    # Expert guidance ratio schedule (moved here next to epsilon for unified exploration control)
    expert_ratio_start: float = 0.95      # Initial probability of expert control
    # During GS_ZoomingDown (0x20), exploration is disruptive; scale epsilon down at inference time
    zoom_epsilon_scale: float = 0.25
    expert_ratio_min: float = 0.01        # Minimum expert control probability
    expert_ratio_decay: float = 0.996     # Multiplicative decay factor per step interval
    expert_ratio_decay_steps: int = 10000 # Step interval for applying decay
    memory_size: int = 4000000           # Balanced buffer size (was 4000000)
    hidden_size: int = 256               # More moderate size - 2048 too slow for rapid experimentation
    num_layers: int = 6                  
    target_update_freq: int = 2000        # Reverted from 1000 - more frequent updates destabilized learning
    update_target_every: int = 2000       # Reverted - more frequent target updates made plateau worse
    save_interval: int = 10000            # Model save frequency
    use_noisy_nets: bool = False          # DISABLED: Use pure epsilon-greedy exploration for debugging
    # Prioritized Replay (PER) removed - hybrid-only simple replay
    # NOTE: gradient_accumulation_steps is defined above and should remain 2 for responsiveness
    use_mixed_precision: bool = True      # Enable automatic mixed precision for better performance  
    # Phase 1 Optimization: More frequent updates for faster convergence
    training_steps_per_sample: int = 8    # Increased from 12 for better sample efficiency
    training_workers: int = 4             # Reverted to 1 - multi-threaded training causes autograd conflicts
    use_torch_compile: bool = True        # ENABLED - torch.compile for loss computation (safe in single-threaded training)
    use_soft_target: bool = True          # Enable soft target updates for stability
    tau: float = 0.012                    # Slight bump for more responsive soft target tracking
    # Optional hard refresh of target net even when using soft updates
    hard_target_refresh_every_steps: int = 25000  # Slightly faster hard refresh cadence to re-anchor periodically
    # Warmup hard-refresh policy (active only while total_training_steps < warmup_until_steps)
    warmup_hard_refresh_until_steps: int = 2000
    warmup_hard_refresh_every_steps: int = 200
    # Watchdog: ensure a hard refresh happens at least every N seconds once training is active
    hard_update_watchdog_seconds: float = 3600.0     # Once per hour; rely on soft targets primarily
    # Legacy setting removed: zap_random_scale used only by legacy discrete agent
    # Modest n-step to aid credit assignment without destabilizing
    n_step: int = 3
    # Enable dueling architecture for better value/advantage separation
    use_dueling: bool = True              # ENABLED: Deeper network can benefit from dueling streams             
    # Loss function type: 'mse' for vanilla DQN, 'huber' for more robust training
    loss_type: str = 'huber'              # Use Huber for robustness to outliers
    # Gradient clipping configuration
    max_grad_norm: float = 5.0            # Clip threshold for total grad norm (L2)
    # Target clamp to stabilize bootstrapping near plateaus - DISABLED to observe natural Q-value range with gamma=0.95
    clamp_targets: bool = False           # DISABLED: Let Q-values grow naturally to detect any remaining inflation
    target_clamp_value: float = 8.0       # Value preserved for potential re-enable if needed
    # Require fresh frames after load before resuming training
    min_new_frames_after_load_to_train: int = 50000

    # Optimization: learning-rate schedule (frame-based)
    # Options: 'none', 'cosine'
    lr_schedule: str = 'cosine'
    lr_base: float = 0.001
    lr_min: float = 5e-05
    lr_warmup_frames: int = 250_000       # linear warmup from lr_min -> lr_base
    lr_hold_until_frames: int = 1_000_000 # hold at lr_base until this frame
    lr_decay_until_frames: int = 12_000_000 # cosine decay from hold -> this frame

    # Targets: use Double DQN bootstrapping for discrete head
    use_double_dqn: bool = True

    # Replay sampling bias toward most recent data (windowed uniform)
    # If bias > 0, sample this fraction from the most recent (window_frac) of the buffer
    recent_sample_bias: float = 0.5       # 0.0 disables; 0.5 = half recent, half global
    recent_window_frac: float = 0.25      # last 25% of buffer considered "recent"

    # Exploration policy: allow adaptive epsilon floor adjustments as performance improves
    adaptive_epsilon_floor: bool = True
    # Expert policy: allow adaptive expert-ratio floor adjustments based on performance trend
    adaptive_expert_floor: bool = False

    # Loss weighting: balance continuous head relative to discrete head
    continuous_loss_weight: float = 0.5

    # Reward shaping/normalization controls (to stabilize targets when external reward scale changes)
    reward_scale: float = 0.1            # Multiply incoming rewards by this factor before TD target
    reward_clamp_abs: float = 0.0        # If > 0, clamp rewards to [-reward_clamp_abs, +reward_clamp_abs]
    reward_tanh: bool = False            # If True, apply tanh to (scaled) rewards

    # Optional gradient value clamp (0.0 disables)
    max_grad_value: float = 0.0

    # Superzap gate: probability that a DQN-selected zap is actually executed (0..1)
    # Expert-directed zaps are not gated.
    superzap_prob: float = 0.01

# Create instance of RLConfigData after its definition
RL_CONFIG = RLConfigData()

@dataclass
class MetricsData:
    """Metrics tracking for training progress"""
    frame_count: int = 0
    guided_count: int = 0
    total_controls: int = 0
    episode_rewards: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    dqn_rewards: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    expert_rewards: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    losses: Deque[float] = field(default_factory=lambda: deque(maxlen=1000))
    epsilon: float = field(default_factory=lambda: RL_CONFIG.epsilon_start)
    expert_ratio: float = RL_CONFIG.expert_ratio_start
    last_decay_step: int = 0
    last_epsilon_decay_step: int = 0 # Added tracker for epsilon decay
    enemy_seg: int = -1
    open_level: bool = False
    override_expert: bool = False
    saved_expert_ratio: float = 0.75
    expert_mode: bool = False
    last_action_source: str = ""
    frames_last_second: int = 0
    last_fps_time: float = 0
    fps: float = 0.0
    client_count: int = 0
    lock: threading.Lock = field(default_factory=threading.Lock)
    total_inference_time: float = 0.0
    total_inference_requests: int = 0
    average_level: float = 0  # Average level number across all clients
    # PER-specific metrics removed
    # Gradient clipping diagnostics
    grad_clip_delta: float = 1.0   # post-clip L2 norm divided by pre-clip L2 norm (should be ~1.0 if not clipping)
    grad_norm: float = 0.0         # pre-clip gradient L2 norm magnitude for monitoring learning dynamics
    # Loss averaging since last metrics print
    loss_sum_interval: float = 0.0
    loss_count_interval: int = 0
    # Training steps since last metrics print and when last row printed
    training_steps_interval: int = 0
    last_metrics_row_time: float = 0.0
    # Frames since last metrics print
    frames_count_interval: int = 0
    
    # Training-specific metrics
    memory_buffer_size: int = 0  # Current replay buffer size
    total_training_steps: int = 0  # Total training steps completed
    last_target_update_frame: int = 0  # Frame count when target network was last updated
    last_inference_sync_frame: int = 0 # Frame count when inference net was last synced
    last_target_update_time: float = 0.0   # Wall time of last target update
    last_inference_sync_time: float = 0.0  # Wall time of last inference sync
    # Hard target update telemetry (full copy)
    last_hard_target_update_frame: int = 0
    last_hard_target_update_time: float = 0.0
    # Rolling performance metrics (from metrics_display)
    dqn5m_avg: float = 0.0  # Weighted average DQN reward across last 5M frames
    dqn5m_slopeM: float = 0.0  # Weighted regression slope per million frames
    # Frame count at load time for enforcing post-load burn-in
    loaded_frame_count: int = 0
    
    # Reward component tracking (for analysis and display)
    # State summary stats (rolling)
    state_mean: float = 0.0
    state_std: float = 0.0
    state_min: float = 0.0
    state_max: float = 0.0
    state_invalid_frac: float = 0.0  # Fraction of entries equal to -1.0 (our invalid sentinel)
    # Level averaging since last metrics print (0-based levels)
    level_sum_interval: float = 0.0
    level_count_interval: int = 0
    # Reward averaging since last metrics print
    reward_sum_interval_total: float = 0.0
    reward_count_interval_total: int = 0
    reward_sum_interval_dqn: float = 0.0
    reward_count_interval_dqn: int = 0
    reward_sum_interval_expert: float = 0.0
    reward_count_interval_expert: int = 0
    # Training enable/disable (UI toggle). When False, background workers do no training.
    training_enabled: bool = True
    # Epsilon override: when True, force epsilon=0.0 (pure greedy) regardless of other overrides
    override_epsilon: bool = False
    
    def update_frame_count(self, delta: int = 1):
        """Update frame count and FPS tracking"""
        with self.lock:
            # Update total frame count
            if delta < 1:
                delta = 1
            self.frame_count += delta
            # Track interval frames for rate calculations
            try:
                self.frames_count_interval += delta
            except Exception:
                pass
            
            # Update FPS tracking
            current_time = time.time()
            
            # Initialize last_fps_time if this is the first frame
            if self.last_fps_time == 0:
                self.last_fps_time = current_time
                
            # Count frames for this second
            self.frames_last_second += delta
            
            # Calculate FPS every second
            elapsed = current_time - self.last_fps_time
            if elapsed >= 1.0:
                # Calculate frames per second with more accuracy
                new_fps = self.frames_last_second / elapsed
                
                # Store the new FPS value
                self.fps = new_fps
                
                # Reset counters
                self.frames_last_second = 0
                self.last_fps_time = current_time
                
            return self.frame_count
    
    def get_epsilon(self):
        """Get current epsilon value"""
        with self.lock:
            return self.epsilon

    def get_effective_epsilon(self) -> float:
        """Return the epsilon value that will actually be used for action selection.

        When override_epsilon is ON, this returns 0.0 (pure greedy) regardless of other modes.
        Otherwise, returns the current decayed epsilon.
        """
        with self.lock:
            return 0.0 if self.override_epsilon else float(self.epsilon)
    
    def update_epsilon(self):
        """Update epsilon based on frame count"""
        with self.lock:
            # Import here to avoid circular imports
            from aimodel import decay_epsilon
            self.epsilon = decay_epsilon(self.frame_count)
            return self.epsilon
    
    def update_expert_ratio(self):
        """Update expert ratio based on frame count"""
        with self.lock:
            # Import here to avoid circular imports
            from aimodel import decay_expert_ratio
            # Skip decay if expert mode or override mode is active
            if self.expert_mode or self.override_expert:
                return self.expert_ratio
            decay_expert_ratio(self.frame_count)
            return self.expert_ratio
    
    def add_episode_reward(self, total_reward, dqn_reward, expert_reward):
        """Add episode rewards to tracking (include negatives/zeros for accurate means)"""
        with self.lock:
            self.episode_rewards.append(float(total_reward))
            self.dqn_rewards.append(float(dqn_reward))
            self.expert_rewards.append(float(expert_reward))
            # Track interval reward averages
            try:
                self.reward_sum_interval_total += float(total_reward)
                self.reward_count_interval_total += 1
                self.reward_sum_interval_dqn += float(dqn_reward)
                self.reward_count_interval_dqn += 1
                self.reward_sum_interval_expert += float(expert_reward)
                self.reward_count_interval_expert += 1
            except Exception:
                pass
    
    def increment_guided_count(self):
        """Increment guided count"""
        with self.lock:
            self.guided_count += 1
    
    def increment_total_controls(self):
        """Increment total controls"""
        with self.lock:
            self.total_controls += 1
    
    def update_action_source(self, source):
        """Update last action source"""
        with self.lock:
            self.last_action_source = source
    
    def update_game_state(self, enemy_seg, open_level):
        """Update game state"""
        with self.lock:
            self.enemy_seg = enemy_seg
            self.open_level = open_level
    
    def get_expert_ratio(self):
        """Get current expert ratio"""
        with self.lock:
            return self.expert_ratio
    
    def is_override_active(self):
        """Check if override is active"""
        with self.lock:
            return self.override_expert
    
    def get_fps(self):
        """Get current FPS"""
        with self.lock:
            return self.fps
    
    
    def get_reward_component_averages(self) -> Dict[str, float]:
        """Get recent averages of reward components"""
        with self.lock:
            averages = {}
            for component, history in self.reward_component_history.items():
                if history:
                    averages[component] = sum(history) / len(history)
                else:
                    averages[component] = 0.0
            return averages
    
    def toggle_override(self, kb_handler=None):
        """Toggle override mode"""
        with self.lock:
            self.override_expert = not self.override_expert
            if self.override_expert:
                self.saved_expert_ratio = self.expert_ratio
                self.expert_ratio = 0.0
            else:
                self.expert_ratio = self.saved_expert_ratio
            if kb_handler and IS_INTERACTIVE:
                # Import here to avoid circular import at top level
                from aimodel import print_with_terminal_restore
                print_with_terminal_restore(kb_handler, f"\nOverride mode: {'ON' if self.override_expert else 'OFF'}\r")
    
    def toggle_expert_mode(self, kb_handler=None):
        """Toggle expert mode"""
        with self.lock:
            self.expert_mode = not self.expert_mode
            if self.expert_mode:
                # Save current expert ratio and set to 1.0 (100%) when expert mode is ON
                self.saved_expert_ratio = self.expert_ratio
                self.expert_ratio = 1.0
            else:
                # Restore the saved expert ratio when expert mode is OFF
                self.expert_ratio = self.saved_expert_ratio
            if kb_handler and IS_INTERACTIVE:
                # Import here to avoid circular import at top level
                from aimodel import print_with_terminal_restore
                print_with_terminal_restore(kb_handler, f"\nExpert mode: {'ON' if self.expert_mode else 'OFF'}\r")

    def toggle_training_mode(self, kb_handler=None):
        """Toggle training enable/disable (does not affect data collection)."""
        with self.lock:
            self.training_enabled = not self.training_enabled
            status = 'ON' if self.training_enabled else 'OFF'
            if kb_handler and IS_INTERACTIVE:
                from aimodel import print_with_terminal_restore
                print_with_terminal_restore(kb_handler, f"\nTrain: {status}\r")

    def toggle_epsilon_override(self, kb_handler=None):
        """Toggle epsilon override. When ON, epsilon is treated as 0.0 everywhere (pure greedy)."""
        with self.lock:
            self.override_epsilon = not self.override_epsilon
            status = 'ON' if self.override_epsilon else 'OFF'
            if kb_handler and IS_INTERACTIVE:
                from aimodel import print_with_terminal_restore
                print_with_terminal_restore(kb_handler, f"\nEpsilon override: {status}\r")

# Define hybrid action space
# Discrete fire/zap combinations (4 total)
FIRE_ZAP_MAPPING = {
    0: (0, 0),  # No fire, no zap
    1: (1, 0),  # Fire, no zap  
    2: (0, 1),  # No fire, zap
    3: (1, 1),  # Fire, zap
}

# Continuous spinner range - matches expert system full range
SPINNER_MIN = -0.9
SPINNER_MAX = 0.9

# Legacy discrete action mapping removed (pure hybrid model)

# Create instances of config classes
metrics = MetricsData()

# # Import print_with_terminal_restore from metrics_display to avoid circular imports
# # # DEF print_with_terminal_restore(kb_handler, *args, **kwargs):
# # #     \"\"\"Print with terminal restore if in interactive mode\"\"\"
# # #     if IS_INTERACTIVE and kb_handler:
# # #         # Import here to avoid circular imports
# # #         from metrics_display import print_with_terminal_restore as _print
# # #         _print(*args, **kwargs)
# # #     else:
# # #         print(*args, **kwargs)
#!/usr/bin/env python3
"""
Tempest AI Main Entry Point
Coordinates the socket server, metrics display, and keyboard handling.
"""

import os
import time
import threading
from datetime import datetime
import traceback

from aimodel import (
    HybridDQNAgent, KeyboardHandler
)
from config import (
    RL_CONFIG, MODEL_DIR, LATEST_MODEL_PATH, IS_INTERACTIVE, metrics, SERVER_CONFIG
)
from metrics_display import display_metrics_header, display_metrics_row
from socket_server import SocketServer

def stats_reporter(agent, kb_handler):
    """Thread function to report stats periodically"""
    print("Starting stats reporter thread...")
    last_report = time.time()
    report_interval = 60.0  # Reduced frequency: print 1/5 as often
    
    # Display the header once at the beginning
    display_metrics_header()
    
    while True:
        try:
            current_time = time.time()
            if current_time - last_report >= report_interval:
                display_metrics_row(agent, kb_handler)
                last_report = current_time
            
            # Check if server is still running
            if metrics.global_server is None or not metrics.global_server.running:
                print("Server stopped running, exiting stats reporter")
                break
                
            time.sleep(0.1)
        except Exception as e:
            print(f"Error in stats reporter: {e}")
            traceback.print_exc()
            break

def keyboard_input_handler(agent, keyboard_handler):
    """Thread function to handle keyboard input"""
    print("Starting keyboard input handler thread...")
    
    while True:
        try:
            # Check for keyboard input
            key = keyboard_handler.check_key()
            
            if key:
                # Handle different keys
                if key == 'q':
                    print("Quit command received, shutting down...")
                    try:
                        if metrics.global_server:
                            metrics.global_server.running = False
                            metrics.global_server.stop()
                    except Exception:
                        pass
                    try:
                        if agent:
                            agent.stop(join=True)
                    except Exception:
                        pass
                    break
                elif key == 's':
                    print("Save command received, saving model...")
                    agent.save(LATEST_MODEL_PATH)
                    print(f"Model saved to {LATEST_MODEL_PATH}")
                elif key == 'o':
                    metrics.toggle_override(keyboard_handler)
                    display_metrics_row(agent, keyboard_handler)
                elif key == 'e':
                    metrics.toggle_expert_mode(keyboard_handler)
                    display_metrics_row(agent, keyboard_handler)
                elif key.lower() == 'p':
                    metrics.toggle_epsilon_override(keyboard_handler)
                    display_metrics_row(agent, keyboard_handler)
                elif key.lower() == 't':
                    metrics.toggle_training_mode(keyboard_handler)
                    # Propagate to agent
                    try:
                        agent.set_training_enabled(metrics.training_enabled)
                    except Exception:
                        pass
                    display_metrics_row(agent, keyboard_handler)
                elif key.lower() == 'h':
                    # Print the header
                    display_metrics_header()
                elif key == ' ':  # Handle space key
                    # Print only one row (no header)
                    display_metrics_row(agent, keyboard_handler)
            
            time.sleep(0.1)
        except Exception as e:
            print(f"Error in keyboard input handler: {e}")
            break

def main():
    """Main function to run the Tempest AI application"""
    # Create model directory if it doesn't exist
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
    
    # Initialize the Agent
    # Use HybridDQNAgent (4 discrete fire/zap + 1 continuous spinner)
    # Fall back to legacy DQNAgent if needed by flipping this block.
    agent = HybridDQNAgent(
        state_size=RL_CONFIG.state_size,
        discrete_actions=4,
        learning_rate=RL_CONFIG.lr,
        gamma=RL_CONFIG.gamma,
        epsilon=RL_CONFIG.epsilon,
        epsilon_min=RL_CONFIG.epsilon_min,
        memory_size=RL_CONFIG.memory_size,
        batch_size=RL_CONFIG.batch_size
    )
    
    # Load the model if it exists
    if os.path.exists(LATEST_MODEL_PATH):
        agent.load(LATEST_MODEL_PATH)
    
    # Initialize the socket server
    server = SocketServer(SERVER_CONFIG.host, SERVER_CONFIG.port, agent, metrics)
    
    # Set the global server reference in metrics
    metrics.global_server = server
    
    # Initialize client_count in metrics
    metrics.client_count = 0
    
    # Start the server in a separate thread
    server_thread = threading.Thread(target=server.start)
    server_thread.daemon = True
    server_thread.start()
    
    # Set up keyboard handler for interactive mode
    keyboard_handler = None
    if IS_INTERACTIVE:
        keyboard_handler = KeyboardHandler()
        keyboard_handler.setup_terminal()
        keyboard_thread = threading.Thread(target=keyboard_input_handler, args=(agent, keyboard_handler))
        keyboard_thread.daemon = True
        keyboard_thread.start()
    
    # Start the stats reporter in a separate thread
    stats_thread = threading.Thread(target=stats_reporter, args=(agent, keyboard_handler))
    stats_thread.daemon = True
    stats_thread.start()
    
    # Track last save time
    last_save_time = time.time()
    save_interval = 300  # 5 minutes in seconds
    
    try:
        # Keep the main thread alive
        while server.running:
            current_time = time.time()
            # Save model every 5 minutes
            if current_time - last_save_time >= save_interval:
                agent.save(LATEST_MODEL_PATH)
                last_save_time = current_time
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nKeyboard interrupt received, saving and shutting down...")
    finally:
        # Save the model before exiting
        agent.save(LATEST_MODEL_PATH)
        print("Final model state saved")
        
        # Restore terminal settings
        if IS_INTERACTIVE and keyboard_handler:
            keyboard_handler.restore_terminal()
        
        # Stop server and agent gracefully
        try:
            if server:
                server.stop()
        except Exception:
            pass
        try:
            if agent:
                agent.stop(join=True)
        except Exception:
            pass
        
        # Join server thread to avoid abrupt abort on exit
        try:
            server_thread.join(timeout=2.0)
        except Exception:
            pass
        
        print("Application shutdown complete")

if __name__ == "__main__":
    main()#!/usr/bin/env python3
"""
Metrics display for Tempest AI.
"""

# Prevent direct execution
if __name__ == "__main__":
    print("This is not the main application, run 'main.py' instead")
    exit(1)

import os
import sys
import time
import threading
import numpy as np
from typing import Optional, List, Dict, Any
from collections import deque

# Import from config.py
from config import metrics, IS_INTERACTIVE, RL_CONFIG

# Add a counter to track the number of rows printed
row_counter = 0

# Rolling window for DQN reward over last 5M frames
DQN_WINDOW_FRAMES = 5_000_000
_dqn_window = deque()  # entries: (frames_in_interval: int, dqn_reward_mean: float, frame_end: int)
_dqn_window_frames = 0
_last_frame_count_seen = None

def _update_dqn_window(mean_dqn_reward: float):
    """Update the 5M-frames rolling window with the latest interval.

    Uses the number of frames progressed since the last row as the weight.
    """
    global _dqn_window_frames, _last_frame_count_seen
    current_frame = metrics.frame_count
    # Determine frames elapsed since last sample
    if _last_frame_count_seen is None:
        delta_frames = 0
    else:
        delta_frames = max(0, current_frame - _last_frame_count_seen)
    _last_frame_count_seen = current_frame

    # If no frame progress (e.g., first row), just return without adding
    if delta_frames <= 0:
        return

    # Append new interval
    _dqn_window.append((delta_frames, float(mean_dqn_reward), int(current_frame)))
    _dqn_window_frames += delta_frames

    # Trim window to last 5M frames (may need partial trim of the oldest bucket)
    while _dqn_window and _dqn_window_frames > DQN_WINDOW_FRAMES:
        overflow = _dqn_window_frames - DQN_WINDOW_FRAMES
        oldest_frames, oldest_val, oldest_end = _dqn_window[0]
        if oldest_frames <= overflow:
            _dqn_window.popleft()
            _dqn_window_frames -= oldest_frames
        else:
            # Partially trim the oldest bucket
            kept_frames = oldest_frames - overflow
            _dqn_window[0] = (kept_frames, oldest_val, oldest_end)
            _dqn_window_frames = DQN_WINDOW_FRAMES
            break

def _compute_dqn_window_stats():
    """Compute weighted average and weighted regression slope (per million frames) for the 5M-frame window.

    Returns (avg, slope_per_million).
    """
    if not _dqn_window or _dqn_window_frames <= 0:
        return 0.0, 0.0

    # Weighted average
    w_sum = float(_dqn_window_frames)
    wy_sum = sum(fr * val for fr, val, _ in _dqn_window)
    avg = wy_sum / w_sum if w_sum > 0 else 0.0

    # Weighted linear regression slope of y vs x, with weights = frames
    # Use frame_end as x for each bucket
    wx_sum = sum(fr * x for fr, _, x in _dqn_window)
    wxx_sum = sum(fr * (x * x) for fr, _, x in _dqn_window)
    wy_sum = sum(fr * y for fr, y, _ in _dqn_window)
    wxy_sum = sum(fr * x * y for fr, y, x in _dqn_window)

    denom = (wxx_sum - (wx_sum * wx_sum) / w_sum) if w_sum > 0 else 0.0
    if denom <= 0:
        slope = 0.0
    else:
        slope = (wxy_sum - (wx_sum * wy_sum) / w_sum) / denom

    slope_per_million = slope * 1_000_000.0
    return avg, slope_per_million

def clear_screen():
    """Clear the screen and move cursor to home position"""
    if IS_INTERACTIVE:
        # Clear screen and move to home
        sys.stdout.write("\033[2J\033[H")
        sys.stdout.flush()

def print_metrics_line(message, is_header=False):
    """Print a metrics line with proper formatting"""
    global row_counter
    
    if IS_INTERACTIVE:
        if is_header:
            # For header: print at current position
            print(message)
            print("-" * len(message))  # Add separator line
            # Reset row counter when header is printed
            row_counter = 0
        else:
            # For rows: just print at current position
            print(message)
            # Increment row counter
            row_counter += 1
        sys.stdout.flush()
    else:
        print(message)

def display_metrics_header():
    """Display the header for metrics output"""
    row_counter = 0
    # clear_screen()
    
    # Header with Q-Value Range moved before Training Stats, reward components removed
    header = (
        f"{'Frame':>11} {'FPS':>6} {'Epsi':>6} {'Xprt':>6} "
        f"{'Rwrd':>6} {'DQN':>6} {'Exp':>6} {'DQN5M':>6} {'SlpM':>6} {'Loss':>10} "
        f"{'Clnt':>4} {'Levl':>5} {'OVR':>3} {'Expert':>6} {'Train':>5} "
        f"{'ISync F/T':>12} {'HardUpd F/T':>13} "
        f"{'AvgInf':>7} {'Samp/s':>8} {'Steps/s':>8} {'GradNorm':>8} {'ClipΔ':>6} {'Q-Value Range':>14} {'Training Stats':>15}"
    )
    
    print_metrics_line(header, is_header=True)

    # Initialize the timing anchor for Steps/s so the first row divides by real elapsed seconds
    try:
        now = time.time()
        with metrics.lock:
            last_t = getattr(metrics, 'last_metrics_row_time', 0.0)
            if not isinstance(last_t, (int, float)) or last_t <= 0.0:
                metrics.last_metrics_row_time = now
    except Exception:
        # If metrics.lock or attributes aren't available yet, skip init safely
        pass

def display_metrics_row(agent, kb_handler):
    """Display a row of metrics data"""
    global row_counter
    global _last_show_reward_cols
    
    # Check if we need to print the header (every 30th row)
    if row_counter > 0 and row_counter % 30 == 0:
        display_metrics_header()
    
    # Compute reward averages since last print; fallback to recent deque averages (aligned across all three)
    mean_reward = 0.0
    mean_dqn_reward = 0.0
    mean_expert_reward = 0.0
    used_fallback_aligned = False
    with metrics.lock:
        if getattr(metrics, 'reward_count_interval_total', 0) > 0:
            mean_reward = metrics.reward_sum_interval_total / max(metrics.reward_count_interval_total, 1)
        if getattr(metrics, 'reward_count_interval_dqn', 0) > 0:
            mean_dqn_reward = metrics.reward_sum_interval_dqn / max(metrics.reward_count_interval_dqn, 1)
        if getattr(metrics, 'reward_count_interval_expert', 0) > 0:
            mean_expert_reward = metrics.reward_sum_interval_expert / max(metrics.reward_count_interval_expert, 1)
        # Reset interval counters so next print is fresh
        metrics.reward_sum_interval_total = 0.0
        metrics.reward_count_interval_total = 0
        metrics.reward_sum_interval_dqn = 0.0
        metrics.reward_count_interval_dqn = 0
        metrics.reward_sum_interval_expert = 0.0
        metrics.reward_count_interval_expert = 0
    # Fallback if no interval episodes finished: compute aligned means across the same last-N episodes
    if mean_reward == 0.0 and mean_dqn_reward == 0.0 and mean_expert_reward == 0.0:
        try:
            total_q = list(metrics.episode_rewards) if metrics.episode_rewards else []
            dqn_q = list(metrics.dqn_rewards) if metrics.dqn_rewards else []
            exp_q = list(metrics.expert_rewards) if metrics.expert_rewards else []
            n = min(len(total_q), len(dqn_q), len(exp_q), 20)
            if n > 0:
                mean_reward = sum(total_q[-n:]) / float(n)
                mean_dqn_reward = sum(dqn_q[-n:]) / float(n)
                mean_expert_reward = sum(exp_q[-n:]) / float(n)
                used_fallback_aligned = True
        except Exception:
            pass
    
    # Get the latest loss value (fallback) and compute avg since last print; also compute Avg Inference time and Steps/s
    latest_loss = metrics.losses[-1] if metrics.losses else 0.0
    loss_avg = latest_loss
    avg_inference_time_ms = 0.0
    steps_per_sec = 0.0
    samples_per_sec = 0.0
    steps_per_1k_frames = 0.0
    with metrics.lock:
        # Average inference time and reset
        if metrics.total_inference_requests > 0:
            avg_inference_time_ms = (metrics.total_inference_time / metrics.total_inference_requests) * 1000
        metrics.total_inference_time = 0.0
        metrics.total_inference_requests = 0

        # Average loss since last row and reset
        if getattr(metrics, 'loss_count_interval', 0) > 0:
            loss_avg = metrics.loss_sum_interval / max(metrics.loss_count_interval, 1)
        # Reset interval accumulators
        metrics.loss_sum_interval = 0.0
        metrics.loss_count_interval = 0

        # Steps/s: compute using time elapsed since last row
        now = time.time()
        last_t = getattr(metrics, 'last_metrics_row_time', 0.0)
        elapsed = now - last_t if last_t > 0.0 else None
        steps_int = int(getattr(metrics, 'training_steps_interval', 0))
        frames_int = int(getattr(metrics, 'frames_count_interval', 0))
        if elapsed and elapsed > 0:
            steps_per_sec = steps_int / elapsed
        else:
            steps_per_sec = float(steps_int)
        # Samples/s reflects batch_size * Steps/s; this makes batch size changes visible
        try:
            samples_per_sec = steps_per_sec * float(RL_CONFIG.batch_size)
        except Exception:
            samples_per_sec = 0.0
        # Interval Steps/1kF using the same interval counts
        denom_frames = max(1, frames_int)
        steps_per_1k_frames = (steps_int * 1000.0) / float(denom_frames)
        # Reset intervals and update last row time
        metrics.training_steps_interval = 0
        metrics.frames_count_interval = 0
        metrics.last_metrics_row_time = now
    
    # Average level since last print, default to current snapshot; display as 1-based
    display_level = metrics.average_level + 1.0
    with metrics.lock:
        if getattr(metrics, 'level_count_interval', 0) > 0:
            avg_level_interval = metrics.level_sum_interval / max(metrics.level_count_interval, 1)
            display_level = avg_level_interval + 1.0
        # Reset interval accumulators
        metrics.level_sum_interval = 0.0
        metrics.level_count_interval = 0

    # Update 5M-frame DQN window stats now that we have this row's mean_dqn_reward
    try:
        _update_dqn_window(mean_dqn_reward)
        dqn5m_avg, dqn5m_slopeM = _compute_dqn_window_stats()
    except Exception:
        dqn5m_avg, dqn5m_slopeM = 0.0, 0.0

    # Publish DQN5M stats to global metrics for gating logic elsewhere
    try:
        with metrics.lock:
            metrics.dqn5m_avg = float(dqn5m_avg)
            metrics.dqn5m_slopeM = float(dqn5m_slopeM)
    except Exception:
        pass

    # steps_per_1k_frames already computed above using the same interval counts
    
    # Calculate frames since last target update  
    frames_since_target_update = metrics.frame_count - metrics.last_target_update_frame
    
    # Format training stats: MemK/Steps/StepsPer1kF/TargetAge
    mem_k = getattr(metrics, 'memory_buffer_size', 0) // 1000
    training_stats = f"{mem_k}k/{metrics.total_training_steps}/{steps_per_1k_frames:.1f}/{frames_since_target_update//1000}k"

    # Get Q-value range from the agent
    q_range = "N/A"
    if agent:
        try:
            min_q, max_q = agent.get_q_value_range()
            if not (np.isnan(min_q) or np.isnan(max_q)):
                q_range = f"[{min_q:.2f}, {max_q:.2f}]"
        except Exception:
            q_range = "Error"

    # Compute frames/time since last inference sync and last target update
    now = time.time()
    # Frames since
    sync_df = metrics.frame_count - getattr(metrics, 'last_inference_sync_frame', 0)
    targ_df = metrics.frame_count - getattr(metrics, 'last_hard_target_update_frame', 0)

    # Seconds since (guard against unset timestamps which default to 0.0)
    last_sync_time = getattr(metrics, 'last_inference_sync_time', 0.0)
    last_targ_time = getattr(metrics, 'last_hard_target_update_time', 0.0)
    sync_dt = (now - last_sync_time) if last_sync_time > 0.0 else None
    targ_dt = (now - last_targ_time) if last_targ_time > 0.0 else None
    sync_col = f"{sync_df//1000}k/{(f'{sync_dt:>4.1f}s' if sync_dt is not None else 'n/a'):>6}"
    targ_col = f"{targ_df//1000}k/{(f'{targ_dt:>4.1f}s' if targ_dt is not None else 'n/a'):>6}"

    # Base row text with Q-Value Range moved before Training Stats, reward components removed
    # Show effective epsilon (0.00 when epsilon override is ON)
    try:
        effective_eps = metrics.get_effective_epsilon()
    except Exception:
        effective_eps = metrics.epsilon

    row = (
        f"{metrics.frame_count:>11,} {metrics.fps:>6.1f} {effective_eps:>6.2f} "
    f"{metrics.expert_ratio*100:>5.1f}% {mean_reward:>6.2f} {mean_dqn_reward:>6.2f} {mean_expert_reward:>6.2f} "
        f"{dqn5m_avg:>6.2f} {dqn5m_slopeM:>6.2f} {loss_avg:>10.6f} "
    f"{metrics.client_count:04d} {display_level:>5.1f} "
        f"{'ON' if metrics.override_expert else 'OFF':>3} "
        f"{'ON' if metrics.expert_mode else 'OFF':>6} "
        f"{'ON' if metrics.training_enabled else 'OFF':>5} "
        f"{sync_col:>12} {targ_col:>13} "
        f"{avg_inference_time_ms:>7.2f} "
        f"{samples_per_sec:>8.0f} "
        f"{steps_per_sec:>8.1f} "
        f"{metrics.grad_norm:>8.3f} "
        f"{metrics.grad_clip_delta:>6.3f} "
        f"{q_range:>14} {training_stats:>15}"
    )
    
    print_metrics_line(row)

def run_stats_reporter(metrics):
    """Run the stats reporter in a loop"""
    display_metrics_header()
    
    while True:
        try:
            time.sleep(1)  # Update every second
            display_metrics_row(None, None)
        except Exception as e:
            print(f"Error in stats reporter: {e}")
            time.sleep(1)  # Wait a bit before retrying#!/usr/bin/env python3
"""
Lightweight n-step return preprocessor used by DQNAgent.
This module is intentionally dependency-light to make unit tests fast.
"""
from collections import deque
from typing import Deque, List, Tuple
import numpy as np


class NStepReplayBuffer:
    """
    Sliding-window n-step return preprocessor.
    - add(s, a, r, s_next, done) returns a list of 0..n experiences to push into the main replay buffer.
    - On each step (non-terminal), when we have at least n items, we emit exactly one matured experience.
    - On terminal, we flush the remaining tail so no transitions are lost across episode boundaries.
    Contract:
      Input: (state, action, reward, next_state, done)
      Output: List[Tuple(state, action, R_n, next_state_n, done_n)]
    """
    def __init__(self, n_step: int, gamma: float, store_aux_action: bool = False):
        assert n_step >= 1
        self.n_step = int(n_step)
        self.gamma = float(gamma)
        # When store_aux_action=True, we will store an extra per-step auxiliary action
        # (e.g., a continuous action) alongside the discrete action and return it in outputs.
        self.store_aux_action = bool(store_aux_action)
        self._deque: Deque[Tuple] = deque()

    def reset(self):
        self._deque.clear()

    def _make_experience_from_start(self):
        R = 0.0
        done_flag = False
        last_next_state = None
        if self.store_aux_action:
            s0, a0, aux0, _, _, _ = self._deque[0]
        else:
            s0, a0, _, _, _ = self._deque[0]

        for i in range(self.n_step):
            if i >= len(self._deque):
                break
            if self.store_aux_action:
                _, _, _, r, ns, d = self._deque[i]
            else:
                _, _, r, ns, d = self._deque[i]
            R += (self.gamma ** i) * float(r)
            last_next_state = ns
            if d:
                done_flag = True
                break

        assert last_next_state is not None
        if self.store_aux_action:
            return (s0, a0, aux0, R, last_next_state, done_flag)
        else:
            return (s0, a0, R, last_next_state, done_flag)

    def add(self, state, action, reward, next_state, done, aux_action=None):
        # Normalize action to int
        try:
            if isinstance(action, np.ndarray):
                a_idx = int(action.reshape(-1)[0])
            elif isinstance(action, (list, tuple)):
                a_idx = int(action[0])
            else:
                a_idx = int(action)
        except Exception:
            a_idx = int(action)

        if self.store_aux_action:
            self._deque.append((state, a_idx, float(aux_action) if aux_action is not None else 0.0,
                                float(reward), next_state, bool(done)))
        else:
            self._deque.append((state, a_idx, float(reward), next_state, bool(done)))

        outputs: List[Tuple] = []

        if not done:
            if len(self._deque) >= self.n_step:
                outputs.append(self._make_experience_from_start())
                self._deque.popleft()
        else:
            while len(self._deque) > 0:
                outputs.append(self._make_experience_from_start())
                self._deque.popleft()

        return outputs
#!/usr/bin/env python3
"""
Socket server for Tempest AI (hybrid-only).
Bridges Lua frames to a HybridDQNAgent (4 discrete fire/zap + 1 continuous spinner).
"""

# Prevent direct execution
if __name__ == "__main__":
    print("This is not the main application, run 'main.py' instead")
    exit(1)

import os
import sys
import time
import socket
import select
import struct
import threading
import traceback
import random
import errno
from collections import deque

import numpy as np

from aimodel import (
    parse_frame_data,
    get_expert_action,
    hybrid_to_game_action,
    fire_zap_to_discrete,
    SafeMetrics,
)
from nstep_buffer import NStepReplayBuffer
from config import RL_CONFIG, SERVER_CONFIG, metrics, LATEST_MODEL_PATH


class SocketServer:
    def __init__(self, host, port, agent, metrics_wrapper):
        self.host = host
        self.port = port
        self.agent = agent
        # Always wrap with SafeMetrics for thread-safe updates
        self.metrics = SafeMetrics(metrics_wrapper)

        self.server_socket = None
        self.running = False
        self.shutdown_event = threading.Event()

        # client_id -> thread
        self.clients = {}
        # client_id -> state dict
        self.client_states = {}
        self.client_lock = threading.Lock()

        # Action distribution stats
        self.expert_action_counts = np.zeros(16, dtype=np.int64)  # 4 discrete * 4 spinner buckets (diagnostic only)
        self.dqn_action_counts = np.zeros(16, dtype=np.int64)

    def _server_nstep_enabled(self) -> bool:
        """Decide whether this server should perform n-step preprocessing.

        Rules:
        - If RL_CONFIG.n_step <= 1: no server-side n-step.
        - If the agent already exposes its own n_step_buffer (non-None), let the agent handle n-step.
        - Otherwise, perform server-side n-step (typical for HybridDQNAgent).
        """
        try:
            n = int(getattr(RL_CONFIG, 'n_step', 1) or 1)
        except Exception:
            n = 1
        if n <= 1:
            return False
        # If the agent has its own n-step buffer, skip server-side n-step to avoid double application
        if hasattr(self.agent, 'n_step_buffer') and getattr(self.agent, 'n_step_buffer') is not None:
            return False
        return True

    def start(self):
        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_socket.bind((self.host, self.port))
        self.server_socket.listen(SERVER_CONFIG.max_clients)
        self.server_socket.setblocking(False)

        self.running = True
        print(f"SocketServer listening on {self.host}:{self.port}")

        try:
            while self.running and not self.shutdown_event.is_set():
                # select may raise or the socket may be closed concurrently during shutdown; handle gracefully
                try:
                    readable, _, _ = select.select([self.server_socket], [], [], 0.05)
                except (OSError, ValueError) as e:
                    # If we're shutting down, break quietly
                    if self.shutdown_event.is_set() or not self.running:
                        break
                    # If the server socket was closed, EBADF/EINVAL are expected; exit loop silently
                    if isinstance(e, OSError) and getattr(e, 'errno', None) in (errno.EBADF, errno.EINVAL):
                        break
                    # Otherwise, re-raise to outer handler
                    raise

                if not self.server_socket:
                    break

                if self.server_socket in readable:
                    try:
                        client_socket, addr = self.server_socket.accept()
                    except OSError as e:
                        # During shutdown accept may see EBADF/EINVAL; exit loop quietly
                        if (self.shutdown_event.is_set() or not self.running) and getattr(e, 'errno', None) in (errno.EBADF, errno.EINVAL):
                            break
                        # EAGAIN/EWOULDBLOCK can happen with non-blocking sockets; continue loop
                        if getattr(e, 'errno', None) in (errno.EAGAIN, errno.EWOULDBLOCK):
                            continue
                        raise
                    client_id = self._allocate_client_id()
                    self._init_client_state(client_id)
                    t = threading.Thread(target=self.handle_client, args=(client_socket, client_id), daemon=True)
                    with self.client_lock:
                        self.clients[client_id] = t
                    t.start()
        except Exception as e:
            # Suppress noisy tracebacks if we're shutting down intentionally
            if not (self.shutdown_event.is_set() or not self.running):
                print(f"Server loop error: {e}")
                traceback.print_exc()
        finally:
            self.stop()

    def stop(self):
        self.running = False
        self.shutdown_event.set()
        try:
            if self.server_socket:
                try:
                    self.server_socket.shutdown(socket.SHUT_RDWR)
                except Exception:
                    pass
                self.server_socket.close()
                self.server_socket = None
        except Exception:
            pass

    def _allocate_client_id(self):
        with self.client_lock:
            existing = set(self.clients.keys())
            cid = 0
            while cid in existing:
                cid += 1
            return cid

    def _init_client_state(self, client_id):
        with self.client_lock:
            self.client_states[client_id] = {
                'frames_processed': 0,
                'frames_processed_since_stats': 0,
                'last_frame_time': time.time(),
                'fps': 0.0,
                'level_number': 0,
                'prev_frame': None,
                'current_frame': None,
                'last_state': None,
                'last_action_hybrid': None,  # (discrete, continuous)
                'last_action_source': None,
                'total_reward': 0.0,
                'episode_dqn_reward': 0.0,
                'episode_expert_reward': 0.0,
                'was_done': False,
                # Only create an n-step buffer if the server is responsible for n-step preprocessing
                'nstep_buffer': (
                    NStepReplayBuffer(RL_CONFIG.n_step, RL_CONFIG.gamma, store_aux_action=True)
                    if self._server_nstep_enabled() else None
                )
            }
            metrics.client_count = len(self.client_states)

    def handle_client(self, client_socket, client_id):
        try:
            client_socket.setblocking(False)
            client_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
            client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 65536)
            client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)

            buffer_size = 32768

            # handshake: expect 2 bytes quickly
            try:
                client_socket.setblocking(True)
                client_socket.settimeout(5.0)
                ping = client_socket.recv(2)
                if not ping or len(ping) < 2:
                    raise ConnectionError("No initial ping header")
            finally:
                client_socket.setblocking(False)
                client_socket.settimeout(None)

            METRICS_BATCH = 8
            local_frame_accum = 0

            while self.running and not self.shutdown_event.is_set():
                ready = select.select([client_socket], [], [], 0.0)
                if not ready[0]:
                    time.sleep(0.0005)
                    continue

                # read length
                length_data = client_socket.recv(2)
                if not length_data or len(length_data) < 2:
                    raise ConnectionError("Failed to read length")
                data_length = struct.unpack('>H', length_data)[0]

                # read payload
                data = b''
                remaining = data_length
                while remaining > 0:
                    chunk = client_socket.recv(min(buffer_size, remaining))
                    if not chunk:
                        raise ConnectionError("Connection broken during receive")
                    data += chunk
                    remaining -= len(chunk)

                # quick param-count sanity
                if len(data) >= 2:
                    num_values_received = struct.unpack('>H', data[:2])[0]
                    if num_values_received != SERVER_CONFIG.params_count:
                        print(f"Client {client_id}: Param mismatch {num_values_received} != {SERVER_CONFIG.params_count}")
                        break
                else:
                    print(f"Client {client_id}: Data too short for param count")
                    break

                frame = parse_frame_data(data)
                if not frame:
                    client_socket.sendall(struct.pack('bbb', 0, 0, 0))
                    continue

                # update per-client state
                with self.client_lock:
                    if client_id not in self.client_states:
                        break
                    state = self.client_states[client_id]
                    state['frames_processed'] += 1
                    state['level_number'] = frame.level_number
                    state['prev_frame'] = state.get('current_frame')
                    state['current_frame'] = frame
                    # No per-level frame tracking needed for probabilistic zap gate
                    now = time.time()
                    elapsed = now - state['last_frame_time']
                    if elapsed >= 1.0:
                        state['fps'] = 1.0 / elapsed
                        state['last_frame_time'] = now

                # global metrics batching
                local_frame_accum += 1
                if local_frame_accum >= METRICS_BATCH:
                    current_frame = self.metrics.update_frame_count(delta=local_frame_accum)
                    local_frame_accum = 0
                    self.metrics.update_epsilon()
                    self.metrics.update_expert_ratio()
                    # Update average level across clients periodically
                    try:
                        self.calculate_average_level()
                    except Exception:
                        pass
                self.metrics.update_game_state(frame.enemy_seg, frame.open_level)

                # N-step experience processing (server-side only when enabled) or direct 1-step fallback
                if state.get('last_state') is not None and state.get('last_action_hybrid') is not None:
                    da, ca = state['last_action_hybrid']

                    if self._server_nstep_enabled() and state.get('nstep_buffer') is not None:
                        # Add experience to n-step buffer and get matured experiences
                        experiences = state['nstep_buffer'].add(
                            state['last_state'],
                            int(da),
                            frame.reward,
                            frame.state,
                            frame.done,
                            aux_action=float(ca)
                        )

                        # Push all matured experiences to agent
                        if self.agent and experiences:
                            for item in experiences:
                                try:
                                    # Handle both shapes depending on store_aux_action flag
                                    if len(item) == 6:
                                        exp_state, exp_action, exp_continuous, exp_reward, exp_next_state, exp_done = item
                                        self.agent.step(exp_state, exp_action, exp_continuous, exp_reward, exp_next_state, exp_done)
                                    else:
                                        exp_state, exp_action, exp_reward, exp_next_state, exp_done = item
                                        # For legacy agents without continuous action
                                        self.agent.step(exp_state, exp_action, exp_reward, exp_next_state, exp_done)
                                except TypeError:
                                    pass
                    else:
                        # Server is not handling n-step: push single-step transition directly to the agent
                        try:
                            if self.agent:
                                # Hybrid agents expect (state, discrete, continuous, reward, next_state, done)
                                self.agent.step(state['last_state'], int(da), float(ca), float(frame.reward), frame.state, bool(frame.done))
                        except TypeError:
                            # Fallback for agents without continuous action in signature
                            try:
                                self.agent.step(state['last_state'], int(da), float(frame.reward), frame.state, bool(frame.done))
                            except Exception:
                                pass

                    # reward accounting
                    state['total_reward'] = state.get('total_reward', 0.0) + frame.reward
                    src = state.get('last_action_source')
                    if src == 'dqn':
                        state['episode_dqn_reward'] = state.get('episode_dqn_reward', 0.0) + frame.reward
                    elif src == 'expert':
                        state['episode_expert_reward'] = state.get('episode_expert_reward', 0.0) + frame.reward

                # terminal handling
                if frame.done:
                    if not state.get('was_done', False):
                        self.metrics.add_episode_reward(
                            state.get('total_reward', 0.0),
                            state.get('episode_dqn_reward', 0.0),
                            state.get('episode_expert_reward', 0.0)
                        )
                    state['was_done'] = True

                    # Reset n-step buffer only if server-side n-step is enabled
                    try:
                        if self._server_nstep_enabled() and state.get('nstep_buffer') is not None:
                            state['nstep_buffer'].reset()
                    except Exception:
                        pass

                    # confirm done
                    try:
                        client_socket.sendall(struct.pack('bbb', 0, 0, 0))
                    except Exception:
                        break

                    # reset for next episode
                    state['last_state'] = None
                    state['last_action_hybrid'] = None
                    try:
                        state['nstep_buf'].clear()
                    except Exception:
                        pass
                    state['total_reward'] = 0.0
                    state['episode_dqn_reward'] = 0.0
                    state['episode_expert_reward'] = 0.0
                    continue

                elif state.get('was_done', False):
                    state['was_done'] = False
                    state['total_reward'] = 0.0
                    state['episode_dqn_reward'] = 0.0
                    state['episode_expert_reward'] = 0.0
                    # No per-level frame tracking

                # choose action (hybrid-only)
                self.metrics.increment_total_controls()

                discrete_action, continuous_spinner = 0, 0.0
                action_source = 'unknown'

                if self.agent:
                    # expert vs dqn mixture with override forcing pure dqn
                    expert_ratio = self.metrics.get_expert_ratio()
                    if frame.gamestate == 0x20:  # GS_ZoomingDown
                        expert_ratio *= 2
                    use_expert = (random.random() < expert_ratio) and (not self.metrics.is_override_active())

                    if use_expert:
                        fire, zap, spin = get_expert_action(
                            frame.enemy_seg,
                            frame.player_seg,
                            frame.open_level,
                            frame.expert_fire,
                            frame.expert_zap,
                        )
                        discrete_action = fire_zap_to_discrete(fire, zap)
                        continuous_spinner = float(spin)
                        action_source = 'expert'
                    else:
                        # Epsilon policy: when override_epsilon is ON, force 0.0 (pure greedy).
                        # Otherwise, always use the current decayed epsilon even during expert/inference overrides.
                        try:
                            # SafeMetrics may or may not expose get_effective_epsilon; fall back to metrics method
                            if hasattr(self.metrics, 'get_effective_epsilon'):
                                epsilon = float(self.metrics.get_effective_epsilon())
                            elif hasattr(self.metrics, 'metrics') and hasattr(self.metrics.metrics, 'get_effective_epsilon'):
                                with self.metrics.lock:
                                    epsilon = float(self.metrics.metrics.get_effective_epsilon())
                            else:
                                epsilon = float(self.metrics.get_epsilon())
                        except Exception:
                            epsilon = float(self.metrics.get_epsilon())

                        # Reduce random exploration while zooming: use a fraction of the current epsilon
                        try:
                            if frame.gamestate == 0x20:  # GS_ZoomingDown
                                scale = float(getattr(RL_CONFIG, 'zoom_epsilon_scale', 0.25) or 0.25)
                                epsilon = epsilon * scale
                                # Clamp to sane [0,1] bounds; intentionally allow below epsilon_min as this is an effective runtime scale
                                if epsilon < 0.0:
                                    epsilon = 0.0
                                elif epsilon > 1.0:
                                    epsilon = 1.0
                        except Exception:
                            pass
                        start_t = time.perf_counter()
                        da, ca = self.agent.act(frame.state, epsilon)
                        infer_t = time.perf_counter() - start_t
                        # Record inference timing via SafeMetrics API
                        if hasattr(self.metrics, 'add_inference_time'):
                            self.metrics.add_inference_time(infer_t)
                        else:
                            # Fallback: best-effort direct update with lock if exposed
                            try:
                                with self.metrics.lock:
                                    self.metrics.total_inference_time += infer_t
                                    self.metrics.total_inference_requests += 1
                            except Exception:
                                pass
                        discrete_action, continuous_spinner = int(da), float(ca)
                        # Probabilistic superzap gate for DQN actions: allow zap with probability superzap_prob
                        try:
                            pzap = float(getattr(RL_CONFIG, 'superzap_prob', 0.01))
                        except Exception:
                            pzap = 0.01
                        try:
                            # If DQN chose a zap (bit0==1), keep it only with probability pzap
                            if (discrete_action & 1) == 1:
                                if random.random() >= max(0.0, min(1.0, pzap)):
                                    discrete_action = (discrete_action & 2)  # clear zap bit, preserve fire
                        except Exception:
                            pass
                        action_source = 'dqn'
                else:
                    action_source = 'none'

                # store for next step
                state['last_state'] = frame.state
                state['last_action_hybrid'] = (discrete_action, continuous_spinner)
                state['last_action_source'] = action_source

                # send to game
                game_fire, game_zap, game_spinner = hybrid_to_game_action(discrete_action, continuous_spinner)
                try:
                    client_socket.sendall(struct.pack('bbb', game_fire, game_zap, game_spinner))
                except Exception:
                    break

                # maintenance
                if (client_id == 0 and 'current_frame' in locals() and self.agent
                        and current_frame % RL_CONFIG.update_target_every == 0):
                    if not getattr(RL_CONFIG, 'use_soft_target', True):
                        if hasattr(self.agent, 'force_hard_target_update'):
                            self.agent.force_hard_target_update()
                        elif hasattr(self.agent, 'update_target_network'):
                            self.agent.update_target_network()

                if client_id == 0 and 'current_frame' in locals() and self.agent and current_frame % RL_CONFIG.save_interval == 0:
                    try:
                        self.agent.save(LATEST_MODEL_PATH)
                    except Exception:
                        pass

        except Exception as e:
            print(f"Error handling client {client_id}: {e}")
            traceback.print_exc()
        finally:
            try:
                client_socket.shutdown(socket.SHUT_RDWR)
            except Exception:
                pass
            try:
                client_socket.close()
            except Exception:
                pass

            with self.client_lock:
                if client_id in self.client_states:
                    del self.client_states[client_id]
                if client_id in self.clients:
                    self.clients[client_id] = None
                metrics.client_count = len([c for c in self.clients.values() if c is not None])

            threading.Timer(1.0, self.cleanup_disconnected_clients).start()

    def cleanup_disconnected_clients(self):
        cleaned = 0
        with self.client_lock:
            to_delete = [cid for cid, t in self.clients.items() if t is None]
            for cid in to_delete:
                del self.clients[cid]
                cleaned += 1
            if cleaned:
                metrics.client_count = len(self.clients)

    def calculate_average_level(self):
        with self.client_lock:
            # Include level 0 (first level) as a valid value; exclude only negatives/uninitialized
            valid = [s.get('level_number', 0) for s in self.client_states.values() if s.get('level_number', 0) >= 0]
            if valid:
                avg = sum(valid) / len(valid)
                # Update the global metrics object so display picks it up
                metrics.average_level = avg
                try:
                    metrics.level_sum_interval += float(avg)
                    metrics.level_count_interval += 1
                except Exception:
                    pass
                return avg
            else:
                metrics.average_level = 0
                return 0

    def is_override_active(self):
        with self.client_lock:
            return self.metrics.override_expert

    def get_fps(self):
        with self.client_lock:
            return self.metrics.fps

    # Internal helpers
--[[
    display.lua
    Handles the on-screen display logic for the Tempest AI MAME script.
--]]

local M = {} -- Module table for export

-- Constants (Copied from main.lua's original display logic context)
local INVALID_SEGMENT = -32768

-- Helper function to format segment values for display
local function format_segment(value)
    if value == INVALID_SEGMENT then
        return "---"
    else
        -- Use %+03d: sign, pad with 0 to width 2 (total 3 chars like +01, -07)
        return string.format("%+03d", value)
    end
end

-- Helper function to format a fixed-width segment value for our enemy tables
local function format_enemy_segment(value)
    -- Display as '---' when zero/absent (matches prior convention)
    if value == nil or value == 0 then
        return string.format("%7s", "---")
    end

    -- Use integer format for true integers; otherwise print as signed float with 2 decimals
    if type(value) == "number" then
        if value == math.floor(value) then
            -- Pad to a fixed width for column alignment
            return string.format("%7s", string.format("%+03d", value))
        else
            -- Two-decimal float, signed, then pad to fixed width
            return string.format("%7s", string.format("%+.2f", value))
        end
    end

    -- Fallback to tostring padded
    return string.format("%7s", tostring(value))
end

-- Helper to format a segment with a fixed overall width (including sign)
local function format_segment_wide(value, width)
    width = width or 6
    if value == INVALID_SEGMENT or value == nil then
        return string.format("%" .. width .. "s", "---")
    else
        -- Right-align the signed integer within the given width
        return string.format("%" .. width .. "s", string.format("%+d", value))
    end
end

-- Function to format section for display
local function format_section(title, metrics)
    local width = 80 -- Adjusted width for standard terminal
    local title_padding = math.floor((width - #title - 4) / 2)
    local separator = string.rep("-", width)
    local result = string.format("%s--[ %s ]%s\n", string.rep("-", title_padding), title, string.rep("-", width - title_padding - #title - 4))


    -- Find the longest key for alignment
    local max_key_length = 0
    for key, _ in pairs(metrics) do
        max_key_length = math.max(max_key_length, string.len(key))
    end

    -- Format each metric
    local metric_lines = {}
    local sorted_keys = {}
    for key, _ in pairs(metrics) do table.insert(sorted_keys, key) end
    table.sort(sorted_keys) -- Sort keys alphabetically

    for _, key in ipairs(sorted_keys) do
         local value = metrics[key]
         table.insert(metric_lines, string.format("  %-" .. max_key_length .. "s : %s", key, tostring(value)))
    end

    return result .. table.concat(metric_lines, "\n") .. "\n" .. separator .. "\n"
end


-- Function to move the cursor to a specific row (using ANSI escape code)
local function move_cursor_to_row(row)
    io.write(string.format("\027[%d;1H", row)) -- Use 1H for column 1
end

-- Main display update function (exported as M.update)
-- Signature matches the call in the original main.lua frame_callback
function M.update(status_message, game_state, level_state, player_state, enemies_state, num_values, last_reward) -- Note: Original didn't pass total_bytes_sent

    move_cursor_to_row(1) -- Move to top-left corner

    local display_str = ""

    -- Game State Section
    local game_metrics = {
        ["Status"] = status_message,
        ["Gamestate"] = string.format("0x%02X", game_state.gamestate),
        ["Game Mode"] = string.format("0x%02X (%s)", game_state.game_mode, (game_state.game_mode & 0x80 == 0) and "Attract" or "Play"),
        ["Countdown"] = string.format("0x%02X", game_state.countdown_timer),
        ["Credits"] = game_state.credits,
        ["P1 Lives"] = game_state.p1_lives,
        ["P1 Level"] = game_state.p1_level,
        ["Frame"] = game_state.frame_counter,
        ["FPS"] = string.format("%.1f", game_state.current_fps),
        ["Data Size"] = string.format("%d vals", num_values),
        -- ["Bytes Sent"] = total_bytes_sent, -- total_bytes_sent is not passed in the original call
        ["Last Reward"] = string.format("%.1f", last_reward), -- Show reward with decimals
    }
    display_str = display_str .. format_section("Game State", game_metrics)

    -- Player State Section
    local player_metrics = {
        ["Position"] = string.format("%d (Seg %d)", player_state.position, player_state.position & 0x0F),
        ["State"] = string.format("0x%02X", player_state.player_state),
        ["Depth"] = string.format("0x%02X", player_state.player_depth),
        ["Alive"] = (player_state.alive == 1) and "Yes" or "No",
        ["Score"] = player_state.score,
        ["Zapper Uses"] = player_state.superzapper_uses,
        ["Zapper Active"] = (player_state.superzapper_active ~= 0) and string.format("Yes (%d)", player_state.superzapper_active) or "No", -- Show countdown
        ["Shot Count"] = player_state.shot_count,
        ["Debounce"] = string.format("0x%02X", player_state.debounce),
        ["Fire Detect"] = player_state.fire_detected,
        ["Zap Detect"] = player_state.zap_detected,
        ["Spinner Accum"] = player_state.SpinnerAccum,
        ["Spinner Cmd"] = player_state.spinner_commanded,
        ["Spinner Detect"] = player_state.spinner_detected,
    }
    display_str = display_str .. format_section("Player State", player_metrics)

    -- Player Shots
    local shots_pos_str = ""
    local shots_seg_str = ""
    for i = 1, 8 do
        shots_pos_str = shots_pos_str .. string.format(" %02X", player_state.shot_positions[i])
        shots_seg_str = shots_seg_str .. " " .. format_segment(player_state.shot_segments[i])
    end
    display_str = display_str .. "Player Shots Pos:" .. shots_pos_str .. "\n"
    display_str = display_str .. "Player Shots Seg:" .. shots_seg_str .. "\n\n"

    -- Level State Section
    local level_metrics = {
         ["Level Num"] = level_state.level_number,
         ["Level Type"] = string.format("0x%02X (%s)", level_state.level_type, (level_state.level_type == 0xFF) and "Open" or "Closed"),
         ["Level Shape"] = level_state.level_shape,
    }
    display_str = display_str .. format_section("Level State", level_metrics)
    local spike_heights_str = ""
    for i = 0, 15 do spike_heights_str = spike_heights_str .. string.format("%02X ", level_state.spike_heights[i] or 0) end
    display_str = display_str .. "Spike Heights: " .. spike_heights_str .. "\n\n"

    -- Enemies State Section
    local enemies_metrics = {
        ["Flippers"] = string.format("%d/%d", enemies_state.active_flippers, enemies_state.spawn_slots_flippers),
        ["Pulsars"] = string.format("%d/%d", enemies_state.active_pulsars, enemies_state.spawn_slots_pulsars),
        ["Tankers"] = string.format("%d/%d", enemies_state.active_tankers, enemies_state.spawn_slots_tankers),
        ["Spikers"] = string.format("%d/%d", enemies_state.active_spikers, enemies_state.spawn_slots_spikers),
        ["Fuseballs"] = string.format("%d/%d", enemies_state.active_fuseballs, enemies_state.spawn_slots_fuseballs),
        ["Total Active"] = enemies_state:get_total_active(),
        ["In Tube"] = enemies_state.num_enemies_in_tube,
        ["On Top"] = enemies_state.num_enemies_on_top,
        ["Pending"] = enemies_state.enemies_pending,
        ["Pulse State"] = string.format("Beat:%02X Pulse:%02X Rate:%02X", enemies_state.pulse_beat, enemies_state.pulsing, enemies_state.pulsar_fliprate),
        ["Nearest Target"] = string.format("Seg:%s Depth:%02X Align:%.0f Err:%.0f",
                                    format_segment(enemies_state.nearest_enemy_seg),
                                    enemies_state.nearest_enemy_depth_raw,
                                    enemies_state.is_aligned_with_nearest * 100, -- Show as percentage 0 or 100
                                    enemies_state.alignment_error_magnitude * 100), -- Show as percentage
    }
    display_str = display_str .. format_section("Enemies State (Active/Spawnable)", enemies_metrics)

    -- Enemy Slots Details
    local enemy_details = { "Slot:", "Type:", "State:", "AbsSeg:", "RelSeg:", "Depth:" }
    for i = 1, 7 do
        enemy_details[1] = enemy_details[1] .. string.format(" %4d", i)
        -- Use decode methods if they exist on the enemies_state object
        local type_str = enemies_state.decode_enemy_type and enemies_state:decode_enemy_type(enemies_state.enemy_type_info[i]) or string.format("%d", enemies_state.enemy_type_info[i])
        local state_str = enemies_state.decode_enemy_state and enemies_state:decode_enemy_state(enemies_state.active_enemy_info[i]) or string.format("0x%02X", enemies_state.active_enemy_info[i])
        enemy_details[2] = enemy_details[2] .. " " .. string.format("%4s", type_str)
        enemy_details[3] = enemy_details[3] .. " " .. string.format("%4s", state_str)
        enemy_details[4] = enemy_details[4] .. string.format(" %4s", (enemies_state.enemy_abs_segments[i] ~= INVALID_SEGMENT) and enemies_state.enemy_abs_segments[i] or "---")
        enemy_details[5] = enemy_details[5] .. " " .. string.format("%4s", format_segment(enemies_state.enemy_segments[i]))
        enemy_details[6] = enemy_details[6] .. string.format(" %02X  ", enemies_state.enemy_depths[i]) -- Use 4 chars width
    end
    display_str = display_str .. table.concat(enemy_details, "\n") .. "\n\n"

    -- Enemy Shots Details
    local e_shots_pos_str = ""
    local e_shots_seg_str = ""
    for i = 1, 4 do
        -- Show precise shot positions with two decimals, width-aligned
        local pos = enemies_state.shot_positions[i] or 0.0
        e_shots_pos_str = e_shots_pos_str .. string.format(" %6.2f", pos)
        -- Widen segment formatting to align with positions
        e_shots_seg_str = e_shots_seg_str .. " " .. format_segment_wide(enemies_state.enemy_shot_segments[i], 6)
    end
    display_str = display_str .. "Enemy Shots Pos:" .. e_shots_pos_str .. "\n"
    display_str = display_str .. "Enemy Shots Seg:" .. e_shots_seg_str .. "\n\n"

    -- Display our three enemy tables with consistent formatting
    -- Charging Fuseballs array (7 entries)
    local charging_fuseball_str = "Charging Fuseball: "
    for i = 1, 7 do
        charging_fuseball_str = charging_fuseball_str .. format_enemy_segment(enemies_state.charging_fuseball[i]) .. " "
    end
    display_str = display_str .. charging_fuseball_str .. "\n"
    
    -- Active Pulsars array (7 entries)
    local active_pulsar_str = "Active Pulsars:    "
    for i = 1, 7 do
        active_pulsar_str = active_pulsar_str .. format_enemy_segment(enemies_state.active_pulsar[i]) .. " "
    end
    display_str = display_str .. active_pulsar_str .. "\n"
    
    -- Top Rail Enemies array (7 entries)
    local top_rail_enemies_str = "Top Rail Enemies:  "
    for i = 1, 7 do
        top_rail_enemies_str = top_rail_enemies_str .. format_enemy_segment(enemies_state.active_top_rail_enemies[i]) .. " "
    end
    display_str = display_str .. top_rail_enemies_str .. "\n"
    
    -- Fractional Enemy Segments array (7 entries)
    local fractional_segs_str = "Fractional Segs:   "
    for i = 1, 7 do
        local value = enemies_state.fractional_enemy_segments_by_slot[i]
        if value == INVALID_SEGMENT then
            fractional_segs_str = fractional_segs_str .. " ---- "
        else
            fractional_segs_str = fractional_segs_str .. string.format(" %04X ", value & 0xFFFF)
        end
    end
    display_str = display_str .. fractional_segs_str .. "\n\n"

    -- Pending Data (Show first 16 for brevity)
    local pending_vid_str = ""
    local pending_seg_str = ""
    for i = 1, 16 do
        pending_vid_str = pending_vid_str .. string.format("%02X ", enemies_state.pending_vid[i])
        pending_seg_str = pending_seg_str .. " " .. format_segment(enemies_state.pending_seg[i])
    end
    display_str = display_str .. "Pending VID:   " .. pending_vid_str .. "...\n"
    display_str = display_str .. "Pending SEG:   " .. pending_seg_str .. "...\n"

    -- Add padding to overwrite previous longer lines at the end
    display_str = display_str .. string.rep(" ", 80 * 5) -- Add blank lines to clear potential leftover lines

    -- Write the entire display string at once
    io.write(display_str)
    io.flush() -- Ensure output is written immediately
end

return Mlocal state_defs = require("state") -- Assuming state.lua is in the same dir

-- Define enemy type constants (copy from main.lua)
local ENEMY_TYPE_FLIPPER = 0
local ENEMY_TYPE_PULSAR = 1
local ENEMY_TYPE_TANKER = 2
local ENEMY_TYPE_SPIKER = 3
local ENEMY_TYPE_FUSEBALL = 4

-- Define constants (copy from main.lua)
local INVALID_SEGMENT = state_defs.INVALID_SEGMENT

-- New constants for top rail logic
local TOP_RAIL_DEPTH = 0x15
local SAFE_DISTANCE = 1
local FLIPPER_WAIT_DISTANCE = 5 -- segments within which we prefer to wait and conserve shots on top rail
local FLIPPER_REACT_DISTANCE_R = 2.0 -- distance at which we move one segment and fire (right-side, float)
local FLIPPER_REACT_DISTANCE_L = 2.0 -- distance at which we move one segment and fire (left-side, float)
local FREEZE_FIRE_PRIO_LOW = 2
local FREEZE_FIRE_PRIO_HIGH = 8
local AVOID_FIRE_PRIORITY = 3
local PULSAR_THRESHOLD = 0xE0 -- Pulsing threshold for avoidance (match dangerous pulsar threshold)
-- Open-level tuning: react slightly sooner to top-rail flippers using fractional distance
local OPEN_FLIPPER_REACT_DISTANCE = 1.10
-- Retreat positions for open level flipper handling
local RIGHT_RETREAT_SEGMENT = 1   -- retreat to segment 1 when flippers to the right
local LEFT_RETREAT_SEGMENT = 13  -- retreat to segment 13 when flippers to the left  
-- Pulsar target offset (segments away from pulsar when hunting/avoiding)
local PULSAR_TARGET_DISTANCE = 2
-- Optional: Conserve fire mode (hold fire/movement until react distance)
local CONSERVE_FIRE_MODE = false
local CONSERVE_REACT_DISTANCE = 1.10
-- Configurable pulsar hunting preferences
local PULSAR_PREF_DISTANCE = 1.0   -- desired lanes away from the pulsar (can be fractional for hold/fire logic)
local PULSAR_PREF_TOLERANCE = 0.15 -- acceptable window around preferred distance to hold and fire

local M = {} -- Module table

-- Global variables needed by calculate_reward (scoped within this module)
-- Reward shaping parameters (tunable)
local SCORE_UNIT = 10000.0           -- 10k points ~= 1 life worth of reward
local LEVEL_COMPLETION_BONUS = 2.0   -- Edge-triggered bonus when level increments
local DEATH_PENALTY = 0.3            -- Edge-triggered penalty when dying (raised to better balance vs completion)
local ZAP_COST = 0.2                 -- Edge-triggered Small cost per zap frame
 
local previous_score = 0
local previous_level = 0
local previous_alive_state = 1 -- Track previous alive state, initialize as alive
local LastRewardState = 0
-- Track superzapper usage and activation edge per level (for one-time charging)
local previous_superzapper_active = 0
local previous_superzapper_uses_in_level = 0
local previous_zap_detected = 0
local previous_fire_detected = 0
-- Track previous player position for positioning reward
local previous_player_position = 0
-- Track score at the beginning of the current level (for optional ratio-based bonuses)
local score_at_level_start = 0

-- Track previous top-rail alignment (for progress reward)
local previous_toprail_min_abs_rel = nil

-- Helper function to find nearest enemy of a specific type (copied from state.lua for locality within logic)
-- NOTE: Duplicated from state.lua for now to keep logic self-contained. Consider unifying later.
local function find_nearest_enemy_of_type(enemies_state, player_abs_segment, is_open, enemy_type, abs_to_rel_func)
    local nearest_seg_abs = -1
    local min_dist = 255
    local nearest_depth = 255

    for i = 1, 7 do
        if enemies_state.enemy_core_type[i] == enemy_type and enemies_state.enemy_depths[i] > 0 then
            local enemy_abs_seg = enemies_state.enemy_abs_segments[i]
            if enemy_abs_seg ~= INVALID_SEGMENT then
                local rel_dist = abs_to_rel_func(player_abs_segment, enemy_abs_seg, is_open)
                local abs_dist = math.abs(rel_dist)

                if abs_dist < min_dist then
                    min_dist = abs_dist
                    nearest_seg_abs = enemy_abs_seg
                    nearest_depth = enemies_state.enemy_depths[i]
                -- Tie-breaking: shallower enemy is preferred
                elseif abs_dist == min_dist and enemies_state.enemy_depths[i] < nearest_depth then
                    nearest_seg_abs = enemy_abs_seg
                    nearest_depth = enemies_state.enemy_depths[i]
                end
            end
        end
    end
    return nearest_seg_abs, nearest_depth
end


-- Function to get the relative distance to a target segment
function M.absolute_to_relative_segment(current_abs_segment, target_abs_segment, is_open_level)
    current_abs_segment = tonumber(current_abs_segment) or 0
    target_abs_segment = tonumber(target_abs_segment) or 0
    current_abs_segment = math.floor(current_abs_segment) % 16
    target_abs_segment = math.floor(target_abs_segment) % 16

    if is_open_level then
        return target_abs_segment - current_abs_segment
    else
        local diff = target_abs_segment - current_abs_segment
        -- Wrap into [-8, +8] range, but treat exact ties (±8) neutrally
        if diff > 8 then
            diff = diff - 16
        elseif diff < -8 then
            diff = diff + 16
        end
        -- Neutral tie-breaker: if exactly opposite (distance 8), randomly pick direction
        if diff == 8 or diff == -8 then
            diff = (math.random() < 0.5) and 8 or -8
        end
        return diff
    end
end

-- Helper function to hunt enemies in preference order
function M.hunt_enemies(enemies_state, player_abs_segment, is_open, abs_to_rel_func, forbidden_segments)
    local hunt_order = {
        ENEMY_TYPE_FUSEBALL, ENEMY_TYPE_FLIPPER, ENEMY_TYPE_TANKER, ENEMY_TYPE_SPIKER
    }
    for _, enemy_type in ipairs(hunt_order) do
        local target_seg_abs, target_depth = find_nearest_enemy_of_type(enemies_state, player_abs_segment, is_open, enemy_type, abs_to_rel_func)
        if target_seg_abs ~= -1 then
            -- NOTE: Top rail flipper avoidance (depth 0x10) is removed here, handled by new logic in find_target_segment
            -- if target_depth <= 0x10 and enemy_type == ENEMY_TYPE_FLIPPER then ... end
            return target_seg_abs, target_depth, false -- Returns target_abs, depth, should_fire (always false from hunt)
        end
    end
    return -1, 255, false
end

-- Function to handle tube zoom state (0x20)
function M.zoom_down_tube(player_abs_seg, level_state, is_open)
    local current_spike_h = level_state.spike_heights[player_abs_seg]
    if current_spike_h == 0 then return player_abs_seg, 0, true, false end

    local left_neighbour_seg, right_neighbour_seg = -1, -1
    if is_open then
        if player_abs_seg > 0 then left_neighbour_seg = player_abs_seg - 1 end
        if player_abs_seg < 15 then right_neighbour_seg = player_abs_seg + 1 end
    else
        left_neighbour_seg = (player_abs_seg - 1 + 16) % 16
        right_neighbour_seg = (player_abs_seg + 1) % 16
    end

    local left_spike_h = (left_neighbour_seg ~= -1) and level_state.spike_heights[left_neighbour_seg] or -1
    local right_spike_h = (right_neighbour_seg ~= -1) and level_state.spike_heights[right_neighbour_seg] or -1

    if left_spike_h == 0 then return left_neighbour_seg, 0, true, false end
    if right_spike_h == 0 then return right_neighbour_seg, 0, true, false end

    local temp_target = player_abs_seg
    local is_left_better = (left_spike_h > current_spike_h)
    local is_right_better = (right_spike_h > current_spike_h)

    if is_left_better and is_right_better then
        temp_target = (left_spike_h >= right_spike_h) and left_neighbour_seg or right_neighbour_seg
    elseif is_left_better then temp_target = left_neighbour_seg
    elseif is_right_better then temp_target = right_neighbour_seg end

    return temp_target, 0, true, false
end

-- Function to check for fuseball threats
function M.fuseball_check(player_abs_seg, enemies_state, is_open, abs_to_rel_func)
    local fuseball_threat_nearby = false
    local escape_target_seg = -1
    for i = 1, 7 do
        if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FUSEBALL and
           enemies_state.enemy_depths[i] <= 0x40 and
           enemies_state.enemy_abs_segments[i] ~= INVALID_SEGMENT then
            local fuseball_abs_seg = enemies_state.enemy_abs_segments[i]
            local rel_dist = abs_to_rel_func(player_abs_seg, fuseball_abs_seg, is_open)
            if math.abs(rel_dist) <= 2 then
                fuseball_threat_nearby = true
                escape_target_seg = (rel_dist <= 0) and ((player_abs_seg + 3) % 16) or ((player_abs_seg - 3 + 16) % 16)
                break
            end
        end
    end
    return fuseball_threat_nearby, escape_target_seg
end

-- Function to check for pulsar threats
function M.pulsar_check(player_abs_seg, enemies_state, is_open, abs_to_rel_func, forbidden_segments)
    if enemies_state.pulsing < PULSAR_THRESHOLD then return false, player_abs_seg, 0, false, false end

    local is_in_pulsar_lane = false
    local current_pulsar_seg = -1
    for i = 1, 7 do
        if enemies_state.enemy_core_type[i] == ENEMY_TYPE_PULSAR and
           enemies_state.enemy_depths[i] > 0 then
            if enemies_state.enemy_abs_segments[i] == player_abs_seg then
                is_in_pulsar_lane = true
                current_pulsar_seg = player_abs_seg
                break
            end
        end
    end

    if not is_in_pulsar_lane then return false, player_abs_seg, 0, false, false end

    local adj = adjacent_to_pulsar_closest_to_player(current_pulsar_seg, player_abs_seg, is_open, abs_to_rel_func)
    if adj == -1 then
        adj = find_nearest_non_pulsar_segment(player_abs_seg, enemies_state, is_open)
    end
    return true, adj, 0, false, false
end

-- Function to check for immediate threats in a segment
function M.check_segment_threat(segment, enemies_state)
    for i = 1, 4 do -- Check shots
        if enemies_state.enemy_shot_abs_segments[i] == segment and enemies_state.shot_positions[i] > 0 and enemies_state.shot_positions[i] <= 0x30 then return true end
    end
    for i = 1, 7 do -- Check enemies
        if enemies_state.enemy_abs_segments[i] == segment and enemies_state.enemy_depths[i] > 0 and enemies_state.enemy_depths[i] <= 0x30 then return true end
    end
    for i = 1, 7 do -- Check pulsars (regardless of pulsing state for general threat)
        if enemies_state.enemy_core_type[i] == ENEMY_TYPE_PULSAR and enemies_state.enemy_abs_segments[i] == segment and enemies_state.enemy_depths[i] > 0 then return true end
    end
    return false
end

-- Returns true if the segment is a danger lane (more specific criteria for immediate action)
function M.is_danger_lane(segment, enemies_state)
    if enemies_state.pulsing >= PULSAR_THRESHOLD then -- Check dangerous pulsars first
        for i = 1, 7 do
            if enemies_state.enemy_core_type[i] == ENEMY_TYPE_PULSAR and enemies_state.enemy_abs_segments[i] == segment and enemies_state.enemy_depths[i] > 0 then return true end
        end
    end
    for i = 1, 7 do -- Check enemies with type-specific danger distances
        if enemies_state.enemy_abs_segments[i] == segment and enemies_state.enemy_depths[i] > 0 then
            local enemy_type = enemies_state.enemy_core_type[i]
            local depth = enemies_state.enemy_depths[i]
            -- Fuseballs are fatal on contact, so they're dangerous at greater distances on top rail
            if enemy_type == ENEMY_TYPE_FUSEBALL and depth <= TOP_RAIL_DEPTH then return true end
            -- Other enemies are dangerous when close
            if depth <= 0x20 then return true end
        end
    end
    for i = 1, 4 do -- Check close enemy shots (depth <= 0x30)
        if enemies_state.enemy_shot_abs_segments[i] == segment and enemies_state.shot_positions[i] > 0 and enemies_state.shot_positions[i] <= 0x30 then return true end
    end
    return false
end

-- Helper function to find the segment of the nearest enemy at depth 0x10
function M.find_nearest_top_rail_enemy_seg(player_abs_seg, enemies_state, abs_to_rel_func, is_open)
    local nearest_enemy_seg, min_dist = -1, 255
    for i = 1, 7 do
        if enemies_state.enemy_depths[i] == 0x10 then
            local enemy_seg = enemies_state.enemy_abs_segments[i]
            if enemy_seg ~= INVALID_SEGMENT then
                local abs_dist = math.abs(abs_to_rel_func(player_abs_seg, enemy_seg, is_open))
                if abs_dist < min_dist then min_dist = abs_dist; nearest_enemy_seg = enemy_seg end
            end
        end
    end
    return nearest_enemy_seg
end

-- Returns the highest priority enemy type and its priority value in a segment
function M.get_enemy_priority(segment, enemies_state)
    local best_priority = 100
    local best_type = nil
    local priority_map = {[ENEMY_TYPE_PULSAR]=1, [ENEMY_TYPE_FLIPPER]=2, [ENEMY_TYPE_TANKER]=3, [ENEMY_TYPE_FUSEBALL]=4, [ENEMY_TYPE_SPIKER]=5}
    for i = 1, 7 do
        if enemies_state.enemy_abs_segments[i] == segment and enemies_state.enemy_depths[i] > 0 then
            local t = enemies_state.enemy_core_type[i]
            local p = priority_map[t] or 99
            if p < best_priority then best_priority = p; best_type = t end
        end
    end
    return best_type, best_priority
end

-- Public API: configure conserve fire mode
function M.set_conserve_fire_mode(enabled, react_distance)
    CONSERVE_FIRE_MODE = not not enabled
    if type(react_distance) == "number" and react_distance >= 0.5 then
        CONSERVE_REACT_DISTANCE = react_distance
    end
end

function M.get_conserve_fire_mode()
    return CONSERVE_FIRE_MODE, CONSERVE_REACT_DISTANCE
end

-- Helper to find the nearest safe segment (not a danger lane)
local function find_nearest_safe_segment(start_seg, enemies_state, is_open)
    if not M.is_danger_lane(start_seg, enemies_state) then return start_seg end

    for d = 1, 8 do -- Search radius up to 8
        local left_seg = (start_seg - d + 16) % 16
        if not M.is_danger_lane(left_seg, enemies_state) then return left_seg end

        local right_seg = (start_seg + d + 16) % 16
        if not M.is_danger_lane(right_seg, enemies_state) then return right_seg end
    end
    return start_seg -- Fallback: stay put if no safe found nearby
end

-- NEW Helper: Check if a segment contains an active Pulsar
local function is_pulsar_lane(segment, enemies_state)
    for i = 1, 7 do
        if enemies_state.enemy_core_type[i] == ENEMY_TYPE_PULSAR and
           enemies_state.enemy_abs_segments[i] == segment and
           enemies_state.enemy_depths[i] > 0 then
            return true
        end
    end
    return false
end

-- Public API: configure pulsar hunting preferences
function M.set_pulsar_preference(distance, tolerance)
    if type(distance) == "number" and distance >= 1 then PULSAR_PREF_DISTANCE = distance end
    if type(tolerance) == "number" and tolerance >= 0 then PULSAR_PREF_TOLERANCE = tolerance end
end

function M.get_pulsar_preference()
    return PULSAR_PREF_DISTANCE, PULSAR_PREF_TOLERANCE
end

-- Helper: Get the lane adjacent to a given pulsar that's closest to the player
local function adjacent_to_pulsar_closest_to_player(pulsar_seg, player_seg, is_open, abs_to_rel_func)
    local left_adj = (pulsar_seg - 1 + 16) % 16
    local right_adj = (pulsar_seg + 1) % 16
    if is_open then
        -- Clamp for open levels
        left_adj = (left_adj >= 0 and left_adj <= 15) and left_adj or -1
        right_adj = (right_adj >= 0 and right_adj <= 15) and right_adj or -1
    end
    local best = -1
    local best_dist = 999
    if left_adj ~= -1 then
        local d = math.abs(abs_to_rel_func(player_seg, left_adj, is_open))
        if d < best_dist then best, best_dist = left_adj, d end
    end
    if right_adj ~= -1 then
        local d = math.abs(abs_to_rel_func(player_seg, right_adj, is_open))
        if d < best_dist then best, best_dist = right_adj, d end
    end
    return best
end

-- Helper: Find nearest segment that is NOT a pulsar lane (ignores other dangers by design)
local function find_nearest_non_pulsar_segment(start_seg, enemies_state, is_open)
    if not is_pulsar_lane(start_seg, enemies_state) then return start_seg end
    for d = 1, 8 do
        local left_seg = is_open and (start_seg - d) or ((start_seg - d + 16) % 16)
        local right_seg = is_open and (start_seg + d) or ((start_seg + d) % 16)
        if left_seg >= 0 and left_seg <= 15 and not is_pulsar_lane(left_seg, enemies_state) then return left_seg end
        if right_seg >= 0 and right_seg <= 15 and not is_pulsar_lane(right_seg, enemies_state) then return right_seg end
    end
    return start_seg -- fallback (should be rare)
end

-- NEW Helper: Find nearest safe segment that also respects distance from a constraint segment
local function find_nearest_constrained_safe_segment(start_seg, enemies_state, is_open, constraint_seg, abs_to_rel_func)
    -- Search outwards from the start segment
    for d = 0, 8 do -- Check current segment first (d=0), then outwards
        local segments_to_check = {}
        if d == 0 then
            segments_to_check = {start_seg}
        else
            local left_seg = (start_seg - d + 16) % 16
            local right_seg = (start_seg + d + 16) % 16
            segments_to_check = {left_seg, right_seg}
        end

        for _, check_seg in ipairs(segments_to_check) do
            if not M.is_danger_lane(check_seg, enemies_state) then
                local dist_to_constraint = math.abs(abs_to_rel_func(check_seg, constraint_seg, is_open))
                if dist_to_constraint >= SAFE_DISTANCE then
                    -- Found a segment that is safe AND respects the distance constraint
                    return check_seg
                end
            end
        end
    end

    -- Fallback: If no segment satisfies both, return the simple nearest safe segment
    return start_seg -- NEW FALLBACK: Prefer original unsafe target over potentially worse simple safe target
end

-- Function to find the target segment and recommended action (expert policy)
function M.find_target_segment(game_state, player_state, level_state, enemies_state, abs_to_rel_func)
    -- Simplified targeting logic per spec
    local is_open = (level_state.level_type == 0xFF)
    local player_abs_seg = math.floor(player_state.position) % 16
    local shot_count = player_state.shot_count or 0

    -- Tube Zoom behavior unchanged
    if game_state.gamestate == 0x20 then
        local seg, _, should_fire, _ = M.zoom_down_tube(player_abs_seg, level_state, is_open)
        return seg, 0, should_fire, false
    end
    if game_state.gamestate ~= 0x04 then
        return player_abs_seg, 0, false, false
    end

    -- Immediate fuseball avoidance: if a charging fuseball is in our lane or adjacent and near the top,
    -- move one segment away and keep firing. This preempts other targeting logic.
    do
        local FUSEBALL_NEAR_DEPTH = 0x50 -- consider near-top fuseballs (<= 0x50) as immediate threats
        local best_threat_rel = nil
        local best_threat_abs_seg = -1
        for i = 1, 7 do
            if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FUSEBALL and
               enemies_state.enemy_abs_segments[i] ~= INVALID_SEGMENT then
                local depth = enemies_state.enemy_depths[i]
                local moving_away = (enemies_state.active_enemy_info and ((enemies_state.active_enemy_info[i] or 0) & 0x80) ~= 0) or false
                if depth > 0 and depth <= FUSEBALL_NEAR_DEPTH and not moving_away then
                    local rel = abs_to_rel_func(player_abs_seg, enemies_state.enemy_abs_segments[i], is_open)
                    local abs_rel = math.abs(rel)
                    if abs_rel <= 1 then
                        -- choose the closest such threat
                        if not best_threat_rel or abs_rel < math.abs(best_threat_rel) then
                            best_threat_rel = rel
                            best_threat_abs_seg = enemies_state.enemy_abs_segments[i]
                        end
                    end
                end
            end
        end
        if best_threat_rel ~= nil then
            local move_right = (best_threat_rel <= 0) -- threat aligned/left -> move right
            local candidate = -1
            if move_right then
                if is_open then
                    if player_abs_seg < 15 then candidate = player_abs_seg + 1 end
                else
                    candidate = (player_abs_seg + 1) % 16
                end
            else
                if is_open then
                    if player_abs_seg > 0 then candidate = player_abs_seg - 1 end
                else
                    candidate = (player_abs_seg - 1 + 16) % 16
                end
            end
            -- Fallback to the opposite side if open edge blocked
            if candidate == -1 then
                if move_right then
                    if is_open then
                        if player_abs_seg > 0 then candidate = player_abs_seg - 1 end
                    else
                        candidate = (player_abs_seg - 1 + 16) % 16
                    end
                else
                    if is_open then
                        if player_abs_seg < 15 then candidate = player_abs_seg + 1 end
                    else
                        candidate = (player_abs_seg + 1) % 16
                    end
                end
            end
            if candidate ~= -1 then
                return candidate, 0, true, false
            end
        end
    end

    -- Scan top-rail threats (flippers and pulsars are equivalent once on top rail)
    local nr_seg, nl_seg = nil, nil
    local nr_dist, nl_dist = 999, 999
    local nr_dist_float, nl_dist_float = 999, 999
    local right_exists, left_exists = false, false
    local min_abs_rel_float = 999 -- nearest top-rail flipper/pulsar fractional distance
    for i = 1, 7 do
        local depth = enemies_state.enemy_depths[i]
        if depth > 0 and depth <= TOP_RAIL_DEPTH then
            local t = enemies_state.enemy_core_type[i]
            if t == ENEMY_TYPE_FLIPPER or t == ENEMY_TYPE_PULSAR then
                local seg = enemies_state.enemy_abs_segments[i]
                if seg ~= INVALID_SEGMENT then
                    local rel_int = abs_to_rel_func(player_abs_seg, seg, is_open)
                    local rel_float = enemies_state.active_top_rail_enemies[i]
                    if rel_float == 0 and enemies_state.enemy_between_segments[i] == 0 then
                        rel_float = rel_int
                    end
                    local abs_int = math.abs(rel_int)
                    local abs_float = math.abs(rel_float)
                    if rel_int > 0 and abs_int < nr_dist then nr_dist, nr_dist_float, nr_seg, right_exists = abs_int, abs_float, seg, true end
                    if rel_int < 0 and abs_int < nl_dist then nl_dist, nl_dist_float, nl_seg, left_exists = abs_int, abs_float, seg, true end
                    if abs_float < min_abs_rel_float then min_abs_rel_float = abs_float end
                end
            end
        end
    end

    -- Open-level fixed retreats (right-only -> 1, left-only -> 13). Both sides falls through to closed-level logic
    local target_seg = player_abs_seg
    if is_open then
        if right_exists and not left_exists then
            target_seg = RIGHT_RETREAT_SEGMENT -- 1
        elseif left_exists and not right_exists then
            target_seg = LEFT_RETREAT_SEGMENT -- 13
        end
    end

    -- Closed-level (and open with both sides) movement policy
    if (not is_open) or (is_open and right_exists and left_exists) then
        if right_exists and not left_exists then
            -- Right-only: move left if within safe distance
            if nr_dist <= SAFE_DISTANCE then
                target_seg = (nr_seg - 1 + 16) % 16
            else
                target_seg = player_abs_seg
            end
        elseif left_exists and not right_exists then
            -- Left-only: move right if within safe distance
            if nl_dist <= SAFE_DISTANCE then
                target_seg = (nl_seg + 1) % 16
            else
                target_seg = player_abs_seg
            end
        elseif right_exists and left_exists then
            -- Both sides: freeze (hold position)
            target_seg = player_abs_seg
        else
            -- No top-rail flippers/pulsars: hold
            target_seg = player_abs_seg
        end
    end

    -- Firing policy
    -- Shoot if: something is in our lane OR any top-rail flipper/pulsar within shooting distance (~0.8)
    local SHOOT_DIST = 0.80
    local lane_has_threat = M.check_segment_threat(player_abs_seg, enemies_state)
    local within_shooting_distance = (min_abs_rel_float <= SHOOT_DIST)

    local should_fire
    if lane_has_threat or within_shooting_distance then
        should_fire = true
    else
        -- Keep only 3 shots onscreen
        should_fire = (shot_count < 3)
    end

    -- Expert tweak: When we would fire at a top-rail flipper/pulsar but the shot buffer is full (>=8),
    -- still recommend FIRE and also MOVE AWAY by one segment opposite the nearest threat.
    -- Applies only if there is at least one top-rail flipper/pulsar detected (right_exists/left_exists)
    if should_fire and shot_count >= 8 and (right_exists or left_exists) then
        -- Determine nearest side using fractional distances if available
        local move_right -- boolean: true => move to player_abs_seg+1, false => -1
        if right_exists and left_exists then
            -- Choose the smaller fractional distance; fall back to integer distances if needed
            local dR = nr_dist_float or nr_dist or 999
            local dL = nl_dist_float or nl_dist or 999
            -- If threat is closer/equal on left, move right; else move left
            move_right = (dL <= dR)
        elseif right_exists then
            -- Threat only on the right -> move left
            move_right = false
        else -- left_exists only
            -- Threat only on the left -> move right
            move_right = true
        end

        -- Compute candidate adjacent segment respecting open/closed topology
        local candidate = -1
        if move_right then
            if is_open then
                if player_abs_seg < 15 then candidate = player_abs_seg + 1 end
            else
                candidate = (player_abs_seg + 1) % 16
            end
        else
            if is_open then
                if player_abs_seg > 0 then candidate = player_abs_seg - 1 end
            else
                candidate = (player_abs_seg - 1 + 16) % 16
            end
        end

        if candidate ~= -1 then
            target_seg = candidate
        end
    end

    -- Superzap heuristic retained (3+ top-rail enemies)
    local top_rail_count = 0
    for i = 1, 7 do
        local depth = enemies_state.enemy_depths[i]
        local seg = enemies_state.enemy_abs_segments[i]
        if depth > 0 and depth <= TOP_RAIL_DEPTH and seg ~= INVALID_SEGMENT then
            top_rail_count = top_rail_count + 1
        end
    end
    local superzapper_available = (player_state.superzapper_uses or 0) < 2
    local should_superzap = superzapper_available and (top_rail_count >= 3)

    return target_seg, 0, should_fire, should_superzap
end

-- Function to calculate desired spinner direction and distance to target enemy
function M.direction_to_nearest_enemy(game_state, level_state, player_state, enemies_state, abs_to_rel_func)
    local player_abs_seg = math.floor(player_state.position) % 16
    local is_open = (level_state.level_type == 0xFF)
    local target_abs_segment = enemies_state.nearest_enemy_abs_seg_internal or -1

    if target_abs_segment == -1 then return 0, 0, 255 end -- No target

    local relative_dist = abs_to_rel_func(player_abs_seg, target_abs_segment, is_open)
    if relative_dist == 0 then return 0, 0, enemies_state.nearest_enemy_depth_raw end -- Aligned

    local distance = math.abs(relative_dist)
    local intensity = math.min(0.9, 0.3 + (distance * 0.05))
    local spinner = (relative_dist > 0) and intensity or -intensity
    return spinner, distance, 255 -- Misaligned (depth 255 indicates not aligned)
end

-- Function to calculate reward for the current frame
function M.calculate_reward(game_state, level_state, player_state, enemies_state, abs_to_rel_func)
    local reward, bDone = 0.0, false

    -- Terminal: death (edge-triggered) - Scaled to match 1 life = 1.0 reward unit
    if player_state.alive == 0 and previous_alive_state == 1 then
        reward = reward - DEATH_PENALTY
        bDone = true
    else
        -- Primary dense signal: scaled/clipped score delta
        local score_delta = (player_state.score or 0) - (previous_score or 0)
        if score_delta ~= 0 and score_delta < 1000 then                         -- Filter our large completion bonuses
            local r_score = score_delta / SCORE_UNIT                            -- Scaled: 20k points = 1.0 reward unit
            if r_score > 1.0 then r_score = 1.0 end
            if r_score < -1.0 then r_score = -1.0 end
            reward = reward + r_score
        end

        -- Level completion bonus (edge-triggered) - Scaled to match death penalty magnitude
        if (level_state.level_number or 0) > (previous_level or 0) then
            -- Fixed completion bonus; optional ratio-based scaling can be added later using score_at_level_start
            reward = reward + LEVEL_COMPLETION_BONUS
            bDone = true
        end

        -- Zap cost (edge-triggered on button press)
        local zap_now = player_state.zap_detected or 0
        if zap_now == 1 and previous_zap_detected == 0 then
            reward = reward - ZAP_COST
        end

        -- === OBJECTIVE DENSE REWARD COMPONENTS ===
        -- Only apply during active gameplay (not tube zoom, high score entry, etc.)
        if game_state.gamestate == 0x04 or game_state.gamestate == 0x20 then
            local player_abs_seg = player_state.position & 0x0F
            local is_open = (level_state.level_type == 0xFF)
            
            -- Level-specific scaling factors
            local level_type = (level_state.level_number - 1) % 4
            local proximity_scale = 1.0
            local safety_scale = 1.0
            
            if level_type == 2 then -- Open levels (every 4th level starting from 3)
                proximity_scale = 1.2  -- Increase proximity rewards on open levels
                safety_scale = 0.8     -- Reduce safety emphasis (more space available)
            elseif level_type == 0 then -- Level 1 type (every 4th level starting from 1)
                proximity_scale = 0.9  -- Slightly reduce proximity rewards on simpler levels
                safety_scale = 1.1     -- Increase safety emphasis on basic levels
            end

            -- Compute expert target once for reward gating (avoid rewarding moves toward danger)
            local expert_target_seg_cached = -1
            do
                local ets, _, _, _ = M.find_target_segment(game_state, player_state, level_state, enemies_state, abs_to_rel_func)
                expert_target_seg_cached = ets or -1
            end
            
            -- 1. DANGER AVOIDANCE REWARD (Penalties removed per spec)
            -- No penalty for being in dangerous positions; optional safe bonus remains disabled
            
            -- 2. PROXIMITY OPTIMIZATION REWARD (Distance-based Positioning)
            -- Reward optimal distance to nearest enemy (not too close, not too far)
            -- IMPORTANT: If expert target lane is dangerous, skip proximity shaping entirely.
            do
                local danger_target_lane = (expert_target_seg_cached ~= -1) and M.is_danger_lane(expert_target_seg_cached, enemies_state)
                local nearest_distance = enemies_state.alignment_error_magnitude or 1.0
                if not danger_target_lane and enemies_state.nearest_enemy_seg ~= INVALID_SEGMENT then
                    -- Convert normalized distance (0-1) to segments for clearer logic
                    local max_dist = is_open and 15.0 or 8.0
                    local distance_segments = nearest_distance * max_dist
                    
                    -- Optimal range: 1-3 segments (allows reaction time but maintains offensive capability)
                    local optimal_min, optimal_max = 1.0, 3.0
                    local prox_reward = 0.0
                    if distance_segments >= optimal_min and distance_segments <= optimal_max then
                        prox_reward = 0.10 * proximity_scale  -- Good positioning bonus
                    else
                        prox_reward = 0.0 -- Penalties removed
                    end
                    -- Neutral reward for 3-5 segments (acceptable range)
                    reward = reward + prox_reward
                end
            end
            
            -- 3. STRATEGIC SHOT MANAGEMENT REWARD (Smart Resource Management)
            -- Requested policy:
            --   0-2 shots: penalty (too few shots)
            --   4-7 shots: reward (good management)
            --   8 shots: penalty (overuse)
            do
                local shot_count = player_state.shot_count or 0
                local shot_reward = 0.0
                if shot_count <= 2 then
                    shot_reward = 0 -- Penalty removed
                elseif shot_count >= 4 and shot_count <= 7 then
                    shot_reward = 1
                elseif shot_count >= 8 then
                    shot_reward = 0 -- Penalty removed
                end
                reward = reward + shot_reward / SCORE_UNIT
            end
            
            -- 4. THREAT RESPONSIVENESS REWARD (Reaction to Immediate Dangers)
            -- Bonus for appropriate responses to critical threats
            local critical_threats = 0
            for i = 1, 7 do
                local abs_seg = enemies_state.enemy_abs_segments[i]
                local depth = enemies_state.enemy_depths[i]
                local enemy_type = enemies_state.enemy_core_type[i]
                
                if abs_seg == player_abs_seg and depth > 0 and depth <= 0x30 then
                    critical_threats = critical_threats + 1
                end
            end
            
            -- Threat penalties removed per spec (no subtraction)
            
            -- 5. PULSAR SAFETY REWARD (Objective Hazard Assessment)
            -- Extra penalty for being in pulsing pulsar lanes (objectively lethal)
            -- Pulsar safety penalties removed per spec

            -- 6. EXPERT POSITIONING REWARD (Follow Expert System Guidance)
            -- Reward the DQN for following expert system's strategic positioning recommendations
            -- IMPORTANT: If expert target lane is dangerous, skip positioning shaping entirely.
            do
                local positioning_reward = 0.0
                if expert_target_seg_cached ~= -1 then
                    local danger_target_lane = M.is_danger_lane(expert_target_seg_cached, enemies_state)
                    if not danger_target_lane then
                        local current_player_seg = player_abs_seg
                        local prev_player_seg = previous_player_position & 0x0F
                        
                        -- Calculate distances to expert target (current and previous)
                        local current_distance = math.abs(abs_to_rel_func(current_player_seg, expert_target_seg_cached, is_open))
                        local previous_distance = math.abs(abs_to_rel_func(prev_player_seg, expert_target_seg_cached, is_open))
                        
                        -- Award for being on target segment
                        if current_distance == 0 then
                            -- Worth ~10 points when perfectly aligned (10 / SCORE_UNIT)
                            positioning_reward = 10.0 / SCORE_UNIT
                        -- Award for moving toward target
                        elseif current_distance < previous_distance then
                            local progress = previous_distance - current_distance
                            -- Scale small progress reward so it remains below score/death signals
                            positioning_reward = 0.5 * (progress / 8.0) * (10.0 / SCORE_UNIT)
                        -- Small penalty for moving away from target or not moving when needed
                        elseif current_distance > 0 then
                            -- Penalties removed for moving away or not moving
                        end
                        
                        -- Apply level-specific scaling
                        if level_type == 2 then -- Open levels
                            positioning_reward = positioning_reward * 1.1  -- Slightly increase positioning importance on open levels
                        elseif level_type == 0 then -- Basic levels
                            positioning_reward = positioning_reward * 0.9  -- Slightly reduce on simpler levels
                        end
                    end
                end
                reward = reward + positioning_reward
            end

            -- 8. TOP-RAIL FLIPPER ENGAGEMENT REWARD (dense shaping using fractional rel positions)
            do
                local min_abs_rel_float = nil
                local aligned_fire_bonus = 0.0
                local hold_penalty = 0.0

                -- Find nearest top-rail flipper using authoritative fractional relative pos
                for i = 1, 7 do
                    if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FLIPPER and
                       enemies_state.enemy_depths[i] > 0 and enemies_state.enemy_depths[i] <= 0x30 then
                        local relf = enemies_state.active_top_rail_enemies[i]
                        if relf ~= 0 or enemies_state.enemy_between_segments[i] == 1 then
                            local d = math.abs(relf)
                            if not min_abs_rel_float or d < min_abs_rel_float then
                                min_abs_rel_float = d
                            end
                        end
                    end
                end

                if min_abs_rel_float then
                    -- Progress toward alignment (reward getting closer)
                    if previous_toprail_min_abs_rel then
                        local delta = previous_toprail_min_abs_rel - min_abs_rel_float
                        if delta > 0 then
                            -- Small reward proportional to progress, capped and scaled under 10 pts
                            local progress_bonus = math.min(1.0, delta) * (3.0 / SCORE_UNIT)
                            reward = reward + progress_bonus
                        end
                    end

                    -- Reward firing when well aligned (tight aim window)
                    local fire_now = (player_state.fire_detected or 0)
                    local fire_edge = (fire_now == 1 and previous_fire_detected == 0)
                    if min_abs_rel_float <= 0.30 and fire_edge then
                        reward = reward + (4.0 / SCORE_UNIT) -- small bonus (~4 pts) for well-timed shot
                    end

                    -- Penalty for not firing while very close removed per spec
                end
            end

            -- 7. USELESS MOVEMENT PENALTY (Discourage random spinner when not needed)
            -- Apply a small penalty when spinning while already aligned (or no target)
            -- and not in immediate danger. Keeps behavior calm instead of fidgety.
            do
                local spin_delta = math.abs(tonumber(player_state.spinner_detected or 0))
                if spin_delta > 0 then
                    local player_abs_seg2 = player_state.position & 0x0F
                    local nearest_abs = enemies_state.nearest_enemy_abs_seg_internal or -1
                    local is_open2 = (level_state.level_type == 0xFF)
                    local need_move = false
                    if nearest_abs ~= -1 then
                        local rel = abs_to_rel_func(player_abs_seg2, nearest_abs, is_open2)
                        need_move = math.abs(rel) > 1 -- require movement if more than one segment off
                    end
                    local in_danger = M.is_danger_lane(player_abs_seg2, enemies_state)
                    if not need_move and not in_danger then
                        -- Scale a gentle penalty with movement magnitude, capped
                        local units = math.min(4, spin_delta)
                        local move_penalty = -0.0002 * units
                        -- reward = reward + move_penalty
                        -- Attribute to proximity shaping bucket for metrics
                    end
                end
            end
        end
    end

    -- State updates
    -- Detect level increment to reset per-level trackers
    local current_level = level_state.level_number or 0
    local current_score = player_state.score or 0
    if current_level > (previous_level or 0) then
        score_at_level_start = current_score
    end
    previous_score = current_score
    previous_level = current_level
    previous_alive_state = player_state.alive or 0
    previous_zap_detected = player_state.zap_detected or 0
    previous_fire_detected = player_state.fire_detected or 0
    previous_player_position = player_state.position or 0
    -- Update top-rail tracking
    do
        local min_abs_rel_float = nil
        for i = 1, 7 do
            if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FLIPPER and
               enemies_state.enemy_depths[i] > 0 and enemies_state.enemy_depths[i] <= 0x30 then
                local relf = enemies_state.active_top_rail_enemies[i]
                if relf ~= 0 or enemies_state.enemy_between_segments[i] == 1 then
                    local d = math.abs(relf)
                    if not min_abs_rel_float or d < min_abs_rel_float then
                        min_abs_rel_float = d
                    end
                end
            end
        end
        previous_toprail_min_abs_rel = min_abs_rel_float
    end
    LastRewardState = reward

    return reward, bDone
end

-- Function to retrieve the last calculated reward (for display)
function M.getLastReward()
    return LastRewardState
end


return M --[[
    Tempest AI Lua Script for MAME - Refactored
    Author: Dave Plummer (davepl) and various AI assists
    Date: [2025-03-06] (Refactored Date)

    Overview: Refactored version focusing on modularity and clarity.
--]]

-- Dynamically add the script's directory to the package path
local script_path = debug.getinfo(1,"S").source:sub(2)
local script_dir = script_path:match("(.*[/\\])") or "./"
package.path = package.path .. ";" .. script_dir .. "?.lua"

-- Require modules
local display = require("display")
local state_defs = require("state")
local logic = require("logic") -- ADDED: Require the new logic module
local unpack = table.unpack or unpack -- Compatibility for unpack function

-- Constants
local SHOW_DISPLAY            = false
local START_ADVANCED          = false
local START_LEVEL_MIN         = 9
local DISPLAY_UPDATE_INTERVAL = 0.02
local SOCKET_ADDRESS          = "socket.ubdellamd:9999"
local SOCKET_READ_TIMEOUT_S   = 3.5  
local SOCKET_RETRY_WAIT_S     = 0.01
local CONNECTION_RETRY_INTERVAL_S = 5 -- How often to retry connecting (seconds)

-- MAME Interface Globals (initialized later)
local mainCpu                 = nil
local mem                     = nil

-- Global State
local current_socket = nil 
local total_bytes_sent = 0
local level_select_counter = 0
local shutdown_requested = false
local last_display_update = 0 -- Timestamp of last display update
local last_connection_attempt_time = 0 -- Timestamp of last connection attempt

-- Frame Skipping State
local last_timer_tick = -1
local frames_to_wait = 0 -- Process every tick by default
local frames_waited = 0

-- FPS Calculation State
local last_fps_time = os.time()
local last_frame_counter_for_fps = 0

-- Initialize MAME Interface (CPU and Memory)
local function initialize_mame_interface()
    local success, err = pcall(function()
        if not manager or not manager.machine then error("MAME manager.machine not available") end
        mainCpu = manager.machine.devices[":maincpu"]
        if not mainCpu then error("Main CPU not found") end
        mem = mainCpu.spaces["program"]
        if not mem then error("Program memory space not found") end
    end)

    if not success then
        print("Error accessing MAME via manager: " .. tostring(err))
        print("Attempting alternative access...")
        success, err = pcall(function()
            if not machine then error("Neither manager.machine nor machine is available") end
            mainCpu = machine.devices[":maincpu"]
            if not mainCpu then error("Main CPU not found via machine") end
            mem = mainCpu.spaces["program"]
            if not mem then error("Program memory space not found via machine") end
        end)

        if not success then
            print("Error with alternative access: " .. tostring(err))
            print("FATAL: Cannot access MAME memory.")
            return false -- Indicate failure
        end
    end
    print("MAME interface initialized successfully.")
    return true -- Indicate success
end

-- Socket Management
local function close_socket()
    if current_socket then
        current_socket:close()
        current_socket = nil
        -- print("Socket closed.") -- Optional: uncomment for debug
    end
end

local function open_socket()
    close_socket() -- Ensure any existing socket is closed first

    local socket_success, err = pcall(function()
        local sock = emu.file("rw")
        local result = sock:open(SOCKET_ADDRESS)
        if result == nil then
            -- Send initial 2-byte handshake message (required by server)
            local handshake_data = string.pack(">H", 0) -- 2-byte unsigned short, big-endian, value 0
            sock:write(handshake_data)

            current_socket = sock -- Assign to global only on success
            print("Socket connection opened to " .. SOCKET_ADDRESS)
            return true
        else
            print("Failed to open socket connection: " .. tostring(result))
            sock:close() -- Close the file handle if open failed
            return false
        end
    end)

    if not socket_success or not current_socket then
        print("Error during socket opening: " .. tostring(err or "unknown error"))
        close_socket() -- Ensure cleanup
        return false
    end
    return true
end

-- Controls Class (Simplified initialization)
local Controls = {}
Controls.__index = Controls

function Controls:new(mame_manager)
    local self = setmetatable({}, Controls)
    local ioport = mame_manager.machine.ioport

    local function find_port_field(port_name, field_name_options)
        local port = ioport.ports[port_name]
        if not port then print("Warning: Could not find port: " .. port_name); return nil end
        for _, field_name in ipairs(field_name_options) do
            local field = port.fields[field_name]
            if field then return field end
        end
        print("Warning: Could not find field " .. table.concat(field_name_options, "/") .. " in port " .. port_name)
        return nil
    end

    self.fire_field = find_port_field(":BUTTONSP1", {"P1 Button 1"})
    self.zap_field = find_port_field(":BUTTONSP1", {"P1 Button 2"})
    self.spinner_field = find_port_field(":KNOBP1", {"Dial"}) -- Spinner value is written directly to memory
    self.p1_start_field = find_port_field(":IN2", {"1 Player Start", "P1 Start", "Start 1"})

    return self
end

-- Apply received AI action and overrides to game controls
function Controls:apply_action(fire, zap, spinner, p1_start, memory)
    -- Debug values just before setting (simplified)
    -- print(string.format("[DEBUG apply_action] p1_start_val=%s, p1_start_field_valid=%s",
    --    tostring(p1_start),
    --    tostring(self.p1_start_field ~= nil)))

    if self.fire_field then self.fire_field:set_value(fire) end
    if self.zap_field then self.zap_field:set_value(zap) end

    if self.p1_start_field then
        -- Debug print *only* when attempting to set start=1
        -- if p1_start == 1 then
        --    print("[DEBUG apply_action] Attempting to set P1 Start = 1")
        -- end
    self.p1_start_field:set_value(p1_start)
    end

    -- Apply spinner value directly to memory (as before)
    local spinner_val = math.max(-128, math.min(127, spinner or 0))
    memory:write_u8(0x0050, spinner_val)
end

-- Instantiate state objects using state_defs
local game_state = state_defs.GameState:new()
local level_state = state_defs.LevelState:new()
local player_state = state_defs.PlayerState:new()
local enemies_state = state_defs.EnemiesState:new()
local controls = nil -- Initialized after MAME interface confirmed

-- Flatten game state to binary format for sending over socket
local function flatten_game_state_to_binary(reward, gs, ls, ps, es, bDone, expert_target_seg, expert_fire_packed, expert_zap_packed)
    local insert = table.insert -- Local alias for performance

    -- Helpers for normalized float32 packing - fail fast on out-of-range values!
    local function assert_range(v, lo, hi, context)
        if v < lo or v > hi then
            error(string.format("Value %g out of range [%g,%g] in %s", v, lo, hi, context or "unknown"))
        end
        return v
    end
    
    local function push_float32(parts, v)
        local val = tonumber(v) or 0.0
        -- CRITICAL: Assert final normalized value is within expected range [-1,1]
        if val < -1.0 or val > 1.0 then
            error(string.format("FINAL NORMALIZED VALUE %g out of range [-1,1] before serialization!", val))
        end
        insert(parts, string.pack(">f", val))  -- Big-endian float32
        return 1
    end
    
    -- Normalize natural 8-bit values [0,255] to [0,1]
    local function push_natural_norm(parts, v)
        local num = tonumber(v) or 0
        local validated = assert_range(num, 0, 255, "natural_8bit")
        local val = validated / 255.0
        return push_float32(parts, val)
    end
    
    -- Normalize relative segments with proper Tempest range handling
    local INVALID_SEGMENT = state_defs.INVALID_SEGMENT or -32768
    local function push_relative_norm(parts, v)
        local num = tonumber(v) or 0
        if num == INVALID_SEGMENT then
            return push_float32(parts, -1.0)  -- INVALID sentinel
        end
        -- Assert actual Tempest range and normalize to [-1,+1]
        local validated = assert_range(num, -15, 15, "relative_segment")
        local normalized = validated / 15.0  -- [-15,+15] → [-1,+1]
        return push_float32(parts, normalized)
    end
    
    -- Normalize boolean to [0,1]
    local function push_bool_norm(parts, v)
        local val = v and 1.0 or 0.0
        return push_float32(parts, val)
    end
    
    -- Normalize unit float [0,1] (should already be normalized - assert if not)
    local function push_unit_norm(parts, v)
        local num = tonumber(v) or 0
        local validated = assert_range(num, 0.0, 1.0, "unit_float")
        return push_float32(parts, validated)
    end

    local binary_data_parts = {}
    local num_values_packed = 0

    -- Game state (5) - all natural values
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, gs.gamestate)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, gs.game_mode)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, gs.countdown_timer)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, gs.p1_lives)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, gs.p1_level)

    -- Targeting / Engineered Features (4) - FIXED: Removed duplicate nearest_enemy_seg
    num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.nearest_enemy_seg)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.nearest_enemy_depth_raw)
    num_values_packed = num_values_packed + push_bool_norm(binary_data_parts, es.is_aligned_with_nearest > 0)
    num_values_packed = num_values_packed + push_unit_norm(binary_data_parts, es.alignment_error_magnitude)

    -- Player state (7 + 8 + 8 = 23)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.position)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.alive)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.player_state)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.player_depth)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.superzapper_uses)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.superzapper_active)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.shot_count)
    for i = 1, 8 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ps.shot_positions[i])
    end
    for i = 1, 8 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, ps.shot_segments[i])
    end

    -- Level state (3 + 16 + 16 = 35) - all natural values
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ls.level_number)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ls.level_type)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ls.level_shape)
    for i = 0, 15 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ls.spike_heights[i] or 0)
    end
    for i = 0, 15 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, ls.level_angles[i] or 0)
    end

    -- Enemies state (counts: 10 + other: 6 = 16) - all natural values
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.active_flippers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.active_pulsars)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.active_tankers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.active_spikers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.active_fuseballs)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.spawn_slots_flippers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.spawn_slots_pulsars)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.spawn_slots_tankers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.spawn_slots_spikers)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.spawn_slots_fuseballs)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.num_enemies_in_tube)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.num_enemies_on_top)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemies_pending)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.pulsar_fliprate)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.pulse_beat)
    num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.pulsing)

    -- Decoded Enemy Info (7 * 6 = 42) - all natural values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_core_type[i])
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_direction_moving[i])
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_between_segments[i])
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_moving_away[i])
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_can_shoot[i])
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_split_behavior[i])
    end

    -- Enemy segments (7) - relative values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.enemy_segments[i])
    end
    -- Enemy depths (7) - natural values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.enemy_depths[i])
    end
    -- Top Enemy Segments (7) - relative values
    for i = 1, 7 do
        local seg = (es.enemy_depths[i] == 0x10) and es.enemy_segments[i] or INVALID_SEGMENT
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, seg)
    end
    -- Enemy shot positions (4) - natural values
    for i = 1, 4 do
        num_values_packed = num_values_packed + push_natural_norm(binary_data_parts, es.shot_positions[i])
    end
    -- Enemy shot segments (4) - relative values
    for i = 1, 4 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.enemy_shot_segments[i])
    end
    -- Charging Fuseball segments (7) - relative values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.charging_fuseball[i])
    end
    -- Active Pulsar segments (7) - relative values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.active_pulsar[i])
    end
    -- Top Rail Enemy segments (7) - relative values
    for i = 1, 7 do
        num_values_packed = num_values_packed + push_relative_norm(binary_data_parts, es.active_top_rail_enemies[i])
    end

    -- Total main payload size: 175

    -- Serialize main data to binary string (float32 values)
    local binary_data = table.concat(binary_data_parts)

    -- VALIDATION: Debug print to verify key segment encodings (first few frames only)
    if gs.frame_counter < 5 then
        print(string.format("[DEBUG] Frame %d: Nearest enemy seg=%.3f, Player shot segs=[%.3f,%.3f,%.3f,%.3f]",
            gs.frame_counter,
            (es.nearest_enemy_seg == INVALID_SEGMENT) and -1.0 or (es.nearest_enemy_seg / 15.0),
            (ps.shot_segments[1] == INVALID_SEGMENT) and -1.0 or (ps.shot_segments[1] / 15.0),
            (ps.shot_segments[2] == INVALID_SEGMENT) and -1.0 or (ps.shot_segments[2] / 15.0),
            (ps.shot_segments[3] == INVALID_SEGMENT) and -1.0 or (ps.shot_segments[3] / 15.0),
            (ps.shot_segments[4] == INVALID_SEGMENT) and -1.0 or (ps.shot_segments[4] / 15.0)
        ))
    end

    -- --- OOB Data Packing ---
    local is_attract_mode = (gs.game_mode & 0x80) == 0
    local is_open_level = ls.level_type == 0xFF
    local score = ps.score or 0
    local score_high = math.floor(score / 65536)
    local score_low = score % 65536
    local frame = gs.frame_counter % 65536

    -- Save signal logic
    local current_time = os.time()
    local save_signal = 0
    if shutdown_requested or current_time - gs.last_save_time >= gs.save_interval then
        save_signal = 1
        gs.last_save_time = current_time
        if shutdown_requested then print("SHUTDOWN SAVE: Sending final save signal.")
        else print("Periodic Save: Sending save signal.") end
    end

    -- Fetch last reward components to transmit out-of-band (avoid recomputation in Python)
    -- Pack OOB data (header only, reward components removed for simplicity)
    -- NOTE: The short (h) after attract encodes the NEAREST ENEMY ABSOLUTE SEGMENT for Python expert steering.
    -- Format legend:
    --   >HdBBBHHHBBBhBhBBBBB
    --   H: num_values, d: reward, BBB: (gamestate, game_mode, done), HHH: (frame, score_hi, score_lo),
    --   BBB: (save, fire, zap), h: spinner, B: attract, h: nearest_enemy_abs_seg, B: player_seg, B: is_open,
    --   BB: (expert_fire, expert_zap), B: level_number
    local oob_format = ">HdBBBHHHBBBhBhBBBBB"
    -- Determine nearest enemy absolute segment to transmit (or -1 if none)
    local oob_nearest_enemy_abs_seg = es.nearest_enemy_abs_seg_internal or -1
    local oob_data = string.pack(oob_format,
        num_values_packed,          -- H: Number of values in main payload (ushort)
        reward,                     -- d: Reward (double)
        gs.gamestate,               -- B: Gamestate (uchar) - was placeholder
        gs.game_mode,               -- B: Game Mode (uchar)
        bDone and 1 or 0,           -- B: Done flag (uchar)
        frame,                      -- H: Frame counter (ushort)
        score_high,                 -- H: Score High (ushort)
        score_low,                  -- H: Score Low (ushort)
        save_signal,                -- B: Save Signal (uchar)
        ps.fire_commanded,          -- B: Commanded Fire (uchar)
        ps.zap_commanded,           -- B: Commanded Zap (uchar)
        ps.spinner_commanded,       -- h: Commanded Spinner (short)o
        is_attract_mode and 1 or 0, -- B: Is Attract Mode (uchar)
        oob_nearest_enemy_abs_seg,  -- h: Nearest Enemy ABS Segment (short)
        ps.position & 0x0F,         -- B: Player Abs Segment (uchar)
        is_open_level and 1 or 0,   -- B: Is Open Level (uchar)
        expert_fire_packed,         -- B: Expert Fire (uchar)
        expert_zap_packed,          -- B: Expert Zap (uchar)
        ls.level_number             -- B: Current Level Number (uchar)
    )

    -- Combine OOB header + main data
    local final_data = oob_data .. binary_data

    -- DEBUG: Updated length info for float32 payload: OOB ~32 bytes, Main=704 bytes -> Total ~736 bytes  
    -- print(string.format("Packed lengths: OOB~%d, Main=%d, Total=%d, Num values: %d", 32, #binary_data, #final_data, num_values_packed))

    return final_data, num_values_packed
end


-- Send state and receive action via socket
local function process_frame_via_socket(rawdata)
    -- Ensure socket connection
    if not current_socket then
        if not open_socket() then
            return 0, 0, 0, false -- Return zeros and error flag
        end
    end

    -- Attempt to write data with length header
    local write_success, write_err = pcall(function()
        local data_length = #rawdata
        local length_header = string.pack(">H", data_length) -- Unsigned short, big-endian length
        current_socket:write(length_header)
        current_socket:write(rawdata)
    end)

    if not write_success then
        print("Error writing to socket: " .. tostring(write_err) .. ". Attempting reconnect.")
        close_socket()
        open_socket() -- Try immediate reconnect
        return 0, 0, 0, false -- Return zeros and error flag
    end

    -- Attempt to read action with timeout
    local fire, zap, spinner = 0, 0, 0
    local read_success, read_result = pcall(function()
        local action_bytes = nil
        local read_start_time = os.clock()
        local elapsed = 0

        while elapsed < SOCKET_READ_TIMEOUT_S do
            -- Try reading 3 bytes for the action (b, b, b)
            action_bytes = current_socket:read(3)

            if action_bytes and #action_bytes == 3 then
                -- Successfully read 3 bytes
                 return { string.unpack("bbb", action_bytes) } -- Return unpacked values
            end

            -- If read failed or got partial data, just loop and rely on main timeout
            -- No explicit wait here; os.clock() check handles timing.
            elapsed = os.clock() - read_start_time
        end

        -- Loop finished without getting 3 bytes (Timeout)
        print("Socket read timeout after " .. string.format("%.3f", elapsed) .. "s. Expected 3 bytes.")
        if elapsed >= SOCKET_READ_TIMEOUT_S then
             print("Socket read timeout exceeded, attempting reconnect...")
             close_socket()
             open_socket()
        end
        return { 0, 0, 0 } -- Default action on timeout

    end)

    if not read_success then
        print("Error reading from socket: " .. tostring(read_result) .. ". Attempting reconnect.")
        close_socket()
        open_socket()
        return 0, 0, 0, false -- Return zeros and error flag
    end

    -- Return the received action values and success flag
    fire, zap, spinner = unpack(read_result) -- Unpack results from the table returned by pcall
    return fire, zap, spinner, true
end

-- Update all game state objects
local function update_game_states(memory)
    game_state:update(memory)
    level_state:update(memory)
    player_state:update(memory, logic.absolute_to_relative_segment) -- Pass helper from logic module
    enemies_state:update(memory, game_state, player_state, level_state, logic.absolute_to_relative_segment) -- Pass dependencies & helper
    -- DEBUG: Print game_mode immediately after update
    -- print(string.format("[DEBUG state update] Frame: %d, game_mode: 0x%02X", game_state.frame_counter, game_state.game_mode))
end

-- Perform AI interaction (calculate reward, expert advice, send state, receive action)
local function handle_ai_interaction()
    -- Calculate reward based on current state and detected actions
    local reward, episode_done = logic.calculate_reward(game_state, level_state, player_state, enemies_state, logic.absolute_to_relative_segment)
    
    -- NOTE: Removed reward clamping [-1,1] since rewards are now properly scaled in logic.calculate_reward()    -- Calculate expert advice (target segment, fire, zap)
    local is_open_level = (level_state.level_number - 1) % 4 == 2
    local expert_target_seg, _, expert_should_fire_lua, expert_should_zap_lua = logic.find_target_segment(
        game_state, player_state, level_state, enemies_state, logic.absolute_to_relative_segment, is_open_level
    )
    local expert_fire_packed = expert_should_fire_lua and 1 or 0
    local expert_zap_packed = expert_should_zap_lua and 1 or 0

    -- Default values if socket is not connected
    local received_fire_cmd, received_zap_cmd, received_spinner_cmd = 0, 0, 0
    local socket_ok = false
    local num_values = 0 -- Default if not sending

    -- Only attempt network ops if socket exists and seems valid
    if current_socket then
        -- Flatten current state (s') including reward (r) and done (d)
        local frame_data -- Declare frame_data here
        frame_data, num_values = flatten_game_state_to_binary(reward, game_state, level_state, player_state, enemies_state, episode_done, expert_target_seg, expert_fire_packed, expert_zap_packed)

        -- Send s', r, d; Receive action a for s'
        received_fire_cmd, received_zap_cmd, received_spinner_cmd, socket_ok = process_frame_via_socket(frame_data)

        -- Update total bytes sent (only if socket write was likely successful)
        if socket_ok then -- Assuming socket_ok implies write likely succeeded before read attempt
            total_bytes_sent = total_bytes_sent + #frame_data
        end
    else
        -- Socket doesn't exist, attempt to open it periodically
        local current_time = os.time()
        if current_time - last_connection_attempt_time > CONNECTION_RETRY_INTERVAL_S then
            -- print(string.format("[handle_ai_interaction] No active socket, attempting connect retry (Last attempt: %ds ago).", current_time - last_connection_attempt_time))
            last_connection_attempt_time = current_time -- Update time *before* attempting
            open_socket() -- Attempt connection
        end
    end

    -- Store received commands (will be 0s if no socket or read failed)
    player_state.fire_commanded = received_fire_cmd
    player_state.zap_commanded = received_zap_cmd
    player_state.spinner_commanded = received_spinner_cmd

    return episode_done, socket_ok, num_values -- Return done flag, socket status, and num_values for display
end

-- Determine the final action based on game state and AI commands (Returns: fire, zap, spinner, start)
local function determine_final_actions()
    -- Initialize all commands to 0, apply overrides below
    local final_fire_cmd = 0
    local final_zap_cmd = 0
    local final_spinner_cmd = 0
    local final_p1_start_cmd = 0
    local is_attract_mode = (game_state.game_mode & 0x80) == 0

    -- Override based on game state
    if game_state.gamestate == 0x12 then -- High Score Entry
        final_fire_cmd = (game_state.frame_counter % 10 == 0) and 1 or 0
        -- Zap, Spinner, Start remain 0
    elseif game_state.gamestate == 0x16 then -- Level Select
        -- DEBUG: Log Level Select State
        -- print(string.format("[DEBUG LevelSelect] Frame: %d, Timer: %d, Counter: %d", game_state.frame_counter, mem and mem:read_u8(0x0003) or -1, level_select_counter))

        if level_select_counter < 60 then
            if (START_ADVANCED) then
                final_spinner_cmd = 18; -- Fire, Zap, Start remain 0
            end
            level_select_counter = level_select_counter + 1
            -- print("  -> Spinning knob (Counter now " .. level_select_counter .. ")")
        elseif level_select_counter == 60 then
            final_fire_cmd = 1; -- Spinner, Zap, Start remain 0
            level_select_counter = level_select_counter + 1 -- Increment past 60 immediately
            -- print("  -> Pressing Fire! (Counter now " .. level_select_counter .. ")")
        else -- Counter is > 60, just do nothing, don't reset here
             -- All commands remain 0
             -- print("  -> Level selected, doing nothing.")
        end
        -- Resetting the counter should ONLY happen in attract mode.
    elseif is_attract_mode then -- Attract Mode
        local should_press_start = (game_state.frame_counter % 50 == 0)
        final_p1_start_cmd = should_press_start and 1 or 0
        -- Fire, Zap, Spinner remain 0

        -- DEBUG: Log counter reset
        -- if level_select_counter ~= 0 then
        --    print("[DEBUG Attract] Resetting level_select_counter from " .. level_select_counter .. " to 0")
        -- end
        level_select_counter = 0 -- Reset level select counter here

    elseif game_state.gamestate == 0x04 or game_state.gamestate == 0x20 or game_state.gamestate == 0x20 then -- Normal Play or Tube Zoom
        -- Use AI commands stored in player_state
        final_fire_cmd = player_state.fire_commanded
        final_zap_cmd = player_state.zap_commanded
        final_spinner_cmd = player_state.spinner_commanded
        -- Start remains 0
    else
        -- Unknown state, all commands default to 0
        -- print(string.format("[WARN] Unknown game state 0x%02X encountered in determine_final_actions", game_state.gamestate))
    end

    return final_fire_cmd, final_zap_cmd, final_spinner_cmd, final_p1_start_cmd
end

-- Update the console display if enabled and interval has passed
local function update_display_if_needed(num_values_packed)
    local current_time_high_res = os.clock()
    if SHOW_DISPLAY and (current_time_high_res - last_display_update) >= DISPLAY_UPDATE_INTERVAL then
        display.update("Running", game_state, level_state, player_state, enemies_state, num_values_packed, logic.getLastReward(), total_bytes_sent)
        last_display_update = current_time_high_res
    end
end

-- Apply cheats/overrides
local function apply_overrides(memory)
    memory:write_u8(0x0006, 2) -- Credits
    memory:write_direct_u8(0xA591, 0xEA) -- NOP Copy Prot
    memory:write_direct_u8(0xA592, 0xEA) -- NOP Copy Prot

    -- NOP out the start level check
    -- memory:write_direct_u8(0x90CD, 0xEA) -- NOP
    -- memory:write_direct_u8(0x90CE, 0xEA) -- NOP

    if (memory:read_u8(0x0126) < START_LEVEL_MIN) then
        memory:write_direct_u8(0x0126, START_LEVEL_MIN) -- NOP out the "Level Select" check
    end
end


-- Main frame callback for MAME
local function frame_callback()
    -- Frame skipping logic
    local currentTimer = mem:read_u8(0x0003)
    if currentTimer == last_timer_tick then return true end
    last_timer_tick = currentTimer
    frames_waited = frames_waited + 1
    if frames_waited <= frames_to_wait then return true end
    frames_waited = 0

    -- Calculate FPS
    local current_time = os.time()
    if current_time > last_fps_time then
        game_state.current_fps = game_state.frame_counter - last_frame_counter_for_fps
        last_frame_counter_for_fps = game_state.frame_counter
        last_fps_time = current_time
    end

    -- Update state from MAME memory
    update_game_states(mem)
  
    -- Apply overrides/cheats
    apply_overrides(mem)

    -- Handle AI Interaction (Send state s', get action a)
    local episode_done, socket_ok, num_values_packed = handle_ai_interaction()

    -- Determine final action based on AI input and game state
    local final_fire, final_zap, final_spinner, final_p1_start = determine_final_actions()

    -- DEBUG: Print final commands before applying
    -- print(string.format("[DEBUG Final Apply] Frame=%d, State=0x%02X, Fire=%d, Zap=%d, Spin=%d, Start=%d",
    --    game_state.frame_counter, game_state.gamestate, final_fire, final_zap, final_spinner, final_p1_start))

    -- Apply actions to controls
    controls:apply_action(final_fire, final_zap, final_spinner, final_p1_start, mem)

    -- Update console display periodically
    update_display_if_needed(num_values_packed)

    return true -- Indicate success to MAME
end

-- Function called when MAME is shutting down
local function on_mame_exit()
    print("MAME is shutting down...")
    shutdown_requested = true -- Signal for final save

    -- Try to process one final frame to send save signal if possible
    if mainCpu and mem and controls and current_socket then
        print("Processing final frame for save signal...")
        update_game_states(mem) -- Get final state
        -- Call AI handler - this calculates reward and flattens state with save signal
        handle_ai_interaction() -- Ignore return values, just need to send state
        print("Final frame processed and sent.")
    else
         print("Could not process final frame: MAME interface, controls, or socket not available.")
    end

    close_socket() -- Ensure socket is closed
    print("Shutdown complete.")
end

-- --- Script Initialization ---
math.randomseed(os.time())

-- Initialize MAME interface first
if not initialize_mame_interface() then
    return -- Stop script if MAME interface failed
end

-- Initialize controls now that MAME interface is confirmed
controls = Controls:new(manager)

-- Attempt initial socket connection
open_socket()

-- Register callbacks with MAME
-- Store reference globally like original script, in case of MAME GC quirks
global_callback_ref = emu.add_machine_frame_notifier(frame_callback)
emu.add_machine_stop_notifier(on_mame_exit)

print("Tempest AI script initialized and callbacks registered.")
--[[ End of main.lua ]]--



--[[
    state.lua
    Contains class definitions for game state objects (Game, Level, Player, Enemies)
    and related helper functions for the Tempest AI project.
--]]

local M = {} -- Module table to hold exported classes and functions

-- Define constants
local INVALID_SEGMENT = -32768 -- Used as sentinel value for invalid segments
M.INVALID_SEGMENT = INVALID_SEGMENT -- Export if needed elsewhere

-- Correct Enemy Type Constants (from assembly)
local ENEMY_TYPE_FLIPPER  = 0
local ENEMY_TYPE_PULSAR   = 1
local ENEMY_TYPE_TANKER   = 2
local ENEMY_TYPE_SPIKER   = 3
local ENEMY_TYPE_FUSEBALL = 4
local ENEMY_TYPE_MASK = 0x07 -- <<< ADDED THIS DEFINITION (%00000111)

-- Helper function for BCD conversion (local to this module)
local function bcd_to_decimal(bcd)
    -- Handle potential non-number inputs gracefully
    if type(bcd) ~= 'number' then return 0 end
    return math.floor(((bcd / 16) % 16) * 10 + (bcd % 16))
end

-- Forward declarations for helper functions used within classes/other helpers
-- Note: abs_to_rel_func will be passed in from main.lua where needed.
local find_nearest_enemy_of_type
local hunt_enemies
local find_target_segment
local find_forbidden_segments
local find_nearest_safe_segment

-- ====================
-- Helper Functions (Internal to this State Module)
-- ====================

-- NEW Helper: Identify forbidden segments
find_forbidden_segments = function(enemies_state, level_state, player_state)
    local forbidden = {} -- Use a table as a set (keys are forbidden segments 0-15)
    local is_pulsing = enemies_state.pulsing ~= 0
    -- print(string.format("FIND_FORBIDDEN: Pulsing active = %s", tostring(is_pulsing))) -- DEBUG

    -- Check enemies
    for i = 1, 7 do
        local core_type = enemies_state.enemy_core_type[i]
        local abs_seg = enemies_state.enemy_abs_segments[i]
        local depth = enemies_state.enemy_depths[i]

        if abs_seg ~= INVALID_SEGMENT and depth > 0 then
            -- 1. Pulsing Pulsar Lane (Check for Pulsar type 1)
            if core_type == ENEMY_TYPE_PULSAR and is_pulsing then -- <<< CORRECTED TYPE
                -- print(string.format("  -> FORBIDDEN (Pulsing Pulsar): Slot %d, Seg %d", i, abs_seg)) -- DEBUG
                forbidden[abs_seg] = true
            end
            -- 2. Top-level enemies (depth <= 0x10)
            -- Includes Flippers, Pulsars, Tankers, Fuseballs, Spikers if they are close
            if depth <= 0x10 then
                 -- print(string.format("  -> FORBIDDEN (Top Enemy): Slot %d, Type %d, Seg %d, Depth %02X", i, core_type, abs_seg, depth)) -- DEBUG
                 forbidden[abs_seg] = true
            end
        end
    end

    -- Check enemy shots
    local has_ammo = player_state.shot_count < 8 -- Check if player can shoot back
    -- print(string.format("FIND_FORBIDDEN: Player can shoot = %s (Shot count %d)", tostring(has_ammo), player_state.shot_count)) -- DEBUG
    for i = 1, 4 do
        local shot_abs_seg = enemies_state.enemy_shot_abs_segments[i]
        local shot_depth = enemies_state.shot_positions[i]
        -- Mark forbidden if shot is close AND player cannot shoot back
        if shot_abs_seg ~= INVALID_SEGMENT and shot_depth > 0 and shot_depth <= 0x30 and not has_ammo then
            -- print(string.format("  -> FORBIDDEN (Enemy Shot): Shot %d, Seg %d, Depth %02X", i, shot_abs_seg, shot_depth)) -- DEBUG
            forbidden[shot_abs_seg] = true
        end
    end
    return forbidden
end

-- NEW Helper: Find nearest safe segment if current is forbidden
find_nearest_safe_segment = function(player_abs_seg, is_open, forbidden_segments, abs_to_rel_func)
    -- Search outwards from the current segment
    local max_dist = is_open and 15 or 8
    for dist = 1, max_dist do
        -- Check Left
        local left_target_seg = -1
        if is_open then
            if player_abs_seg - dist >= 0 then left_target_seg = player_abs_seg - dist end
        else -- Closed
            left_target_seg = (player_abs_seg - dist + 16) % 16
        end

        if left_target_seg ~= -1 and not forbidden_segments[left_target_seg] then
            return left_target_seg -- Found safe segment to the left
        end

        -- Check Right
        local right_target_seg = -1
         if is_open then
            if player_abs_seg + dist <= 15 then right_target_seg = player_abs_seg + dist end
        else -- Closed
            right_target_seg = (player_abs_seg + dist) % 16
        end

         if right_target_seg ~= -1 and not forbidden_segments[right_target_seg] then
            return right_target_seg -- Found safe segment to the right
        end
    end

    -- If no safe segment found (highly unlikely unless all are forbidden)
    print("WARNING: No safe segment found to flee to! Staying put.")
    return player_abs_seg
end

-- Helper function to find nearest enemy of a specific type
-- Needs abs_to_rel_func, is_open, and forbidden_segments passed in
find_nearest_enemy_of_type = function(enemies_state, player_abs_segment, is_open, type_id, abs_to_rel_func, forbidden_segments)
    local nearest_seg_abs = -1
    local nearest_depth = 255
    local min_distance = 255 -- Use a large initial distance

    -- if type_id == ENEMY_TYPE_TANKER then print(string.format("FIND_NEAREST DEBUG: Hunting Tankers (Type %d)", type_id)) end -- DEBUG

    for i = 1, 7 do
        local core_type = enemies_state.enemy_core_type[i]
        local enemy_abs_seg = enemies_state.enemy_abs_segments[i]
        local enemy_depth = enemies_state.enemy_depths[i]
        local is_forbidden = (enemy_abs_seg ~= INVALID_SEGMENT) and forbidden_segments[enemy_abs_seg] or false

        --[[ DEBUG specific to Tankers
        if type_id == ENEMY_TYPE_TANKER then
            print(string.format(
                "  Slot %d: Type=%d, AbsSeg=%s, Depth=%02X, IsForbidden=%s, PassesChecks=%s",
                i, core_type, tostring(enemy_abs_seg), enemy_depth, tostring(is_forbidden),
                tostring(core_type == type_id and enemy_abs_seg ~= INVALID_SEGMENT and enemy_depth > 0x30 and not is_forbidden)
            ))
        end
        --]]

        -- Check if this is the enemy type we're looking for, if it's active,
        -- if its depth is > 0x30, AND if the segment is NOT forbidden
        if core_type == type_id and
           enemy_abs_seg ~= INVALID_SEGMENT and
           enemy_depth > 0x30 and
           not is_forbidden then

            -- Calculate distance using the provided function
            local rel_dist = abs_to_rel_func(player_abs_segment, enemy_abs_seg, is_open)
            local abs_dist = math.abs(rel_dist)

            -- Check if this enemy is closer than the current nearest
            if abs_dist < min_distance then
                min_distance = abs_dist
                nearest_seg_abs = enemy_abs_seg
                nearest_depth = enemy_depth
            -- Optional: Prioritize closer depth if distances are equal
            elseif abs_dist == min_distance and enemy_depth < nearest_depth then
                nearest_seg_abs = enemy_abs_seg
                nearest_depth = enemy_depth
            end
        end
    end

    return nearest_seg_abs, nearest_depth
end

-- Helper function to hunt enemies in preference order
-- Needs abs_to_rel_func, is_open, and forbidden_segments passed in
hunt_enemies = function(enemies_state, player_abs_segment, is_open, abs_to_rel_func, forbidden_segments)
    -- Corrected Hunt Order (based on assembly types): Fuseball(4), Pulsar(1), Tanker(2), Flipper(0), Spiker(3)
    local hunt_order = {
        ENEMY_TYPE_FUSEBALL, -- 4
        ENEMY_TYPE_TANKER,   -- 2
        ENEMY_TYPE_FLIPPER,  -- 0
        ENEMY_TYPE_SPIKER,   -- 3
        ENEMY_TYPE_PULSAR    -- 1
    }

    for _, enemy_type in ipairs(hunt_order) do
        local target_seg_abs, target_depth = find_nearest_enemy_of_type(enemies_state, player_abs_segment, is_open, enemy_type, abs_to_rel_func, forbidden_segments)
        if target_seg_abs ~= -1 then
            -- Check for Top Rail Flipper(0)/Pulsar(1) Avoidance
            if (enemy_type == ENEMY_TYPE_FLIPPER or enemy_type == ENEMY_TYPE_PULSAR) and target_depth <= 0x10 then -- <<< CORRECTED TYPE
                local rel_dist = abs_to_rel_func(player_abs_segment, target_seg_abs, is_open)
                if math.abs(rel_dist) <= 1 then -- If aligned or adjacent
                    local safe_adjacent_seg
                    if rel_dist <= 0 then -- Threat is left or aligned, move right
                        safe_adjacent_seg = (player_abs_segment + 1) % 16
                    else -- Threat is right, move left
                        safe_adjacent_seg = (player_abs_segment - 1 + 16) % 16
                    end
                    -- print(string.format("HUNT AVOID: Top Type %d at %d (Rel %d). Targeting adjacent safe %d", enemy_type, target_seg_abs, rel_dist, safe_adjacent_seg))
                    if forbidden_segments[safe_adjacent_seg] then
                         -- print("HUNT AVOID: Adjacent safe segment " .. safe_adjacent_seg .. " is forbidden! Staying put.")
                         return player_abs_segment, target_depth, true -- Stay put, but mark as avoiding
                    else
                         return safe_adjacent_seg, target_depth, true -- Target safe adjacent, mark as avoiding
                    end
                else
                    -- Top Flipper/Pulsar is not adjacent, target directly
                    return target_seg_abs, target_depth, false
                end
            else
                -- Not a top-rail Flipper/Pulsar, target directly
                return target_seg_abs, target_depth, false
            end
        end
    end

    -- If no enemies from the hunt order are found
    return -1, 255, false
end

-- Function to determine target segment, depth, and firing decision
find_target_segment = function(game_state, player_state, level_state, enemies_state, abs_to_rel_func, is_open)
    local player_abs_seg = player_state.position & 0x0F

    local initial_target_seg_abs
    local target_depth
    local should_fire = false
    local should_zap = false
    local did_flee = false
    local hunting_target_info = "N/A"

    -- Check for Tube Zoom state first
    if game_state.gamestate == 0x20 then
        -- Spike heights are depths: 0=clear/no spike, >0 means spike exists, smaller number = longer spike
        local current_spike_h = level_state.spike_heights[player_abs_seg]
        if current_spike_h == 0 then return player_abs_seg, 0, true, false end
        local left_neighbour_seg = -1
        local right_neighbour_seg = -1
        if is_open then
            if player_abs_seg > 0 then left_neighbour_seg = player_abs_seg - 1 end
            if player_abs_seg < 15 then right_neighbour_seg = player_abs_seg + 1 end
        else
            left_neighbour_seg = (player_abs_seg - 1 + 16) % 16
            right_neighbour_seg = (player_abs_seg + 1) % 16
        end
        local left_spike_h = -1; if left_neighbour_seg ~= -1 then left_spike_h = level_state.spike_heights[left_neighbour_seg] end
        local right_spike_h = -1; if right_neighbour_seg ~= -1 then right_spike_h = level_state.spike_heights[right_neighbour_seg] end
        if left_spike_h == 0 then return left_neighbour_seg, 0, true, false end
        if right_spike_h == 0 then return right_neighbour_seg, 0, true, false end
        local temp_target = player_abs_seg
        local is_left_better = (left_spike_h > current_spike_h)
        local is_right_better = (right_spike_h > current_spike_h)
        if is_left_better and is_right_better then temp_target = (left_spike_h >= right_spike_h) and left_neighbour_seg or right_neighbour_seg
        elseif is_left_better then temp_target = left_neighbour_seg
        elseif is_right_better then temp_target = right_neighbour_seg
        end
        initial_target_seg_abs, target_depth, should_fire, should_zap = (function()
           -- ... (spike logic returns temp_target, 0, true, false)
           return temp_target, 0, true, false
        end)()
    -- Check Flee/Hunt Logic (only in normal play mode)
    elseif game_state.gamestate == 0x04 then
        local forbidden_segments = find_forbidden_segments(enemies_state, level_state, player_state)
        local current_segment_is_forbidden = forbidden_segments[player_abs_seg] or false

        if current_segment_is_forbidden then
            did_flee = true
            initial_target_seg_abs = find_nearest_safe_segment(player_abs_seg, is_open, forbidden_segments, abs_to_rel_func)
            target_depth = 0 -- Depth isn't the focus when fleeing
            should_fire = false
            should_zap = false
        else -- Current segment is SAFE, proceed to HUNT
            -- Pass forbidden_segments to hunt_enemies
            local hunt_target_seg, hunt_target_depth, should_avoid = hunt_enemies(enemies_state, player_abs_seg, is_open, abs_to_rel_func, forbidden_segments)
            hunting_target_info = string.format("HuntTgt=%d, HuntDepth=%02X", hunt_target_seg, hunt_target_depth) -- DEBUG

            if hunt_target_seg ~= -1 then
                initial_target_seg_abs = hunt_target_seg
                target_depth = hunt_target_depth
                local rel_dist = abs_to_rel_func(player_abs_seg, initial_target_seg_abs, is_open)
                should_fire = (rel_dist <= 1) -- Initial fire recommendation if aligned
            else
                initial_target_seg_abs = player_abs_seg -- Stay put if no hunt target
                target_depth = 0x10
                should_fire = false
            end
        end
    else -- Other game states: Stay put
        initial_target_seg_abs = player_abs_seg
        target_depth = player_state.player_depth
        should_fire = false
        should_zap = false
    end

    -- Apply panic braking
    local final_target_seg_abs = initial_target_seg_abs
    local did_brake = false
    if final_target_seg_abs ~= player_abs_seg and game_state.gamestate == 0x04 then
        local initial_relative_dist = abs_to_rel_func(player_abs_seg, final_target_seg_abs, is_open)
        local next_segment_abs = -1

        if initial_relative_dist > 0 then -- Moving right (positive relative dist)
            if is_open then
                if player_abs_seg < 15 then next_segment_abs = player_abs_seg + 1 end
            else -- Closed
                next_segment_abs = (player_abs_seg + 1) % 16
            end
        elseif initial_relative_dist < 0 then -- Moving left (negative relative dist)
             if is_open then
                if player_abs_seg > 0 then next_segment_abs = player_abs_seg - 1 end
            else -- Closed
                next_segment_abs = (player_abs_seg - 1 + 16) % 16
            end
        end

        -- If there is a valid next segment to check
        if next_segment_abs ~= -1 then
            local brake_condition_met = false
            -- Check enemy shots in the next segment
            for i = 1, 4 do
                if enemies_state.enemy_shot_abs_segments[i] == next_segment_abs and
                   enemies_state.shot_positions[i] > 0 and
                   enemies_state.shot_positions[i] <= 0x30 then
                    brake_condition_met = true; break
                end
            end
            -- Check Flippers (0) and Pulsars (1) in the next segment if no shot found yet
            if not brake_condition_met then
                for i = 1, 7 do
                    -- Check for Flipper OR Pulsar if close
                    if (enemies_state.enemy_core_type[i] == ENEMY_TYPE_FLIPPER or enemies_state.enemy_core_type[i] == ENEMY_TYPE_PULSAR) and -- <<< CORRECTED TYPE
                       enemies_state.enemy_abs_segments[i] == next_segment_abs and
                       enemies_state.enemy_depths[i] > 0 and
                       enemies_state.enemy_depths[i] <= 0x30 then
                        brake_condition_met = true; break
                    end
                end
            end

            -- If brake condition met, override target to stay put
            if brake_condition_met then
                did_brake = true -- Record brake engagement
                final_target_seg_abs = player_abs_seg -- Override the target
                target_depth = player_state.player_depth -- Use current depth (might not matter)
                should_fire = false -- Don't fire if braking
                should_zap = false
            end
        end
    end

    -- NEW: Fuseball Avoidance Logic (after panic brake)
    local fuseball_avoid_target = -1
    local min_fuseball_dist = 3 -- Minimum desired distance from a top-level fuseball

    -- Check if the current target is too close to a top-level fuseball
    local too_close = false
    for i = 1, 7 do
        if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FUSEBALL and -- Is it a Fuseball? (Correct type 4)
           enemies_state.enemy_depths[i] <= 0x10 and -- Is it at the top?
           enemies_state.enemy_abs_segments[i] ~= INVALID_SEGMENT then

            local fuseball_abs_seg = enemies_state.enemy_abs_segments[i]
            local dist_to_fuseball = math.abs(abs_to_rel_func(final_target_seg_abs, fuseball_abs_seg, is_open))

            if dist_to_fuseball < min_fuseball_dist then
                too_close = true
                break -- Found one too close, no need to check others
            end
        end
    end

    -- If too close, find the nearest segment >= min_fuseball_dist away from ALL top fuseballs
    if too_close then
        print("FUSEBALL AVOID: Target " .. final_target_seg_abs .. " too close. Finding alternative.")
        local best_safe_seg = -1
        local search_max_dist = is_open and 15 or 8

        for search_dist = 0, search_max_dist do -- Start search from current pos (dist 0)
            local segments_to_check = {}
            if search_dist == 0 then
                segments_to_check = {player_abs_seg}
            else
                -- Check Left
                local left_check = -1
                if is_open then
                    if player_abs_seg - search_dist >= 0 then left_check = player_abs_seg - search_dist end
                else left_check = (player_abs_seg - search_dist + 16) % 16 end
                if left_check ~= -1 then table.insert(segments_to_check, left_check) end

                -- Check Right
                local right_check = -1
                 if is_open then
                    if player_abs_seg + search_dist <= 15 then right_check = player_abs_seg + search_dist end
                else right_check = (player_abs_seg + search_dist) % 16 end
                 if right_check ~= -1 and right_check ~= left_check then table.insert(segments_to_check, right_check) end
            end

            for _, check_seg in ipairs(segments_to_check) do
                local is_seg_safe = true
                -- Check this segment against ALL top-level fuseballs
                for i = 1, 7 do
                     if enemies_state.enemy_core_type[i] == ENEMY_TYPE_FUSEBALL and enemies_state.enemy_depths[i] <= 0x10 and enemies_state.enemy_abs_segments[i] ~= INVALID_SEGMENT then -- Correct type 4
                        local fuseball_abs_seg = enemies_state.enemy_abs_segments[i]
                        local dist = math.abs(abs_to_rel_func(check_seg, fuseball_abs_seg, is_open))
                        if dist < min_fuseball_dist then
                            is_seg_safe = false
                            break -- This segment is too close to this fuseball
                        end
                    end
                end

                if is_seg_safe then
                    best_safe_seg = check_seg
                    goto found_safe_segment -- Exit outer loops once the *nearest* safe segment is found
                end
            end
        end
        ::found_safe_segment::

        if best_safe_seg ~= -1 then
            print("FUSEBALL AVOID: New target = " .. best_safe_seg)
            final_target_seg_abs = best_safe_seg
            target_depth = 0 -- Reset depth indication
            should_fire = false -- Don't fire when avoiding
            should_zap = false
        else
             print("WARNING: Fuseball avoidance could not find any safe segment!")
             -- Keep original target if no safe alternative found (might be stuck)
        end
    end

    -- Apply nearby Flipper firing override (Run this check regardless of brake/avoidance, but before shot count limit)
    local initial_should_fire = should_fire -- Store initial recommendation before override
    if not initial_should_fire then -- Only override if not already firing
        for i = 1, 7 do
            if enemies_state.enemy_depths[i] > 0 and enemies_state.enemy_depths[i] <= 0x30 then -- Is it close vertically?
                local threat_abs_seg = enemies_state.enemy_abs_segments[i]
                if threat_abs_seg ~= INVALID_SEGMENT then
                    local threat_rel_seg = abs_to_rel_func(player_abs_seg, threat_abs_seg, is_open)
                    if math.abs(threat_rel_seg) <= 1 then -- Is it close laterally (or aligned)?
                        -- Always recommend firing at close threats
                        should_fire = true

                        -- Expert tweak: if shot buffer is full, still press FIRE but move one segment away
                        -- Apply only for Flipper/Pulsar top-rail threats per design intent
                        if player_state.shot_count >= 8 then
                            local core_type = enemies_state.enemy_core_type[i]
                            if core_type == ENEMY_TYPE_FLIPPER or core_type == ENEMY_TYPE_PULSAR then
                                -- Move away by one segment opposite the threat direction.
                                local move_right = (threat_rel_seg <= 0) -- threat at/aligned-left -> move right
                                local candidate = -1
                                if move_right then
                                    if is_open then
                                        if player_abs_seg < 15 then candidate = player_abs_seg + 1 end
                                    else
                                        candidate = (player_abs_seg + 1) % 16
                                    end
                                else
                                    if is_open then
                                        if player_abs_seg > 0 then candidate = player_abs_seg - 1 end
                                    else
                                        candidate = (player_abs_seg - 1 + 16) % 16
                                    end
                                end
                                if candidate ~= -1 then
                                    final_target_seg_abs = candidate
                                    target_depth = 0 -- Depth not critical for lateral dodge
                                end
                            end
                        end

                        break -- Found a dangerous close threat, no need to check others
                    end
                end
            end
        end
    end

    -- Apply shot count override (happens last)
    should_fire = should_fire or player_state.shot_count < 3

    return final_target_seg_abs, target_depth, should_fire, should_zap
end


-- ====================
-- GameState Class
-- ====================
M.GameState = {}
M.GameState.__index = M.GameState

function M.GameState:new()
    local self = setmetatable({}, M.GameState)
    self.credits = 0
    self.p1_level = 0
    self.p1_lives = 0
    self.gamestate = 0              -- Game state from address 0
    self.game_mode = 0              -- Game mode from address 5
    self.countdown_timer = 0        -- Countdown timer from address 4
    self.frame_counter = 0          -- Frame counter for tracking progress
    self.last_save_time = os.time() -- Track when we last sent save signal
    self.save_interval = 300        -- Send save signal every 5 minutes (300 seconds)
    self.start_delay = nil          -- Was used for random start delay in attract mode (currently unused)
    self.current_fps = 0            -- Store the calculated FPS value for display
    return self
end

function M.GameState:update(mem)
    self.gamestate = mem:read_u8(0x0000)        -- Game state at address 0
    self.game_mode = mem:read_u8(0x0005)        -- Game mode at address 5
    self.countdown_timer = mem:read_u8(0x0004)  -- Countdown timer from address 4
    self.credits = mem:read_u8(0x0006)          -- Credits
    self.p1_level = mem:read_u8(0x0046)         -- Player 1 level
    self.p1_lives = mem:read_u8(0x0048)         -- Player 1 lives
    self.frame_counter = self.frame_counter + 1 -- Increment frame counter
end

-- ====================
-- LevelState Class
-- ====================
M.LevelState = {}
M.LevelState.__index = M.LevelState

function M.LevelState:new()
    local self = setmetatable({}, M.LevelState)
    self.level_number = 0
    self.spike_heights = {} -- Array of 16 spike heights (0-15 index)
    self.level_type = 0     -- 00 = closed, FF = open (Read from memory, but might be unreliable)
    self.level_angles = {}  -- Array of 16 tube angles (0-15 index)
    self.level_shape = 0    -- Level shape (level_number % 16)
    -- Initialize tables
    for i = 0, 15 do
        self.spike_heights[i] = 0
        self.level_angles[i] = 0
    end
    return self
end

function M.LevelState:update(mem)
    self.level_number = mem:read_u8(0x009F)   -- Level number
    self.level_type = mem:read_u8(0x0111)     -- Level type (00=closed, FF=open)
    self.level_shape = self.level_number % 16 -- Calculate level shape

    -- Read spike heights for all 16 segments and store them indexed by absolute segment number (0-15)
    for i = 0, 15 do
        self.spike_heights[i] = mem:read_u8(0x03AC + i)
    end

    -- Read tube angles for all 16 segments indexed by absolute segment number (0-15)
    for i = 0, 15 do
        self.level_angles[i] = mem:read_u8(0x03EE + i)
    end
end

-- ====================
-- PlayerState Class
-- ====================
M.PlayerState = {}
M.PlayerState.__index = M.PlayerState

function M.PlayerState:new()
    local self = setmetatable({}, M.PlayerState)
    self.position = 0           -- Raw position byte from $0200
    self.alive = 0              -- 1 if alive, 0 if dead
    self.score = 0              -- Player score (decimal)
    self.superzapper_uses = 0
    self.superzapper_active = 0
    self.player_depth = 0       -- Player depth along the tube ($0202)
    self.player_state = 0       -- Player state byte from $0201
    self.shot_segments = {}     -- Table for 8 shot relative segments (1-8 index, or INVALID_SEGMENT)
    self.shot_positions = {}    -- Table for 8 shot positions/depths (1-8 index)
    self.shot_count = 0         -- Number of active shots ($0135)
    self.debounce = 0           -- Input debounce state ($004D)
    self.fire_detected = 0      -- Fire button state detected from debounce
    self.zap_detected = 0       -- Zap button state detected from debounce
    self.SpinnerAccum = 0       -- Raw spinner accumulator ($0051)
    self.prevSpinnerAccum = 0   -- Previous frame's spinner accumulator
    self.spinner_detected = 0   -- Calculated spinner delta based on accumulator change
    self.fire_commanded = 0     -- Fire action commanded by AI
    self.zap_commanded = 0      -- Zap action commanded by AI
    self.spinner_commanded = 0  -- Spinner action commanded by AI

    -- Initialize shot tables
    for i = 1, 8 do
        self.shot_segments[i] = INVALID_SEGMENT
        self.shot_positions[i] = 0
    end

    return self
end

-- PlayerState update needs the absolute_to_relative_segment function passed in
function M.PlayerState:update(mem, abs_to_rel_func)
    self.position = mem:read_u8(0x0200) -- Player position byte
    self.player_state = mem:read_u8(0x0201) -- Player state value at $201
    self.player_depth = mem:read_u8(0x0202) -- Player depth along the tube

    -- Player alive state: High bit of player_state ($201) is set when dead
    self.alive = ((self.player_state & 0x80) == 0) and 1 or 0

    -- Read and convert score from BCD using local helper
    local score_low = bcd_to_decimal(mem:read_u8(0x0040))
    local score_mid = bcd_to_decimal(mem:read_u8(0x0041))
    local score_high = bcd_to_decimal(mem:read_u8(0x0042))
    self.score = score_high * 10000 + score_mid * 100 + score_low

    self.superzapper_uses = mem:read_u8(0x03AA)   -- Superzapper availability
    self.superzapper_active = mem:read_u8(0x0125) -- Superzapper active status
    self.shot_count = mem:read_u8(0x0135)         -- Number of active player shots ($0135)

    -- Read all 8 shot positions and segments
    -- Determine if level is open based on the level type flag (might be unreliable)
    local level_type_flag = mem:read_u8(0x0111)
    local is_open = (level_type_flag == 0xFF)
    -- Or use level number pattern if flag is known bad:
    -- local level_num_zero_based = (mem:read_u8(0x009F) - 1)
    -- local is_open = (level_num_zero_based % 4 == 2)

    local player_abs_segment = self.position & 0x0F
    for i = 1, 8 do
        -- Read depth (position along the tube) from PlayerShotPositions ($02D3 - $02DA)
        self.shot_positions[i] = mem:read_u8(0x02D3 + i - 1)

        -- Shot is inactive if depth is 0
        if self.shot_positions[i] == 0 then
            self.shot_segments[i] = INVALID_SEGMENT
        else
            -- Read absolute segment from PlayerShotSegments ($02AD - $02B4)
            local abs_segment = mem:read_u8(0x02AD + i - 1)

            -- Shot is also inactive if segment byte is 0
            if abs_segment == 0 then
                self.shot_segments[i] = INVALID_SEGMENT
                self.shot_positions[i] = 0 -- Ensure position is also zeroed if segment is invalid
            else
                -- Valid position and valid segment read, calculate relative segment using passed function
                abs_segment = abs_segment & 0x0F  -- Mask to get valid segment 0-15
                self.shot_segments[i] = abs_to_rel_func(player_abs_segment, abs_segment, is_open)
            end
        end
    end

    -- Update detected input states from debounce byte ($004D)
    self.debounce = mem:read_u8(0x004D)
    self.fire_detected = (self.debounce & 0x10) ~= 0 and 1 or 0 -- Bit 4 for Fire
    self.zap_detected = (self.debounce & 0x08) ~= 0 and 1 or 0  -- Bit 3 for Zap

    -- Update spinner state
    local currentSpinnerAccum = mem:read_u8(0x0051) -- Read current accumulator value ($0051)
    -- self.spinner_commanded is updated in Controls:apply_action in main.lua

    -- Calculate inferred spinner movement delta by comparing current accumulator with previous
    local rawDelta = currentSpinnerAccum - self.prevSpinnerAccum

    -- Handle 8-bit wrap-around
    if rawDelta > 127 then
        rawDelta = rawDelta - 256
    elseif rawDelta < -128 then
        rawDelta = rawDelta + 256
    end
    self.spinner_detected = rawDelta -- Store the calculated delta

    -- Update accumulator values for next frame
    self.SpinnerAccum = currentSpinnerAccum
    self.prevSpinnerAccum = currentSpinnerAccum
end

-- ====================
-- EnemiesState Class
-- ====================
M.EnemiesState = {}
M.EnemiesState.__index = M.EnemiesState

function M.EnemiesState:new()
    local self = setmetatable({}, M.EnemiesState)
    -- Active enemy counts
    self.active_flippers = 0
    self.active_pulsars = 0
    self.active_tankers = 0
    self.active_spikers = 0
    self.active_fuseballs = 0
    -- Available spawn slots
    self.spawn_slots_flippers = 0
    self.spawn_slots_pulsars = 0
    self.spawn_slots_tankers = 0
    self.spawn_slots_spikers = 0
    self.spawn_slots_fuseballs = 0
    -- Other enemy state
    self.pulse_beat = 0      -- Pulsar pulse beat counter ($0147)
    self.pulsing = 0         -- Pulsar pulsing state ($0148)
    self.pulsar_fliprate = 0 -- Pulsar flip rate ($00B2)
    self.num_enemies_in_tube = 0 -- ($0108)
    self.num_enemies_on_top = 0  -- ($0109)
    self.enemies_pending = 0 -- ($03AB)

    -- Enemy info arrays (Size 7, for enemy slots 1-7)
    self.enemy_type_info = {} -- Raw type byte ($0283 + i - 1)
    self.active_enemy_info = {} -- Raw state byte ($028A + i - 1)
    self.enemy_segments = {}  -- Relative segment (-7 to +8 or -15 to +15, or INVALID_SEGMENT)
    self.enemy_abs_segments = {} -- Absolute segment (0-15, or INVALID_SEGMENT)
    self.enemy_depths = {}    -- Enemy depth/position ($02DF + i - 1)

    -- Decoded Enemy Info Tables (Size 7)
    self.enemy_core_type = {}      -- Bits 0-2 from type byte (0-4 based on assembly)
    self.enemy_direction_moving = {} -- Bit 6 from type byte (0/1)
    self.enemy_between_segments = {} -- Bit 7 from type byte (0/1)
    self.enemy_moving_away = {}    -- Bit 7 from state byte (0/1)
    self.enemy_can_shoot = {}      -- Bit 6 from state byte (0/1)
    self.enemy_split_behavior = {} -- Bits 0-1 from state byte

    -- Raw angle/progress per enemy (Size 7)
    -- Mirrors more_enemy_info at $02CC..$02D2; low nibble (0-15) increments/decrements while between segments
    self.more_enemy_info = {}

    -- Enemy Shot Info (Size 4)
    self.shot_positions = {}          -- Absolute depth/position ($02DB + i - 1)
    self.shot_positions_lsb = {}      -- Fractional LSB for enemy shots ($02E6 + i - 1)
    self.enemy_shot_segments = {}     -- Relative segment (-7 to +8 or -15 to +15, or INVALID_SEGMENT)
    self.enemy_shot_abs_segments = {} -- Absolute segment (0-15, or INVALID_SEGMENT)

    -- Pending enemy data (Size 64)
    self.pending_vid = {}              -- ($0243 + i - 1)
    self.pending_seg = {}              -- Relative segment ($0203 + i - 1, or INVALID_SEGMENT)

    -- Charging Fuseball Tracking (Size 7, one per enemy slot that can be a fuseball)
    -- Array contains relative segment positions (-7 to +8 or -15 to +15, or 0 when no charging fuseball)
    self.charging_fuseball = {} -- Relative segments of charging fuseballs
    
    -- Pulsar Tracking (Size 7, one per enemy slot that can be a pulsar)
    -- Array contains relative segment positions (-7 to +8 or -15 to +15, or 0 when no pulsar)
    self.active_pulsar = {} -- Relative segments of pulsars
    
    -- Top Rail Enemy Tracking (Size 7, one per enemy slot that can be a pulsar or flipper at depth 0x20)
    -- Array contains relative segment positions (-7 to +8 or -15 to +15, or 0 when no valid enemy)
    self.active_top_rail_enemies = {} -- Relative segments of pulsars/flippers at the top rail
    
    -- NEW: Fractional Enemy Segments By Slot (Size 7, indexed 1-7)
    -- Stores fractional segment position as 12-bit integer
    self.fractional_enemy_segments_by_slot = {}

    -- Engineered Features for AI (Calculated in update)
    self.nearest_enemy_seg = INVALID_SEGMENT        -- Relative segment of nearest target enemy
    self.nearest_enemy_abs_seg_internal = -1        -- Absolute segment of nearest target enemy (-1 if none)
    self.nearest_enemy_should_fire = false          -- Whether expert logic recommends firing at target
    self.nearest_enemy_should_zap = false           -- Whether expert logic recommends zapping
    self.is_aligned_with_nearest = 0.0              -- 1.0 if aligned, 0.0 otherwise
    self.nearest_enemy_depth_raw = 255              -- Depth of nearest target enemy (0-255)
    self.alignment_error_magnitude = 0.0            -- Normalized alignment error (0.0-1.0) scaled later

    -- Initialize tables
    for i = 1, 7 do
        self.enemy_type_info[i] = 0
        self.active_enemy_info[i] = 0
        self.enemy_segments[i] = INVALID_SEGMENT
        self.enemy_abs_segments[i] = INVALID_SEGMENT
        self.enemy_depths[i] = 0
        self.enemy_core_type[i] = 0
        self.enemy_direction_moving[i] = 0
        self.enemy_between_segments[i] = 0
        self.enemy_moving_away[i] = 0
        self.enemy_can_shoot[i] = 0
        self.enemy_split_behavior[i] = 0
        self.more_enemy_info[i] = 0
    end
    for i = 1, 4 do
        self.shot_positions[i] = 0
        self.shot_positions_lsb[i] = 0
        self.enemy_shot_segments[i] = INVALID_SEGMENT
        self.enemy_shot_abs_segments[i] = INVALID_SEGMENT
    end
     for i = 1, 64 do
        self.pending_vid[i] = 0
        self.pending_seg[i] = INVALID_SEGMENT
    end
    for i = 1, 7 do
        self.charging_fuseball[i] = 0
        self.active_pulsar[i] = 0
        self.active_top_rail_enemies[i] = 0
        self.fractional_enemy_segments_by_slot[i] = 0
    end

    return self
end

-- EnemiesState update needs game_state, player_state, level_state, and abs_to_rel_func
function M.EnemiesState:update(mem, game_state, player_state, level_state, abs_to_rel_func)
    -- Get player position and level type for relative calculations
    local player_abs_segment = player_state.position & 0x0F -- Get current player absolute segment
    -- Determine if level is open based *only* on the memory flag now
    local is_open = (level_state.level_type == 0xFF)
    -- Re-enable debug print to monitor memory flag and resulting is_open

    -- Read active enemy counts and related state
    self.active_flippers         = mem:read_u8(0x0142) -- n_flippers
    self.active_pulsars          = mem:read_u8(0x0143) -- n_pulsars
    self.active_tankers          = mem:read_u8(0x0144) -- n_tankers
    self.active_spikers          = mem:read_u8(0x0145) -- n_spikers
    self.active_fuseballs        = mem:read_u8(0x0146) -- n_fuseballs
    self.pulse_beat              = mem:read_u8(0x0147) -- pulse_beat
    self.pulsing                 = mem:read_u8(0x0148) -- pulsing state
    self.pulsar_fliprate         = mem:read_u8(0x00B2) -- Pulsar flip rate
    self.num_enemies_in_tube     = mem:read_u8(0x0108) -- NumInTube
    self.num_enemies_on_top      = mem:read_u8(0x0109) -- NumOnTop
    self.enemies_pending         = mem:read_u8(0x03AB) -- PendingEnemies

    -- Read available spawn slots
    self.spawn_slots_flippers = mem:read_u8(0x013D)  -- avl_flippers
    self.spawn_slots_pulsars = mem:read_u8(0x013E)   -- avl_pulsars
    self.spawn_slots_tankers = mem:read_u8(0x013F)   -- avl_tankers
    self.spawn_slots_spikers = mem:read_u8(0x0140)   -- avl_spikers
    self.spawn_slots_fuseballs = mem:read_u8(0x0141) -- avl_fuseballs

    -- Read and process enemy slots (1-7)
    for i = 1, 7 do
    -- Read depth and segment first to determine activity
        local enemy_depth_raw = mem:read_u8(0x02DF + i - 1) -- EnemyPositions ($02DF-$02E5)
        local abs_segment_raw = mem:read_u8(0x02B9 + i - 1) -- EnemySegments ($02B9-$02BF)

        -- Reset decoded info for this slot
        self.enemy_core_type[i] = 0
        self.enemy_direction_moving[i] = 0
        self.enemy_between_segments[i] = 0
        self.enemy_moving_away[i] = 0
        self.enemy_can_shoot[i] = 0
        self.enemy_split_behavior[i] = 0
        self.enemy_segments[i] = INVALID_SEGMENT
        self.enemy_abs_segments[i] = INVALID_SEGMENT
        self.enemy_depths[i] = 0
        self.enemy_type_info[i] = 0
        self.active_enemy_info[i] = 0

        -- Check if enemy is active (depth > 0 and segment raw byte > 0)
        if enemy_depth_raw > 0 and abs_segment_raw > 0 then
            local abs_segment = abs_segment_raw & 0x0F -- Mask to 0-15
            self.enemy_abs_segments[i] = abs_segment
            self.enemy_segments[i] = abs_to_rel_func(player_abs_segment, abs_segment, is_open)
            self.enemy_depths[i] = enemy_depth_raw

            -- Read raw type/state bytes only for active enemies
            local type_byte = mem:read_u8(0x0283 + i - 1) -- EnemyTypeInfo ($0283-$0289)
            local state_byte = mem:read_u8(0x028A + i - 1) -- ActiveEnemyInfo ($028A-$0290)
            self.enemy_type_info[i] = type_byte
            self.active_enemy_info[i] = state_byte

            -- Decode Type Byte (Use assembly mask)
            self.enemy_core_type[i] = type_byte & ENEMY_TYPE_MASK -- Apply the mask to get 0-4
            self.enemy_direction_moving[i] = (type_byte & 0x40) ~= 0 and 1 or 0 -- Bit 6: Segment increasing?
            self.enemy_between_segments[i] = (type_byte & 0x80) ~= 0 and 1 or 0 -- Bit 7: Between segments?

            -- Decode State Byte
            self.enemy_moving_away[i] = (state_byte & 0x80) ~= 0 and 1 or 0 -- Bit 7: Moving Away?
            self.enemy_can_shoot[i] = (state_byte & 0x40) ~= 0 and 1 or 0   -- Bit 6: Can Shoot?
            self.enemy_split_behavior[i] = state_byte & 0x03                -- Bits 0-1: Split Behavior

            -- Read raw more_enemy_info angle/progress byte ($02CC-$02D2)
            self.more_enemy_info[i] = mem:read_u8(0x02CC + i - 1)
        end -- End if enemy active
    end -- End enemy slot loop

    -- Calculate charging Fuseball segments (reset first)
    for i = 1, 7 do 
        self.charging_fuseball[i] = 0
        self.active_pulsar[i] = 0
        self.active_top_rail_enemies[i] = 0
    end
    
    for i = 1, 7 do
        -- Check if it's an active Fuseball (type 4) moving towards player (bit 7 of state byte is clear)
        if self.enemy_core_type[i] == ENEMY_TYPE_FUSEBALL and self.enemy_abs_segments[i] ~= INVALID_SEGMENT and (self.active_enemy_info[i] & 0x80) == 0 then -- Correct type 4
            self.charging_fuseball[i] = self.enemy_segments[i]
        end
        
        -- Check if it's an active Pulsar (type 1)
        if self.enemy_core_type[i] == ENEMY_TYPE_PULSAR and self.enemy_abs_segments[i] ~= INVALID_SEGMENT then -- Pulsar type 1
            self.active_pulsar[i] = self.enemy_segments[i]
        end

        -- Check if it's a top rail Pulsar or Flipper (close to top)
        if (self.enemy_abs_segments[i] ~= INVALID_SEGMENT and (self.enemy_core_type[i] == ENEMY_TYPE_PULSAR or self.enemy_core_type[i] == ENEMY_TYPE_FLIPPER) and self.enemy_depths[i] <= 0x30) then
            -- Build absolute floating segment using between-segment progress (from more_enemy_info low nibble)
            local abs_int = self.enemy_abs_segments[i]
            local angle_nibble = self.more_enemy_info[i] & 0x0F -- 0..15
            -- Compute a monotonically increasing progress [0,1) based on direction semantics:
            -- Direction bit set (increasing): nibble DECREMENTS each tick -> progress = (15 - nibble)/16
            -- Direction bit clear (decreasing): nibble INCREMENTS each tick -> progress = nibble/16
            local progress = 0.0
            if self.enemy_between_segments[i] == 1 then
                if self.enemy_direction_moving[i] == 1 then
                    progress = (15.0 - angle_nibble) / 16.0
                else
                    progress = angle_nibble / 16.0
                end
            end

            -- Absolute float using StartFlip semantics:
            -- When increasing, enemy_seg already points to the DESTINATION segment; position is (dest - (1 - progress)).
            -- When decreasing, enemy_seg points to the SOURCE segment; position is (source - progress).
            local abs_float
            if self.enemy_between_segments[i] == 1 then
                if self.enemy_direction_moving[i] == 1 then
                    abs_float = abs_int + progress - 1.0
                else
                    abs_float = abs_int - progress
                end
            else
                abs_float = abs_int
            end

            -- Normalize to [0,16)
            abs_float = abs_float % 16.0
            if abs_float < 0 then abs_float = abs_float + 16.0 end

            -- Convert to relative float
            local rel_float
            if is_open then
                rel_float = abs_float - player_abs_segment
            else
                -- Wrap into [-8,+8]
                rel_float = abs_float - player_abs_segment
                while rel_float > 8.0 do rel_float = rel_float - 16.0 end
                while rel_float < -8.0 do rel_float = rel_float + 16.0 end
            end

            self.active_top_rail_enemies[i] = rel_float
            -- print("Top rail at: " .. string.format("%.3f", rel_float) .. " (slot " .. i .. ")")
        end
    end

    -- Read and process enemy shots (1-4)
    for i = 1, 4 do
        -- Read integer depth/position and fractional LSB
        local pos_int = mem:read_u8(0x02DB + i - 1)  -- EnemyShotPositions ($02DB-$02DE)
        local pos_lsb = mem:read_u8(0x02E6 + i - 1)  -- enm_shot_lsb ($02E6-$02E9)

        -- If integer position is 0, shot is inactive regardless of LSB
        if pos_int == 0 then
            self.enemy_shot_segments[i] = INVALID_SEGMENT
            self.enemy_shot_abs_segments[i] = INVALID_SEGMENT
            self.shot_positions_lsb[i] = 0
            self.shot_positions[i] = 0
        else
            -- Read shot segment byte
            local abs_segment_raw = mem:read_u8(0x02B5 + i - 1) -- EnemyShotSegments ($02B5-$02B8)
             -- Also inactive if segment byte is 0
            if abs_segment_raw == 0 then
                self.enemy_shot_segments[i] = INVALID_SEGMENT
                self.enemy_shot_abs_segments[i] = INVALID_SEGMENT
                self.shot_positions_lsb[i] = 0
                self.shot_positions[i] = 0 -- Ensure position is zeroed
            else
                local abs_segment = abs_segment_raw & 0x0F -- Mask to 0-15
                self.enemy_shot_abs_segments[i] = abs_segment
                self.enemy_shot_segments[i] = abs_to_rel_func(player_abs_segment, abs_segment, is_open)
                -- Combine integer position with LSB to form full 8.8 fixed-point as a float
                -- Range remains < 256.0 (pos_int in [1,255], pos_lsb in [0,255])
                self.shot_positions_lsb[i] = pos_lsb
                self.shot_positions[i] = pos_int + (pos_lsb / 256.0)
            end
        end
    end

    -- Read pending_seg (64 bytes starting at 0x0203), store relative
    for i = 1, 64 do
        local abs_segment_raw = mem:read_u8(0x0203 + i - 1)
        if abs_segment_raw == 0 then
            self.pending_seg[i] = INVALID_SEGMENT -- Not active, use sentinel
        else
            local segment = abs_segment_raw & 0x0F    -- Mask to ensure 0-15
            self.pending_seg[i] = abs_to_rel_func(player_abs_segment, segment, is_open)
        end
    end

    -- Read pending_vid (64 bytes starting at 0x0243)
    for i = 1, 64 do
        self.pending_vid[i] = mem:read_u8(0x0243 + i - 1)
    end
    
    -- Calculate true between-segment progress per slot (scaled to 12-bit, 0..4095)
    for i = 1, 7 do
        if self.enemy_abs_segments[i] == INVALID_SEGMENT then
            self.fractional_enemy_segments_by_slot[i] = INVALID_SEGMENT
        else
            -- Use the same direction-aware progress as above
            local progress = 0.0
            if self.enemy_between_segments[i] == 1 then
                local angle_nibble = self.more_enemy_info[i] & 0x0F -- 0..15
                if self.enemy_direction_moving[i] == 1 then
                    progress = (15.0 - angle_nibble) / 16.0
                else
                    progress = angle_nibble / 16.0
                end
            end
            local scaled_value = math.floor(progress * 4096.0 + 0.5) -- Round to nearest
            if scaled_value >= 4096 then scaled_value = 4095 end
            if scaled_value < 0 then scaled_value = 0 end
            self.fractional_enemy_segments_by_slot[i] = scaled_value
        end
    end

    -- === Calculate and store nearest enemy segment and engineered features ===
    -- Reset zap recommendation before calculation
    self.nearest_enemy_should_zap = false
    -- Use the internal helper function find_target_segment, passing the calculated is_open
    local nearest_abs_seg, nearest_depth, should_fire_target, should_zap_target = find_target_segment(game_state, player_state, level_state, self, abs_to_rel_func, is_open)

    -- Store results in the object's fields
    self.nearest_enemy_abs_seg_internal = nearest_abs_seg -- Store absolute segment (-1 if none)
    self.nearest_enemy_should_fire = should_fire_target   -- Store firing recommendation
    self.nearest_enemy_should_zap = should_zap_target     -- Store zapping recommendation

    -- Add debug print for hints
    -- print(string.format("STATE HINTS DEBUG: Abs=%d, Fire=%s, Zap=%s", self.nearest_enemy_abs_seg_internal, tostring(self.nearest_enemy_should_fire), tostring(self.nearest_enemy_should_zap)))

    if nearest_abs_seg == -1 then -- No target found
        self.nearest_enemy_seg = INVALID_SEGMENT
        self.is_aligned_with_nearest = 0.0
        self.nearest_enemy_depth_raw = 255 -- Use max depth as sentinel
        self.alignment_error_magnitude = 0.0
    else -- Valid target found
        -- Use the is_open calculated at the start of this function
        local nearest_rel_seg = abs_to_rel_func(player_abs_segment, nearest_abs_seg, is_open)
        self.nearest_enemy_seg = nearest_rel_seg     -- Store relative segment
        self.nearest_enemy_depth_raw = nearest_depth -- Store raw depth

        -- Calculate Is_Aligned
        self.is_aligned_with_nearest = (nearest_rel_seg == 0) and 1.0 or 0.0

        -- Calculate Alignment_Error_Magnitude (Normalized 0.0-1.0)
        local error_abs = math.abs(nearest_rel_seg)
        local max_error = is_open and 15.0 or 8.0 -- Max possible distance
        self.alignment_error_magnitude = (error_abs > 0) and (error_abs / max_error) or 0.0
        -- Scaling happens during packing
    end
end


-- Helper functions for display (can be called on an instance)
function M.EnemiesState:decode_enemy_type(type_byte)
    local enemy_type = type_byte & ENEMY_TYPE_MASK -- Use the mask
    local between_segments = (type_byte & 0x80) ~= 0
    local segment_increasing = (type_byte & 0x40) ~= 0
    return string.format("%d%s%s",
        enemy_type,
        between_segments and "B" or "-",
        segment_increasing and "+" or "-" -- Use '-' if not increasing
    )
end

function M.EnemiesState:decode_enemy_state(state_byte)
    local split_behavior = state_byte & 0x03
    local can_shoot = (state_byte & 0x40) ~= 0
    local moving_away = (state_byte & 0x80) ~= 0
    return string.format("%s%s%d", -- Show split behavior as number
        moving_away and "A" or "T", -- Away / Towards
        can_shoot and "S" or "-",   -- Can Shoot / Cannot Shoot
        split_behavior
    )
end

function M.EnemiesState:get_total_active()
    return self.active_flippers + self.active_pulsars + self.active_tankers +
        self.active_spikers + self.active_fuseballs
end

-- Return the module table
return M